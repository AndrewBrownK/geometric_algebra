// Note on Operative Statistics:
// Operative Statistics are not a precise predictor of performance or performance comparisons.
// This is due to varying hardware capabilities and compiler optimizations.
// As always, where performance is a concern, there is no substitute for
// real measurements on real work-loads on real hardware.
// Disclaimer aside, enjoy the fun information =)
impl InfixAntiWedge for AntiCircleOnOrigin {}
impl AntiWedge<AntiDualNum> for AntiCircleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatPoint> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiFlector> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for AntiCircleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group1()[3])));
    }
}
impl AntiWedge<AntiScalar> for AntiCircleOnOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other[e12345])),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<Circle> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for AntiCircleOnOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for AntiCircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group1(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group1()[2])),
                    ((self.group1()[2] * other.group1()[0]) + (self.group0()[1] * other.group1()[3])),
                    ((self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for AntiCircleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiCircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[1]),
                    (self.group1()[0] * other.group0()[2]),
                    (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
                ])),
        );
    }
}
impl AntiWedge<Horizon> for AntiCircleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other[e3215])));
    }
}
impl AntiWedge<Line> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for AntiCircleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for AntiCircleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for AntiCircleOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       17        0
    //    simd3        0        1        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       19        0
    //  no simd       13       24        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group1()[2] * other.group6()[2])
                    - (self.group1()[1] * other.group6()[1])
                    - (self.group1()[0] * other.group6()[0])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group9(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other[e45]) + (self.group1()[1] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[1]) + (self.group0()[1] * other[e45])),
                    ((self.group0()[2] * other[e45]) + (self.group1()[0] * other.group9()[2])),
                    (-(self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
                ])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for AntiCircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiCircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Sphere> for AntiCircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiCircleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<SphereOnOrigin> for AntiCircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEven> for AntiCircleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiCircleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiCircleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiCircleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for AntiCircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for AntiCircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group3(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group3()[3]) + (self.group1()[1] * other.group3()[2])),
                    ((self.group1()[2] * other.group3()[0]) + (self.group0()[1] * other.group3()[3])),
                    ((self.group0()[2] * other.group3()[3]) + (self.group1()[0] * other.group3()[1])),
                    (-(self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiCircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                    ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3])),
                    ((self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiCircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                    ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3])),
                    ((self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiCircleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiCircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group1(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group1()[3]),
                    (self.group1()[2] * other.group1()[1]),
                    (self.group1()[0] * other.group1()[2]),
                    (-(self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiCircleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group1()[3])));
    }
}
impl InfixAntiWedge for AntiDipoleOnOrigin {}
impl AntiWedge<AntiDualNum> for AntiDipoleOnOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for AntiDipoleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for AntiDipoleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiLine> for AntiDipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for AntiDipoleOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for AntiDipoleOnOrigin {
    type Output = AntiDipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiDipoleOnOrigin::from_groups(/* e423, e431, e412, e321 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<Circle> for AntiDipoleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiDipoleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiDipoleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for AntiDipoleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiDipoleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group1()[0]),
            (self.group0()[3] * other.group1()[1]),
            (self.group0()[3] * other.group1()[2]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for AntiDipoleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Dipole> for AntiDipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for AntiDipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for AntiDipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for AntiDipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for AntiDipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for AntiDipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for AntiDipoleOnOrigin {
    type Output = AntiDipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiDipoleOnOrigin::from_groups(/* e423, e431, e412, e321 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<FlatOrigin> for AntiDipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for AntiDipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for AntiDipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for AntiDipoleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[1]),
                    (self.group0()[0] * other.group1()[2]),
                    (self.group0()[1] * other.group1()[0]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) - (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group1()[3]) - (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group1()[3]) - (self.group0()[3] * other.group1()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for AntiDipoleOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiDipoleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       14        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[2] * other.group0()[2])),
                ((self.group0()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0] * -1.0),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Horizon> for AntiDipoleOnOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other[e3215]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<Line> for AntiDipoleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for AntiDipoleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for AntiDipoleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Motor> for AntiDipoleOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4, e1, e2, e3
            (-(swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for AntiDipoleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiDipoleOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for AntiDipoleOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       19        0
    //    simd3        1        3        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       12       23        0
    //  no simd       17       32        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group0()[3] * other.group3()[3])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group8()[2], other.group8()[0], other.group8()[1], other.group6()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group6()[0]) + (self.group0()[2] * other.group8()[1])),
                    ((self.group0()[3] * other.group6()[1]) + (self.group0()[0] * other.group8()[2])),
                    ((self.group0()[3] * other.group6()[2]) + (self.group0()[1] * other.group8()[0])),
                    (-(self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
                ])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e423, e431, e412
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for AntiDipoleOnOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        4        8        0
    //  no simd        6       12        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiDipoleOnOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        4        0
    // no simd        3       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<Sphere> for AntiDipoleOnOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        4        8        0
    //  no simd        6       12        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiDipoleOnOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for AntiDipoleOnOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       12        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorEven> for AntiDipoleOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4, e1, e2, e3
            (-(swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group1()[2], other.group2()[2], other.group2()[0], other.group2()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiDipoleOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4, e1, e2, e3
            (-(swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group1()[2], other.group2()[2], other.group2()[0], other.group2()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiDipoleOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e4, e1, e2, e3
            (-(swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group1()[2], other.group2()[2], other.group2()[0], other.group2()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for AntiDipoleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiDipoleOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for AntiDipoleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for AntiDipoleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group3()[1]),
                    (self.group0()[0] * other.group3()[2]),
                    (self.group0()[1] * other.group3()[0]),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group3()[3]) - (self.group0()[3] * other.group3()[0])),
                ((self.group0()[1] * other.group3()[3]) - (self.group0()[3] * other.group3()[1])),
                ((self.group0()[2] * other.group3()[3]) - (self.group0()[3] * other.group3()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiDipoleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group2()[1]),
                    (self.group0()[0] * other.group2()[2]),
                    (self.group0()[1] * other.group2()[0]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) - (self.group0()[3] * other.group2()[0])),
                ((self.group0()[1] * other.group2()[3]) - (self.group0()[3] * other.group2()[1])),
                ((self.group0()[2] * other.group2()[3]) - (self.group0()[3] * other.group2()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiDipoleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group2()[1]),
                    (self.group0()[0] * other.group2()[2]),
                    (self.group0()[1] * other.group2()[0]),
                    (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) - (self.group0()[3] * other.group2()[0])),
                ((self.group0()[1] * other.group2()[3]) - (self.group0()[3] * other.group2()[1])),
                ((self.group0()[2] * other.group2()[3]) - (self.group0()[3] * other.group2()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiDipoleOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiDipoleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       14        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[3]) + (self.group0()[2] * other.group1()[2])),
                ((self.group0()[0] * other.group1()[3]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[1] * other.group1()[1])),
                (self.group0()[3] * other.group0()[3] * -1.0),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[1] * -1.0),
                (self.group0()[3] * other.group1()[2] * -1.0),
                (self.group0()[3] * other.group1()[3] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiDipoleOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        ]));
    }
}
impl InfixAntiWedge for AntiDualNum {}
impl AntiWedge<AntiCircleOnOrigin> for AntiDualNum {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for AntiDualNum {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiScalar> for AntiDualNum {
    type Output = AntiDualNum;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiDualNum::from_groups(/* e3215, scalar */ (self.group0() * Simd32x2::from(other[e12345])));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for AntiDualNum {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[3]));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for AntiDualNum {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[3], other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for AntiDualNum {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[0]])),
        );
    }
}
impl AntiWedge<Circle> for AntiDualNum {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[0]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiDualNum {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[0]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiDualNum {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for AntiDualNum {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<CircleOnOrigin> for AntiDualNum {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[0]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for AntiDualNum {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<Dipole> for AntiDualNum {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for AntiDualNum {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[0]) * other.group0() * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<DipoleAtInfinity> for AntiDualNum {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group0()[0] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleAtOrigin> for AntiDualNum {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<DipoleOnOrigin> for AntiDualNum {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[0]) * other.group0() * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for AntiDualNum {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<DualNum> for AntiDualNum {
    type Output = AntiDualNum;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiDualNum::from_groups(/* e3215, scalar */ (self.group0() * Simd32x2::from(other.group0()[1])));
    }
}
impl AntiWedge<FlatOrigin> for AntiDualNum {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[0] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for AntiDualNum {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group0()[0] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Flector> for AntiDualNum {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return MotorAtInfinity::from_groups(
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiDualNum {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return MotorAtInfinity::from_groups(
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[0]) * swizzle!(other.group0(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Line> for AntiDualNum {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<LineOnOrigin> for AntiDualNum {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<Motor> for AntiDualNum {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e15, e25, e35, e3215
            (Simd32x4::from(self.group0()[0]) * other.group0()),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for AntiDualNum {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e15, e25, e35, e3215
            (Simd32x4::from(self.group0()[0]) * other.group0()),
        );
    }
}
impl AntiWedge<MultiVector> for AntiDualNum {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1       13        0
    //    simd3        0        4        0
    // Totals...
    // yes simd        1       17        0
    //  no simd        1       25        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([((self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other.group0()[1])), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group3()[0] * -1.0),
                (self.group0()[0] * other.group3()[1] * -1.0),
                (self.group0()[0] * other.group3()[2] * -1.0),
                0.0,
            ]),
            // e5
            (self.group0()[0] * other.group3()[3] * -1.0),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]])),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[0]) * other.group7()),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group9()[0] * -1.0)]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]) * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            (self.group0()[0] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for AntiDualNum {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for AntiDualNum {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullSphereAtOrigin> for AntiDualNum {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self.group0()[0] * other[e1234] * -1.0));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for AntiDualNum {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for AntiDualNum {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self.group0()[0]) * swizzle!(other.group0(), 3, 0, 1, 2) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Origin> for AntiDualNum {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other[e4]));
    }
}
impl AntiWedge<Plane> for AntiDualNum {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return LineAtInfinity::from_groups(
            // e235, e315, e125
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiDualNum {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return LineAtInfinity::from_groups(/* e235, e315, e125 */ (Simd32x3::from(self.group0()[0]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<RoundPoint> for AntiDualNum {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[3]));
    }
}
impl AntiWedge<RoundPointAtOrigin> for AntiDualNum {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[0]));
    }
}
impl AntiWedge<Sphere> for AntiDualNum {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiFlatPoint::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other[e4315]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiDualNum {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiFlatOrigin::from_groups(/* e321 */ (self.group0()[0] * other.group0()[1] * -1.0));
    }
}
impl AntiWedge<SphereOnOrigin> for AntiDualNum {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiFlatPoint::from_groups(/* e235, e315, e125, e321 */ (Simd32x4::from(self.group0()[0]) * other.group0() * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<VersorEven> for AntiDualNum {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        1        6        0
    //  no simd        1        9        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                ((self.group0()[0] * other.group3()[3]) + (self.group0()[1] * other.group0()[3])),
            ]),
            // e15, e25, e35, e3215
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiDualNum {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        1        6        0
    //  no simd        1        9        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other.group0()[3])),
            ]),
            // e15, e25, e35, e3215
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiDualNum {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[0])]),
            // e15, e25, e35, e3215
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[0]])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for AntiDualNum {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiDualNum {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        1        6        0
    //  no simd        1        9        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other.group0()[3])),
            ]),
            // e15, e25, e35, e3215
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for AntiDualNum {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group2()[3]])),
        );
    }
}
impl AntiWedge<VersorOdd> for AntiDualNum {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group2()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiDualNum {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (Simd32x4::from(self.group0()[0]) * other.group0() * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiDualNum {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiDualNum {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[3], other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiDualNum {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[0]) * swizzle!(other.group1(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (Simd32x4::from(self.group0()[0]) * other.group0() * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiDualNum {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group2()[3], other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl InfixAntiWedge for AntiFlatOrigin {}
impl AntiWedge<AntiScalar> for AntiFlatOrigin {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self[e321] * other[e12345]));
    }
}
impl AntiWedge<Circle> for AntiFlatOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (Simd32x3::from(self[e321]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiFlatOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e321]) * other.group1()));
    }
}
impl AntiWedge<CircleAtInfinity> for AntiFlatOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (Simd32x3::from(self[e321]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiFlatOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e321]) * other.group1()));
    }
}
impl AntiWedge<Dipole> for AntiFlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e321] * other.group1()[3] * -1.0));
    }
}
impl AntiWedge<DipoleAligningOrigin> for AntiFlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e321] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleAtInfinity> for AntiFlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e321] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleOnOrigin> for AntiFlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e321] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DualNum> for AntiFlatOrigin {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self[e321] * other.group0()[1]));
    }
}
impl AntiWedge<FlatOrigin> for AntiFlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e321] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for AntiFlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e321] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Flector> for AntiFlatOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e321]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiFlatOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e321]) * swizzle!(other.group0(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Line> for AntiFlatOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e321]) * other.group0()));
    }
}
impl AntiWedge<LineOnOrigin> for AntiFlatOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e321]) * other.group0()));
    }
}
impl AntiWedge<Motor> for AntiFlatOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (Simd32x4::from(self[e321]) * swizzle!(other.group0(), 3, 0, 1, 2)));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiFlatOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (Simd32x4::from(self[e321]) * swizzle!(other.group0(), 3, 0, 1, 2)));
    }
}
impl AntiWedge<MultiVector> for AntiFlatOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        6        0
    //    simd3        0        2        0
    // Totals...
    // yes simd        0        8        0
    //  no simd        0       12        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self[e321] * other.group3()[3] * -1.0), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([(self[e321] * other.group6()[0]), (self[e321] * other.group6()[1]), (self[e321] * other.group6()[2]), 0.0]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(self[e321]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]) * Simd32x3::from(-1.0)),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self[e321] * other.group0()[1])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for AntiFlatOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e321]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiFlatOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self[e321]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<Sphere> for AntiFlatOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e321]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for AntiFlatOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e321]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorEven> for AntiFlatOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self[e321]) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiFlatOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self[e321]) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiFlatOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self[e321]) * Simd32x4::from([other.group0()[0], other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiFlatOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self[e321]) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<VersorOdd> for AntiFlatOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e321]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiFlatOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e321]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiFlatOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e321]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiFlatOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e321]) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[3], other.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl InfixAntiWedge for AntiFlatPoint {}
impl AntiWedge<AntiCircleOnOrigin> for AntiFlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for AntiFlatPoint {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for AntiFlatPoint {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiFlatPoint::from_groups(/* e235, e315, e125, e321 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for AntiFlatPoint {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for AntiFlatPoint {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Circle> for AntiFlatPoint {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiFlatPoint {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiFlatPoint {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<CircleAtOrigin> for AntiFlatPoint {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiFlatPoint {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for AntiFlatPoint {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Dipole> for AntiFlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for AntiFlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for AntiFlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleAtOrigin> for AntiFlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for AntiFlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for AntiFlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DualNum> for AntiFlatPoint {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiFlatPoint::from_groups(/* e235, e315, e125, e321 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<FlatOrigin> for AntiFlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for AntiFlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Flector> for AntiFlatPoint {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       14        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]) * Simd32x4::from(-1.0)),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiFlatPoint {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       14        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self.group0()[3]) * swizzle!(other.group0(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[3]) - (self.group0()[2] * other.group0()[2])),
                (-(self.group0()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[1] * other.group0()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Line> for AntiFlatPoint {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for AntiFlatPoint {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Motor> for AntiFlatPoint {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for AntiFlatPoint {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for AntiFlatPoint {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       23        0
    //    simd3        1        3        0
    // Totals...
    // yes simd       15       26        0
    //  no simd       17       32        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group0()[3] * other.group3()[3])
                    - (self.group0()[2] * other.group3()[2])
                    - (self.group0()[0] * other.group3()[0])
                    - (self.group0()[1] * other.group3()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[3] * other.group6()[0]) + (self.group0()[1] * other.group7()[2]) - (self.group0()[2] * other.group7()[1])),
                ((self.group0()[3] * other.group6()[1]) - (self.group0()[0] * other.group7()[2]) + (self.group0()[2] * other.group7()[0])),
                ((self.group0()[3] * other.group6()[2]) + (self.group0()[0] * other.group7()[1]) - (self.group0()[1] * other.group7()[0])),
                0.0,
            ]),
            // e5
            (-(self.group0()[2] * other.group6()[2]) - (self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from([
                ((self.group0()[1] * other.group9()[3]) - (self.group0()[2] * other.group9()[2])),
                (-(self.group0()[0] * other.group9()[3]) + (self.group0()[2] * other.group9()[1])),
                ((self.group0()[0] * other.group9()[2]) - (self.group0()[1] * other.group9()[1])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for AntiFlatPoint {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for AntiFlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for AntiFlatPoint {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other[e1234]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for AntiFlatPoint {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for AntiFlatPoint {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Plane> for AntiFlatPoint {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       12        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
            // e15, e25, e35
            Simd32x3::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiFlatPoint {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        4        0
    // no simd        3       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)),
            // e15, e25, e35
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Sphere> for AntiFlatPoint {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        4        8        0
    //  no simd        6       12        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            ((Simd32x3::from(other[e4315]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e15, e25, e35
            Simd32x3::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiFlatPoint {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for AntiFlatPoint {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        4        8        0
    //  no simd        6       12        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e15, e25, e35
            Simd32x3::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for AntiFlatPoint {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiFlatPoint {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiFlatPoint {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e1, e2, e3, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for AntiFlatPoint {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiFlatPoint {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for AntiFlatPoint {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for AntiFlatPoint {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group2()[3]),
                    (self.group0()[1] * other.group2()[3]),
                    (self.group0()[2] * other.group2()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group3()[2]) - (self.group0()[2] * other.group3()[1])),
                (-(self.group0()[0] * other.group3()[2]) + (self.group0()[2] * other.group3()[0])),
                ((self.group0()[0] * other.group3()[1]) - (self.group0()[1] * other.group3()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiFlatPoint {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group1()[3]),
                    (self.group0()[1] * other.group1()[3]),
                    (self.group0()[2] * other.group1()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group2()[2]) - (self.group0()[2] * other.group2()[1])),
                (-(self.group0()[0] * other.group2()[2]) + (self.group0()[2] * other.group2()[0])),
                ((self.group0()[0] * other.group2()[1]) - (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiFlatPoint {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       14        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group2()[2]) - (self.group0()[2] * other.group2()[1])),
                (-(self.group0()[0] * other.group2()[2]) + (self.group0()[2] * other.group2()[0])),
                ((self.group0()[0] * other.group2()[1]) - (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiFlatPoint {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiFlatPoint {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[3], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group1()[0]),
                    (self.group0()[1] * other.group1()[0]),
                    (self.group0()[2] * other.group1()[0]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[3]) - (self.group0()[2] * other.group1()[2])),
                (-(self.group0()[0] * other.group1()[3]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[1] * other.group1()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiFlatPoint {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group2()[3]),
            (self.group0()[1] * other.group2()[3]),
            (self.group0()[2] * other.group2()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl InfixAntiWedge for AntiFlector {}
impl AntiWedge<AntiCircleOnOrigin> for AntiFlector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for AntiFlector {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for AntiFlector {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other[e12345])),
            // e1, e2, e3, e5
            (self.group1() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for AntiFlector {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            ((self.group1()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for AntiFlector {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Circle> for AntiFlector {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiFlector {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiFlector {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<CircleAtOrigin> for AntiFlector {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiFlector {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for AntiFlector {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Dipole> for AntiFlector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for AntiFlector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for AntiFlector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleAtOrigin> for AntiFlector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for AntiFlector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for AntiFlector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DualNum> for AntiFlector {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e1, e2, e3, e5
            (self.group1() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for AntiFlector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for AntiFlector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Flector> for AntiFlector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0] * -1.0),
                (self.group0()[3] * other.group1()[1] * -1.0),
                (self.group0()[3] * other.group1()[2] * -1.0),
                ((self.group1()[2] * other.group1()[2]) + (self.group1()[1] * other.group1()[1]) - (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group1()[0])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiFlector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[3]) - (self.group0()[2] * other.group0()[2])),
                (-(self.group0()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[1] * other.group0()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Line> for AntiFlector {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for AntiFlector {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Motor> for AntiFlector {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            ((other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for AntiFlector {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            ((other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MultiVector> for AntiFlector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       22       31        0
    //    simd3        1        3        0
    // Totals...
    // yes simd       23       34        0
    //  no simd       25       40        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other.group9()[0]) + (self.group1()[2] * other.group9()[3]) + (self.group1()[1] * other.group9()[2]) + (self.group1()[0] * other.group9()[1])
                    - (self.group0()[3] * other.group3()[3])
                    - (self.group0()[2] * other.group3()[2])
                    - (self.group0()[0] * other.group3()[0])
                    - (self.group0()[1] * other.group3()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[1]) + (self.group0()[3] * other.group6()[0]) + (self.group0()[1] * other.group7()[2]) - (self.group0()[2] * other.group7()[1])),
                ((self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group6()[1]) - (self.group0()[0] * other.group7()[2]) + (self.group0()[2] * other.group7()[0])),
                ((self.group1()[2] * other.group0()[1]) + (self.group0()[3] * other.group6()[2]) + (self.group0()[0] * other.group7()[1]) - (self.group0()[1] * other.group7()[0])),
                0.0,
            ]),
            // e5
            ((self.group1()[3] * other.group0()[1]) - (self.group0()[2] * other.group6()[2]) - (self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from([
                ((self.group0()[1] * other.group9()[3]) - (self.group0()[2] * other.group9()[2])),
                (-(self.group0()[0] * other.group9()[3]) + (self.group0()[2] * other.group9()[1])),
                ((self.group0()[0] * other.group9()[2]) - (self.group0()[1] * other.group9()[1])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for AntiFlector {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for AntiFlector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for AntiFlector {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for AntiFlector {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for AntiFlector {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group1()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Plane> for AntiFlector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiFlector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Sphere> for AntiFlector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       16        0
    //  no simd        9       19        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other[e4315]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) * -1.0),
                    ((self.group0()[3] * other.group0()[1]) * -1.0),
                    ((self.group0()[3] * other.group0()[2]) * -1.0),
                    ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiFlector {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for AntiFlector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       16        0
    //  no simd        9       19        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) * -1.0),
                    ((self.group0()[3] * other.group0()[1]) * -1.0),
                    ((self.group0()[3] * other.group0()[2]) * -1.0),
                    ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for AntiFlector {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiFlector {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiFlector {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e1, e2, e3, e5
            ((self.group1() * Simd32x4::from(other.group0()[0]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for AntiFlector {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiFlector {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for AntiFlector {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for AntiFlector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                - (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group3()[2]) + (self.group1()[1] * other.group3()[1]) + (self.group1()[0] * other.group3()[0])
                        - (self.group0()[2] * other.group0()[2])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group3()[2]) - (self.group0()[2] * other.group3()[1])),
                (-(self.group0()[0] * other.group3()[2]) + (self.group0()[2] * other.group3()[0])),
                ((self.group0()[0] * other.group3()[1]) - (self.group0()[1] * other.group3()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiFlector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                - (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group2()[2]) + (self.group1()[1] * other.group2()[1]) + (self.group1()[0] * other.group2()[0])
                        - (self.group0()[2] * other.group0()[2])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group2()[2]) - (self.group0()[2] * other.group2()[1])),
                (-(self.group0()[0] * other.group2()[2]) + (self.group0()[2] * other.group2()[0])),
                ((self.group0()[0] * other.group2()[1]) - (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiFlector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0] * -1.0),
                (self.group0()[3] * other.group2()[1] * -1.0),
                (self.group0()[3] * other.group2()[2] * -1.0),
                ((self.group1()[2] * other.group2()[2]) + (self.group1()[1] * other.group2()[1]) - (self.group0()[3] * other.group1()[3]) + (self.group1()[0] * other.group2()[0])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group2()[2]) - (self.group0()[2] * other.group2()[1])),
                (-(self.group0()[0] * other.group2()[2]) + (self.group0()[2] * other.group2()[0])),
                ((self.group0()[0] * other.group2()[1]) - (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiFlector {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            ((self.group1()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiFlector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other.group1()[0]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                - (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[3], other.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group1()[3]) + (self.group1()[1] * other.group1()[2]) + (self.group1()[0] * other.group1()[1])
                        - (self.group0()[2] * other.group0()[2])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[3]) - (self.group0()[2] * other.group1()[2])),
                (-(self.group0()[0] * other.group1()[3]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[1] * other.group1()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiFlector {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group2()[3]),
            (self.group0()[1] * other.group2()[3]),
            (self.group0()[2] * other.group2()[3]),
            ((self.group1()[3] * other.group2()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl InfixAntiWedge for AntiFlectorOnOrigin {}
impl AntiWedge<AntiScalar> for AntiFlectorOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<Circle> for AntiFlectorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiFlectorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group1()));
    }
}
impl AntiWedge<CircleAtInfinity> for AntiFlectorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiFlectorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group1()));
    }
}
impl AntiWedge<Dipole> for AntiFlectorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group1()[3] * -1.0));
    }
}
impl AntiWedge<DipoleAligningOrigin> for AntiFlectorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleAtInfinity> for AntiFlectorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleOnOrigin> for AntiFlectorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DualNum> for AntiFlectorOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<FlatOrigin> for AntiFlectorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for AntiFlectorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Flector> for AntiFlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[0] * -1.0),
            (self.group0()[0] * other.group1()[1] * -1.0),
            (self.group0()[0] * other.group1()[2] * -1.0),
            ((self.group0()[3] * other.group1()[2]) + (self.group0()[2] * other.group1()[1]) - (self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiFlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[1] * -1.0),
            (self.group0()[0] * other.group0()[2] * -1.0),
            (self.group0()[0] * other.group0()[3] * -1.0),
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Line> for AntiFlectorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<LineOnOrigin> for AntiFlectorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<Motor> for AntiFlectorOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
            ((self.group0()[0] * other.group0()[1]) + (self.group0()[2] * other.group0()[3])),
            ((self.group0()[0] * other.group0()[2]) + (self.group0()[3] * other.group0()[3])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiFlectorOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
            ((self.group0()[0] * other.group0()[1]) + (self.group0()[2] * other.group0()[3])),
            ((self.group0()[0] * other.group0()[2]) + (self.group0()[3] * other.group0()[3])),
        ]));
    }
}
impl AntiWedge<MultiVector> for AntiFlectorOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd3        0        2        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        6       17        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other.group9()[3]) + (self.group0()[2] * other.group9()[2]) - (self.group0()[0] * other.group3()[3]) + (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group6()[0]) + (self.group0()[1] * other.group0()[1])),
                ((self.group0()[0] * other.group6()[1]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group6()[2]) + (self.group0()[3] * other.group0()[1])),
                0.0,
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]) * Simd32x3::from(-1.0)),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[1])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for AntiFlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0] * -1.0),
            (self.group0()[0] * other.group0()[1] * -1.0),
            (self.group0()[0] * other.group0()[2] * -1.0),
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiFlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0] * -1.0),
            (self.group0()[0] * other.group0()[1] * -1.0),
            (self.group0()[0] * other.group0()[2] * -1.0),
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Sphere> for AntiFlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0] * -1.0),
            (self.group0()[0] * other.group0()[1] * -1.0),
            (self.group0()[0] * other.group0()[2] * -1.0),
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<SphereOnOrigin> for AntiFlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0] * -1.0),
            (self.group0()[0] * other.group0()[1] * -1.0),
            (self.group0()[0] * other.group0()[2] * -1.0),
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorEven> for AntiFlectorOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            ((self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group0()[3])),
            ((self.group0()[0] * other.group1()[1]) + (self.group0()[2] * other.group0()[3])),
            ((self.group0()[0] * other.group1()[2]) + (self.group0()[3] * other.group0()[3])),
        ]));
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiFlectorOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            ((self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group0()[3])),
            ((self.group0()[0] * other.group1()[1]) + (self.group0()[2] * other.group0()[3])),
            ((self.group0()[0] * other.group1()[2]) + (self.group0()[3] * other.group0()[3])),
        ]));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiFlectorOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0]),
            ((self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group0()[0])),
            ((self.group0()[0] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group1()[2]) + (self.group0()[3] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiFlectorOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            ((self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group0()[3])),
            ((self.group0()[0] * other.group1()[1]) + (self.group0()[2] * other.group0()[3])),
            ((self.group0()[0] * other.group1()[2]) + (self.group0()[3] * other.group0()[3])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for AntiFlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group3()[0] * -1.0),
            (self.group0()[0] * other.group3()[1] * -1.0),
            (self.group0()[0] * other.group3()[2] * -1.0),
            ((self.group0()[3] * other.group3()[2]) + (self.group0()[2] * other.group3()[1]) - (self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other.group3()[0])),
        ]));
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiFlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group2()[0] * -1.0),
            (self.group0()[0] * other.group2()[1] * -1.0),
            (self.group0()[0] * other.group2()[2] * -1.0),
            ((self.group0()[3] * other.group2()[2]) + (self.group0()[2] * other.group2()[1]) - (self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other.group2()[0])),
        ]));
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiFlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group2()[0] * -1.0),
            (self.group0()[0] * other.group2()[1] * -1.0),
            (self.group0()[0] * other.group2()[2] * -1.0),
            ((self.group0()[3] * other.group2()[2]) + (self.group0()[2] * other.group2()[1]) - (self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other.group2()[0])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiFlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[1] * -1.0),
            (self.group0()[0] * other.group1()[2] * -1.0),
            (self.group0()[0] * other.group1()[3] * -1.0),
            ((self.group0()[3] * other.group1()[3]) + (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl InfixAntiWedge for AntiLine {}
impl AntiWedge<AntiDipoleOnOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for AntiLine {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other[e12345])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for AntiLine {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other.group1()[3]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Circle> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DualNum> for AntiLine {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for AntiLine {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        9        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group1(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group1()[1]) * -1.0),
                    ((self.group0()[0] * other.group1()[2]) * -1.0),
                    ((self.group0()[1] * other.group1()[0]) * -1.0),
                    ((self.group1()[0] * other.group1()[0]) + (self.group1()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiLine {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        9        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group0(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[2]) * -1.0),
                    ((self.group0()[0] * other.group0()[3]) * -1.0),
                    ((self.group0()[1] * other.group0()[1]) * -1.0),
                    ((self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[2])),
                ])),
        );
    }
}
impl AntiWedge<Line> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for AntiLine {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for AntiLine {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for AntiLine {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       18        0
    //    simd3        0        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       13       24        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group1()[2] * other.group7()[2])
                    - (self.group1()[1] * other.group7()[1])
                    - (self.group1()[0] * other.group7()[0])
                    - (self.group0()[2] * other.group6()[2])
                    - (self.group0()[0] * other.group6()[0])
                    - (self.group0()[1] * other.group6()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[0] * other.group9()[0]) + (self.group0()[1] * other.group9()[3]) - (self.group0()[2] * other.group9()[2])),
                (-(self.group1()[1] * other.group9()[0]) - (self.group0()[0] * other.group9()[3]) + (self.group0()[2] * other.group9()[1])),
                (-(self.group1()[2] * other.group9()[0]) + (self.group0()[0] * other.group9()[2]) - (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e5
            ((self.group1()[2] * other.group9()[3]) + (self.group1()[0] * other.group9()[1]) + (self.group1()[1] * other.group9()[2])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for AntiLine {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other[e1234]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for AntiLine {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other.group0()[3]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<Plane> for AntiLine {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        9        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiLine {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Sphere> for AntiLine {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other[e4315]) - (self.group0()[2] * other.group0()[1])),
                    (-(self.group1()[1] * other[e4315]) - (self.group0()[0] * other.group0()[2])),
                    (-(self.group1()[2] * other[e4315]) - (self.group0()[1] * other.group0()[0])),
                    ((self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiLine {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other.group0()[1]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<SphereOnOrigin> for AntiLine {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[1])),
                    (-(self.group1()[1] * other.group0()[3]) - (self.group0()[0] * other.group0()[2])),
                    (-(self.group1()[2] * other.group0()[3]) - (self.group0()[1] * other.group0()[0])),
                    ((self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEven> for AntiLine {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiLine {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiLine {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiLine {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for AntiLine {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for AntiLine {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group3(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group2()[3]) - (self.group0()[2] * other.group3()[1])),
                    (-(self.group1()[1] * other.group2()[3]) - (self.group0()[0] * other.group3()[2])),
                    (-(self.group1()[2] * other.group2()[3]) - (self.group0()[1] * other.group3()[0])),
                    ((self.group1()[0] * other.group3()[0]) + (self.group1()[1] * other.group3()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiLine {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group2(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group1()[3]) - (self.group0()[2] * other.group2()[1])),
                    (-(self.group1()[1] * other.group1()[3]) - (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[2] * other.group1()[3]) - (self.group0()[1] * other.group2()[0])),
                    ((self.group1()[0] * other.group2()[0]) + (self.group1()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiLine {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        9        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group2(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group2()[1]) * -1.0),
                    ((self.group0()[0] * other.group2()[2]) * -1.0),
                    ((self.group0()[1] * other.group2()[0]) * -1.0),
                    ((self.group1()[0] * other.group2()[0]) + (self.group1()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiLine {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other.group1()[3]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiLine {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group1(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group1()[0]) - (self.group0()[2] * other.group1()[2])),
                    (-(self.group1()[1] * other.group1()[0]) - (self.group0()[0] * other.group1()[3])),
                    (-(self.group1()[2] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    ((self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiLine {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other.group2()[3]) * Simd32x3::from(-1.0)));
    }
}
impl InfixAntiWedge for AntiLineOnOrigin {}
impl AntiWedge<AntiScalar> for AntiLineOnOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group0() * Simd32x3::from(other[e12345])));
    }
}
impl AntiWedge<Circle> for AntiLineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiLineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiLineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiLineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for AntiLineOnOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group0() * Simd32x3::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for AntiLineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group1()[2], other.group1()[0], other.group1()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[0]]))),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiLineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[3], other.group0()[1], other.group0()[2]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[2], other.group0()[3], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<Line> for AntiLineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for AntiLineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for AntiLineOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiLineOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MultiVector> for AntiLineOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        9        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group0()[2] * other.group6()[2]) - (self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[1] * other.group9()[3]) - (self.group0()[2] * other.group9()[2])),
                (-(self.group0()[0] * other.group9()[3]) + (self.group0()[2] * other.group9()[1])),
                ((self.group0()[0] * other.group9()[2]) - (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for AntiLineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiLineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Sphere> for AntiLineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for AntiLineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEven> for AntiLineOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiLineOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiLineOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0]),
            (self.group0()[1] * other.group0()[0]),
            (self.group0()[2] * other.group0()[0]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiLineOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for AntiLineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group3()[2], other.group3()[0], other.group3()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group3()[1], other.group3()[2], other.group3()[0]]))),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiLineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group2()[2], other.group2()[0], other.group2()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group2()[1], other.group2()[2], other.group2()[0]]))),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiLineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group2()[2], other.group2()[0], other.group2()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group2()[1], other.group2()[2], other.group2()[0]]))),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiLineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group1()[3], other.group1()[1], other.group1()[2]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group1()[2], other.group1()[3], other.group1()[1]]))),
        );
    }
}
impl InfixAntiWedge for AntiMotor {}
impl AntiWedge<AntiCircleOnOrigin> for AntiMotor {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for AntiMotor {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (self.group0() * Simd32x4::from(other[e12345])),
            // e15, e25, e35, e3215
            (self.group1() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for AntiMotor {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other.group0()[3]));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for AntiMotor {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group1()[3] * -1.0),
            (-(self.group1()[0] * other.group1()[3]) - (self.group1()[3] * other.group0()[0])),
            (-(self.group1()[1] * other.group1()[3]) - (self.group1()[3] * other.group0()[1])),
            (-(self.group1()[2] * other.group1()[3]) - (self.group1()[3] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for AntiMotor {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            ((self.group1()[3] * other.group1()[0]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Circle> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for AntiMotor {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<CircleOnOrigin> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for AntiMotor {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Dipole> for AntiMotor {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for AntiMotor {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group1()[3]) * other.group0() * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<DipoleAtInfinity> for AntiMotor {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group1()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleAtOrigin> for AntiMotor {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<DipoleOnOrigin> for AntiMotor {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group1()[3]) * other.group0() * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for AntiMotor {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<DualNum> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e15, e25, e35, e3215
            (self.group1() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for AntiMotor {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group1()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for AntiMotor {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group1()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Flector> for AntiMotor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3       16        0
    //  no simd        6       19        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0] * -1.0),
                (self.group1()[3] * other.group1()[1] * -1.0),
                (self.group1()[3] * other.group1()[2] * -1.0),
                0.0,
            ]),
            // e1, e2, e3, e5
            ((swizzle!(other.group1(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group1()[1]) * -1.0),
                    ((self.group0()[0] * other.group1()[2]) * -1.0),
                    ((self.group0()[1] * other.group1()[0]) * -1.0),
                    (-(self.group1()[3] * other.group0()[3]) + (self.group1()[0] * other.group1()[0]) + (self.group1()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiMotor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                (self.group1()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
            // e1, e2, e3, e5
            ((swizzle!(other.group0(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                - (swizzle!(other.group0(), 2, 3, 1, 0) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]))])),
        );
    }
}
impl AntiWedge<Line> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<LineOnOrigin> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Motor> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for AntiMotor {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       19       27        0
    //    simd3        2        6        0
    // Totals...
    // yes simd       21       33        0
    //  no simd       25       45        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other.group1()[3]) - (self.group1()[2] * other.group7()[2]) - (self.group1()[1] * other.group7()[1]) - (self.group1()[0] * other.group7()[0])
                    + (self.group0()[3] * other.group0()[1])
                    - (self.group0()[2] * other.group6()[2])
                    - (self.group0()[0] * other.group6()[0])
                    - (self.group0()[1] * other.group6()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[3] * other.group3()[0]) - (self.group1()[0] * other.group9()[0]) + (self.group0()[1] * other.group9()[3])
                    - (self.group0()[2] * other.group9()[2])),
                (-(self.group1()[3] * other.group3()[1]) - (self.group1()[1] * other.group9()[0]) - (self.group0()[0] * other.group9()[3])
                    + (self.group0()[2] * other.group9()[1])),
                (-(self.group1()[3] * other.group3()[2]) - (self.group1()[2] * other.group9()[0]) + (self.group0()[0] * other.group9()[2])
                    - (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e5
            (-(self.group1()[3] * other.group3()[3]) + (self.group1()[2] * other.group9()[3]) + (self.group1()[0] * other.group9()[1]) + (self.group1()[1] * other.group9()[2])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            ((Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                + (Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])) + (Simd32x3::from(self.group1()[3]) * other.group7())),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group1()[3] * other.group9()[0] * -1.0)]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]) * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            (self.group1()[3] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for AntiMotor {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for AntiMotor {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullSphereAtOrigin> for AntiMotor {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (swizzle!(self.group1(), 3, 0, 1, 2) * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for AntiMotor {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for AntiMotor {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group0()[3] * -1.0),
            (-(self.group1()[0] * other.group0()[3]) - (self.group1()[3] * other.group0()[0])),
            (-(self.group1()[1] * other.group0()[3]) - (self.group1()[3] * other.group0()[1])),
            (-(self.group1()[2] * other.group0()[3]) - (self.group1()[3] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<Origin> for AntiMotor {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for AntiMotor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2       15        0
    //  no simd        5       18        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                0.0,
            ]),
            // e1, e2, e3, e5
            ((swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiMotor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                0.0,
            ]),
            // e1, e2, e3, e5
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for AntiMotor {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other.group0()[3]));
    }
}
impl AntiWedge<RoundPointAtOrigin> for AntiMotor {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other.group0()[0]));
    }
}
impl AntiWedge<Sphere> for AntiMotor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        5       11        0
    //  no simd        8       20        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other[e4315]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            ((swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other[e4315]) - (self.group0()[2] * other.group0()[1])),
                    (-(self.group1()[1] * other[e4315]) - (self.group0()[0] * other.group0()[2])),
                    (-(self.group1()[2] * other[e4315]) - (self.group0()[1] * other.group0()[0])),
                    ((self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiMotor {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (swizzle!(self.group1(), 3, 0, 1, 2) * Simd32x4::from(other.group0()[1]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for AntiMotor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        5       11        0
    //  no simd        8       20        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group1()[3]) * other.group0() * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            ((swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[1])),
                    (-(self.group1()[1] * other.group0()[3]) - (self.group0()[0] * other.group0()[2])),
                    (-(self.group1()[2] * other.group0()[3]) - (self.group0()[1] * other.group0()[0])),
                    ((self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEven> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group3()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group1()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group1()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group1()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group1()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                ((self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[0]) + (self.group1()[3] * other.group1()[0])),
                ((self.group1()[1] * other.group0()[0]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[2])),
                (self.group1()[3] * other.group0()[0]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for AntiMotor {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiMotor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group1()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group1()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for AntiMotor {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            ((self.group1()[3] * other.group2()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for AntiMotor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        6       12        0
    //  no simd       12       24        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group2()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (-(Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + (swizzle!(other.group3(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group2()[3]) - (self.group0()[2] * other.group3()[1])),
                    (-(self.group1()[1] * other.group2()[3]) - (self.group0()[0] * other.group3()[2])),
                    (-(self.group1()[2] * other.group2()[3]) - (self.group0()[1] * other.group3()[0])),
                    ((self.group1()[0] * other.group3()[0]) + (self.group1()[1] * other.group3()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiMotor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        6       12        0
    //  no simd       12       24        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (-(Simd32x4::from(self.group1()[3]) * other.group0())
                + (swizzle!(other.group2(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group1()[3]) - (self.group0()[2] * other.group2()[1])),
                    (-(self.group1()[1] * other.group1()[3]) - (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[2] * other.group1()[3]) - (self.group0()[1] * other.group2()[0])),
                    ((self.group1()[0] * other.group2()[0]) + (self.group1()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiMotor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3       16        0
    //  no simd        6       19        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group1()[3] * other.group2()[0] * -1.0),
                (self.group1()[3] * other.group2()[1] * -1.0),
                (self.group1()[3] * other.group2()[2] * -1.0),
                0.0,
            ]),
            // e1, e2, e3, e5
            ((swizzle!(other.group2(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group2()[1]) * -1.0),
                    ((self.group0()[0] * other.group2()[2]) * -1.0),
                    ((self.group0()[1] * other.group2()[0]) * -1.0),
                    (-(self.group1()[3] * other.group1()[3]) + (self.group1()[0] * other.group2()[0]) + (self.group1()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiMotor {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group1()[3] * -1.0),
            (-(self.group1()[0] * other.group1()[3]) - (self.group1()[3] * other.group0()[0])),
            (-(self.group1()[1] * other.group1()[3]) - (self.group1()[3] * other.group0()[1])),
            (-(self.group1()[2] * other.group1()[3]) - (self.group1()[3] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiMotor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        6       12        0
    //  no simd       12       24        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group1()[3]) * swizzle!(other.group1(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (-(Simd32x4::from(self.group1()[3]) * other.group0())
                + (swizzle!(other.group1(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group1()[0]) - (self.group0()[2] * other.group1()[2])),
                    (-(self.group1()[1] * other.group1()[0]) - (self.group0()[0] * other.group1()[3])),
                    (-(self.group1()[2] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    ((self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiMotor {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group2()[3] * -1.0),
            (-(self.group1()[0] * other.group2()[3]) - (self.group1()[3] * other.group0()[0])),
            (-(self.group1()[1] * other.group2()[3]) - (self.group1()[3] * other.group0()[1])),
            (-(self.group1()[2] * other.group2()[3]) - (self.group1()[3] * other.group0()[2])),
        ]));
    }
}
impl InfixAntiWedge for AntiMotorOnOrigin {}
impl AntiWedge<AntiScalar> for AntiMotorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<Circle> for AntiMotorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiMotorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiMotorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiMotorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for AntiMotorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for AntiMotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
            (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
            ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiMotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[3]) - (self.group0()[2] * other.group0()[2])),
            (-(self.group0()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Line> for AntiMotorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for AntiMotorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for AntiMotorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiMotorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MultiVector> for AntiMotorOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       10        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        6       11        0
    //  no simd        6       13        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other.group0()[1]) - (self.group0()[2] * other.group6()[2]) - (self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[1] * other.group9()[3]) - (self.group0()[2] * other.group9()[2])),
                (-(self.group0()[0] * other.group9()[3]) + (self.group0()[2] * other.group9()[1])),
                ((self.group0()[0] * other.group9()[2]) - (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for AntiMotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiMotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Sphere> for AntiMotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<SphereOnOrigin> for AntiMotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<VersorEven> for AntiMotorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiMotorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiMotorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0]),
            (self.group0()[1] * other.group0()[0]),
            (self.group0()[2] * other.group0()[0]),
            ((self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiMotorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for AntiMotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group3()[2]) - (self.group0()[2] * other.group3()[1])),
            (-(self.group0()[0] * other.group3()[2]) + (self.group0()[2] * other.group3()[0])),
            ((self.group0()[0] * other.group3()[1]) - (self.group0()[1] * other.group3()[0])),
        ]));
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiMotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group2()[2]) - (self.group0()[2] * other.group2()[1])),
            (-(self.group0()[0] * other.group2()[2]) + (self.group0()[2] * other.group2()[0])),
            ((self.group0()[0] * other.group2()[1]) - (self.group0()[1] * other.group2()[0])),
        ]));
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiMotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group2()[2]) - (self.group0()[2] * other.group2()[1])),
            (-(self.group0()[0] * other.group2()[2]) + (self.group0()[2] * other.group2()[0])),
            ((self.group0()[0] * other.group2()[1]) - (self.group0()[1] * other.group2()[0])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiMotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group1()[3]) - (self.group0()[2] * other.group1()[2])),
            (-(self.group0()[0] * other.group1()[3]) + (self.group0()[2] * other.group1()[1])),
            ((self.group0()[0] * other.group1()[2]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl InfixAntiWedge for AntiPlane {}
impl AntiWedge<AntiScalar> for AntiPlane {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group1()[3]));
    }
}
impl AntiWedge<DualNum> for AntiPlane {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Motor> for AntiPlane {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiPlane {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<MultiVector> for AntiPlane {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other.group9()[0]) + (self.group0()[2] * other.group9()[3]) + (self.group0()[0] * other.group9()[1]) + (self.group0()[1] * other.group9()[2])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                0.0,
            ]),
            // e5
            (self.group0()[3] * other.group0()[1]),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e1234]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<Plane> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other[e4315]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[1]));
    }
}
impl AntiWedge<SphereOnOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for AntiPlane {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiPlane {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiPlane {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiPlane {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorOdd> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group2()[3]) + (self.group0()[2] * other.group3()[2]) + (self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group1()[3]) + (self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group1()[3]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group1()[3]) + (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiPlane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group2()[3]));
    }
}
impl InfixAntiWedge for AntiPlaneOnOrigin {}
impl AntiWedge<AntiScalar> for AntiPlaneOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other[e12345])));
    }
}
impl AntiWedge<DualNum> for AntiPlaneOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for AntiPlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiPlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Motor> for AntiPlaneOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiPlaneOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<MultiVector> for AntiPlaneOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[2] * other.group9()[3]) + (self.group0()[0] * other.group9()[1]) + (self.group0()[1] * other.group9()[2])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                0.0,
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for AntiPlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiPlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for AntiPlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for AntiPlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for AntiPlaneOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiPlaneOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiPlaneOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiPlaneOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorOdd> for AntiPlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group3()[2]) + (self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiPlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiPlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiPlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group1()[3]) + (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
        );
    }
}
impl InfixAntiWedge for AntiScalar {}
impl AntiWedge<AntiCircleOnOrigin> for AntiScalar {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self[e12345]) * other.group0()),
            // e23, e31, e12
            (Simd32x3::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for AntiScalar {
    type Output = AntiDipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiDipoleOnOrigin::from_groups(/* e423, e431, e412, e321 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<AntiDualNum> for AntiScalar {
    type Output = AntiDualNum;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        use crate::elements::*;
        return AntiDualNum::from_groups(/* e3215, scalar */ (Simd32x2::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<AntiFlatOrigin> for AntiScalar {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self[e12345] * other[e321]));
    }
}
impl AntiWedge<AntiFlatPoint> for AntiScalar {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        use crate::elements::*;
        return AntiFlatPoint::from_groups(/* e235, e315, e125, e321 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<AntiFlector> for AntiScalar {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e1, e2, e3, e5
            (Simd32x4::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for AntiScalar {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<AntiLine> for AntiScalar {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e12345]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for AntiScalar {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<AntiMotor> for AntiScalar {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e15, e25, e35, e3215
            (Simd32x4::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for AntiScalar {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<AntiPlane> for AntiScalar {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for AntiScalar {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<AntiScalar> for AntiScalar {
    type Output = AntiScalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiScalar::from_groups(/* e12345 */ (self[e12345] * other[e12345]));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for AntiScalar {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for AntiScalar {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e23, e31, e12, e1234
            (Simd32x4::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for AntiScalar {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e4, e1, e2, e3
            (Simd32x4::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<Circle> for AntiScalar {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        use crate::elements::*;
        return Circle::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self[e12345]) * other.group0()),
            // e415, e425, e435, e321
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e235, e315, e125
            (Simd32x3::from(self[e12345]) * other.group2()),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiScalar {
    type Output = CircleAligningOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return CircleAligningOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self[e12345]) * other.group0()),
            // e415, e425, e435
            (Simd32x3::from(self[e12345]) * other.group1()),
            // e235, e315, e125
            (Simd32x3::from(self[e12345]) * other.group2()),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiScalar {
    type Output = CircleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return CircleAtInfinity::from_groups(
            // e415, e425, e435, e321
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for AntiScalar {
    type Output = CircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return CircleAtOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self[e12345]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiScalar {
    type Output = CircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return CircleOnOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self[e12345]) * other.group0()),
            // e415, e425, e435
            (Simd32x3::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for AntiScalar {
    type Output = CircleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return CircleOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<Dipole> for AntiScalar {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self[e12345]) * other.group0()),
            // e23, e31, e12, e45
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e15, e25, e35
            (Simd32x3::from(self[e12345]) * other.group2()),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for AntiScalar {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for AntiScalar {
    type Output = DipoleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return DipoleAtInfinity::from_groups(
            // e23, e31, e12, e45
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for AntiScalar {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self[e12345]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for AntiScalar {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for AntiScalar {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self[e12345]) * other.group0()),
            // e23, e31, e12
            (Simd32x3::from(self[e12345]) * other.group1()),
            // e15, e25, e35
            (Simd32x3::from(self[e12345]) * other.group2()),
        );
    }
}
impl AntiWedge<DualNum> for AntiScalar {
    type Output = DualNum;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return DualNum::from_groups(/* e5, e12345 */ (Simd32x2::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<FlatOrigin> for AntiScalar {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self[e12345] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for AntiScalar {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        use crate::elements::*;
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<FlatPointAtInfinity> for AntiScalar {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        use crate::elements::*;
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<Flector> for AntiScalar {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        use crate::elements::*;
        return Flector::from_groups(
            // e15, e25, e35, e45
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for AntiScalar {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        use crate::elements::*;
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiScalar {
    type Output = FlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return FlectorOnOrigin::from_groups(/* e45, e4235, e4315, e4125 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<Horizon> for AntiScalar {
    type Output = Horizon;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return Horizon::from_groups(/* e3215 */ (self[e12345] * other[e3215]));
    }
}
impl AntiWedge<Infinity> for AntiScalar {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e12345] * other[e5]));
    }
}
impl AntiWedge<Line> for AntiScalar {
    type Output = Line;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        use crate::elements::*;
        return Line::from_groups(
            // e415, e425, e435
            (Simd32x3::from(self[e12345]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<LineAtInfinity> for AntiScalar {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        use crate::elements::*;
        return LineAtInfinity::from_groups(/* e235, e315, e125 */ (Simd32x3::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<LineOnOrigin> for AntiScalar {
    type Output = LineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        use crate::elements::*;
        return LineOnOrigin::from_groups(/* e415, e425, e435 */ (Simd32x3::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<Motor> for AntiScalar {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return Motor::from_groups(
            // e415, e425, e435, e12345
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e235, e315, e125, e5
            (Simd32x4::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for AntiScalar {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        use crate::elements::*;
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiScalar {
    type Output = MotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MotorOnOrigin::from_groups(/* e415, e425, e435, e12345 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<MultiVector> for AntiScalar {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd2        0        1        0
    //    simd3        0        4        0
    //    simd4        0        4        0
    // Totals...
    // yes simd        0       11        0
    //  no simd        0       32        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            (Simd32x2::from(self[e12345]) * other.group0()),
            // e1, e2, e3, e4
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e5
            (self[e12345] * other[e1]),
            // e41, e42, e43, e45
            (Simd32x4::from(self[e12345]) * other.group3()),
            // e15, e25, e35
            (Simd32x3::from(self[e12345]) * other.group4()),
            // e23, e31, e12
            (Simd32x3::from(self[e12345]) * other.group5()),
            // e415, e425, e435, e321
            (Simd32x4::from(self[e12345]) * other.group6()),
            // e423, e431, e412
            (Simd32x3::from(self[e12345]) * other.group7()),
            // e235, e315, e125
            (Simd32x3::from(self[e12345]) * other.group8()),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self[e12345]) * other.group9()),
            // e3215
            (self[e12345] * other[e45]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for AntiScalar {
    type Output = NullCircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullCircleAtOrigin::from_groups(/* e423, e431, e412 */ (Simd32x3::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for AntiScalar {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (Simd32x3::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<NullSphereAtOrigin> for AntiScalar {
    type Output = NullSphereAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullSphereAtOrigin::from_groups(/* e1234 */ (self[e12345] * other[e1234]));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for AntiScalar {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for AntiScalar {
    type Output = NullVersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullVersorOddAtOrigin::from_groups(/* e41, e42, e43, e1234 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<Origin> for AntiScalar {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e12345] * other[e4]));
    }
}
impl AntiWedge<Plane> for AntiScalar {
    type Output = Plane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        use crate::elements::*;
        return Plane::from_groups(/* e4235, e4315, e4125, e3215 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiScalar {
    type Output = PlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        use crate::elements::*;
        return PlaneOnOrigin::from_groups(/* e4235, e4315, e4125 */ (Simd32x3::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<RoundPoint> for AntiScalar {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self[e12345]) * other.group0()), /* e5 */ (self[e12345] * other[e2]));
    }
}
impl AntiWedge<RoundPointAtOrigin> for AntiScalar {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (Simd32x2::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<Scalar> for AntiScalar {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Scalar) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e12345] * other[scalar]));
    }
}
impl AntiWedge<Sphere> for AntiScalar {
    type Output = Sphere;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Sphere::from_groups(
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e1234
            (self[e12345] * other[e4315]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiScalar {
    type Output = SphereAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return SphereAtOrigin::from_groups(/* e3215, e1234 */ (Simd32x2::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<SphereOnOrigin> for AntiScalar {
    type Output = SphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return SphereOnOrigin::from_groups(/* e4235, e4315, e4125, e1234 */ (Simd32x4::from(self[e12345]) * other.group0()));
    }
}
impl AntiWedge<VersorEven> for AntiScalar {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e415, e425, e435, e321
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e235, e315, e125, e5
            (Simd32x4::from(self[e12345]) * other.group2()),
            // e1, e2, e3, e4
            (Simd32x4::from(self[e12345]) * other.group3()),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiScalar {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e415, e425, e435, e4
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e235, e315, e125, e5
            (Simd32x4::from(self[e12345]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiScalar {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e415, e425, e435, e321
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e235, e315, e125, e5
            (Simd32x4::from(self[e12345]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for AntiScalar {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e235, e315, e125, e5
            (Simd32x4::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiScalar {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e415, e425, e435, e4
            (Simd32x4::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for AntiScalar {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e235, e315, e125, e5
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e1, e2, e3, e4
            (Simd32x4::from(self[e12345]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOdd> for AntiScalar {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e23, e31, e12, e45
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e15, e25, e35, e1234
            (Simd32x4::from(self[e12345]) * other.group2()),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self[e12345]) * other.group3()),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiScalar {
    type Output = VersorOddAligningOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOddAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e15, e25, e35, e1234
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self[e12345]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiScalar {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        use crate::elements::*;
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e23, e31, e12, e45
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self[e12345]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiScalar {
    type Output = VersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOddAtOrigin::from_groups(
            // e41, e42, e43, e3215
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e15, e25, e35, e1234
            (Simd32x4::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiScalar {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self[e12345]) * other.group1()),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiScalar {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self[e12345]) * other.group0()),
            // e23, e31, e12, e3215
            (Simd32x4::from(self[e12345]) * other.group1()),
            // e15, e25, e35, e1234
            (Simd32x4::from(self[e12345]) * other.group2()),
        );
    }
}
impl InfixAntiWedge for AntiSphereOnOrigin {}
impl AntiWedge<AntiDualNum> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[0]));
    }
}
impl AntiWedge<AntiMotor> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group1()[3]));
    }
}
impl AntiWedge<AntiScalar> for AntiSphereOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<DualNum> for AntiSphereOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group1()[3]) + (self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Horizon> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e3215]));
    }
}
impl AntiWedge<Motor> for AntiSphereOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiSphereOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<MultiVector> for AntiSphereOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        4        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        5        0
    //  no simd        3        8        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other[e45]) + (self.group0()[2] * other.group9()[3]) + (self.group0()[0] * other.group9()[1]) + (self.group0()[1] * other.group9()[2])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[0]));
    }
}
impl AntiWedge<SphereOnOrigin> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for AntiSphereOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiSphereOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiSphereOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiSphereOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorOdd> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group3()[3]) + (self.group0()[2] * other.group3()[2]) + (self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group2()[3]) + (self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group2()[3]) + (self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group1()[3]) + (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiSphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group1()[3]));
    }
}
impl InfixAntiWedge for AntiVersorEvenOnOrigin {}
impl AntiWedge<AntiDualNum> for AntiVersorEvenOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[3], self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for AntiVersorEvenOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for AntiVersorEvenOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            ((self.group1()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiLine> for AntiVersorEvenOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group1()));
    }
}
impl AntiWedge<AntiMotor> for AntiVersorEvenOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group1()[3]),
            ((self.group0()[0] * other.group1()[3]) + (self.group1()[3] * other.group1()[0])),
            ((self.group0()[1] * other.group1()[3]) + (self.group1()[3] * other.group1()[1])),
            ((self.group0()[2] * other.group1()[3]) + (self.group1()[3] * other.group1()[2])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for AntiVersorEvenOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other.group0()[3]));
    }
}
impl AntiWedge<AntiScalar> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (self.group0() * Simd32x4::from(other[e12345])),
            // e23, e31, e12, e1234
            (self.group1() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<Circle> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group2()[0]),
                (self.group1()[3] * other.group2()[1]),
                (self.group1()[3] * other.group2()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group2()[0]),
                (self.group1()[3] * other.group2()[1]),
                (self.group1()[3] * other.group2()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group1()[0]),
            (self.group1()[3] * other.group1()[1]),
            (self.group1()[3] * other.group1()[2]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleOnOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group1()[0]),
            (self.group1()[3] * other.group1()[1]),
            (self.group1()[3] * other.group1()[2]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<Dipole> for AntiVersorEvenOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for AntiVersorEvenOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group1()));
    }
}
impl AntiWedge<DipoleOnOrigin> for AntiVersorEvenOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Origin::from_groups(/* e4 */ (self.group1()[3] * other.group0()[3]));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group2()));
    }
}
impl AntiWedge<DualNum> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        1        6        0
    //  no simd        1        9        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group0()[0])),
            ]),
            // e23, e31, e12, e1234
            (self.group1() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for AntiVersorEvenOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group1()[3] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for AntiVersorEvenOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self.group1()[3]) * other.group0()));
    }
}
impl AntiWedge<FlatPointAtInfinity> for AntiVersorEvenOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group0()));
    }
}
impl AntiWedge<Flector> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group1()[3]) * other.group1()),
            // e4, e1, e2, e3
            ((Simd32x4::from(self.group1()[3]) * swizzle!(other.group0(), 3, 0, 1, 2))
                - (swizzle!(other.group1(), 2, 1, 2, 0) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    ((self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group1()[2])),
                    ((self.group1()[2] * other.group1()[0]) + (self.group0()[1] * other.group1()[3])),
                    ((self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for AntiVersorEvenOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group0()[3]),
            ((self.group0()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (self.group1()[3] * other.group0()[3]),
                0.0,
            ]),
            // e4, e1, e2, e3
            ((swizzle!(self.group1(), 3, 1, 2, 0) * swizzle!(other.group0(), 0, 3, 1, 2))
                - (swizzle!(other.group0(), 3, 2, 3, 1) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([(-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<Horizon> for AntiVersorEvenOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group1()[3], self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<Infinity> for AntiVersorEvenOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<LineAtInfinity> for AntiVersorEvenOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<Motor> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group1()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group1()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for AntiVersorEvenOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            ((self.group1()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group1()[3] * other.group0()[0]),
                    (self.group1()[3] * other.group0()[1]),
                    (self.group1()[3] * other.group0()[2]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e1234
            (self.group1() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for AntiVersorEvenOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       24        0
    //    simd3        1        3        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       17       29        0
    //  no simd       25       41        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other[e1]) - (self.group1()[2] * other.group6()[2]) - (self.group1()[1] * other.group6()[1]) - (self.group1()[0] * other.group6()[0])
                    + (self.group0()[3] * other.group0()[1])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group4()[0], other.group4()[1], other.group4()[2], other.group3()[3]]))
                - (swizzle!(other.group9(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other[e45]) + (self.group1()[1] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[1]) + (self.group0()[1] * other[e45])),
                    ((self.group0()[2] * other[e45]) + (self.group1()[0] * other.group9()[2])),
                    (-(self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
                ])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[1]) + (self.group1()[3] * other.group6()[0])),
                ((self.group0()[1] * other.group0()[1]) + (self.group1()[3] * other.group6()[1])),
                ((self.group0()[2] * other.group0()[1]) + (self.group1()[3] * other.group6()[2])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            ((Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])) + (Simd32x3::from(self.group1()[3]) * other.group8())),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group1()[3] * other[e45])]),
            // e423, e431, e412
            (Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group1()[3] * other.group0()[1]), 0.0, 0.0, 0.0]),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group1()[3]) * other.group0()),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for AntiVersorEvenOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other[e2]));
    }
}
impl AntiWedge<RoundPointAtOrigin> for AntiVersorEvenOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other.group0()[1]));
    }
}
impl AntiWedge<Sphere> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group1()[3]) * other.group0()),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[3], self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        9        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                ])),
        );
    }
}
impl AntiWedge<VersorEven> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group2()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group2()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group2()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group2()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group2()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group2()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[0]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[0]) + (self.group1()[3] * other.group2()[0])),
                ((self.group1()[1] * other.group0()[0]) + (self.group1()[3] * other.group2()[1])),
                ((self.group1()[2] * other.group0()[0]) + (self.group1()[3] * other.group2()[2])),
                (self.group1()[3] * other.group0()[0]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group1()[0]),
            (self.group1()[3] * other.group1()[1]),
            (self.group1()[3] * other.group1()[2]),
            ((self.group1()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group1()[3] * other.group1()[0]),
                    (self.group1()[3] * other.group1()[1]),
                    (self.group1()[3] * other.group1()[2]),
                    (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) - (self.group1()[0] * other.group1()[0])),
                ])),
            // e23, e31, e12, e1234
            (self.group1() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group1()[0]),
            (self.group1()[3] * other.group1()[1]),
            (self.group1()[3] * other.group1()[2]),
            ((self.group1()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group1()[3]) * other.group3()),
            // e4, e1, e2, e3
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[3], other.group2()[0], other.group2()[1], other.group2()[2]]))
                - (swizzle!(other.group3(), 2, 1, 2, 0) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                    ((self.group0()[0] * other.group3()[3]) + (self.group1()[1] * other.group3()[2])),
                    ((self.group1()[2] * other.group3()[0]) + (self.group0()[1] * other.group3()[3])),
                    ((self.group0()[2] * other.group3()[3]) + (self.group1()[0] * other.group3()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group1()[3]) * other.group2()),
            // e4, e1, e2, e3
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]]))
                - (swizzle!(other.group2(), 2, 1, 2, 0) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                    ((self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                    ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3])),
                    ((self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group1()[3]) * other.group2()),
            // e4, e1, e2, e3
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[3], other.group0()[1], other.group0()[2], other.group0()[3]]))
                - (swizzle!(other.group2(), 2, 1, 2, 0) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                    ((self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                    ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3])),
                    ((self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group0()[3]),
            ((self.group0()[0] * other.group0()[3]) + (self.group1()[3] * other.group1()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group1()[3] * other.group1()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group1()[3] * other.group1()[2])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                (self.group1()[3] * other.group1()[3]),
                0.0,
            ]),
            // e4, e1, e2, e3
            ((swizzle!(self.group1(), 3, 1, 2, 0) * Simd32x4::from([other.group0()[3], other.group1()[3], other.group1()[1], other.group1()[2]]))
                - (swizzle!(other.group1(), 3, 2, 3, 1) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([(-(self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiVersorEvenOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group1()[3]),
            ((self.group0()[0] * other.group1()[3]) + (self.group1()[3] * other.group2()[0])),
            ((self.group0()[1] * other.group1()[3]) + (self.group1()[3] * other.group2()[1])),
            ((self.group0()[2] * other.group1()[3]) + (self.group1()[3] * other.group2()[2])),
        ]));
    }
}
impl InfixAntiWedge for AntiVersorOddOnOrigin {}
impl AntiWedge<AntiDualNum> for AntiVersorOddOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[0]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for AntiVersorOddOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for AntiVersorOddOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiLine> for AntiVersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for AntiVersorOddOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            ((self.group1()[0] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for AntiVersorOddOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other[e12345])),
            // e4, e1, e2, e3
            (self.group1() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<Circle> for AntiVersorOddOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for AntiVersorOddOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group1()[0]),
            (self.group0()[3] * other.group1()[1]),
            (self.group0()[3] * other.group1()[2]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Dipole> for AntiVersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for AntiVersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for AntiVersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for AntiVersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for AntiVersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for AntiVersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for AntiVersorOddOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e4, e1, e2, e3
            (self.group1() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for AntiVersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for AntiVersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for AntiVersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for AntiVersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[3]]))
                + (swizzle!(other.group1(), 1, 2, 0, 2) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group1()[1]) + (self.group1()[1] * other.group1()[0]) + (self.group1()[0] * other.group1()[3])
                        - (self.group0()[2] * other.group0()[2])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) - (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group1()[3]) - (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group1()[3]) - (self.group0()[3] * other.group1()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for AntiVersorOddOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group1()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * swizzle!(other.group0(), 3, 1, 2, 0))
                + (swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group1()[2] * other.group0()[2]) + (self.group1()[1] * other.group0()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Horizon> for AntiVersorOddOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[0]])),
        );
    }
}
impl AntiWedge<Line> for AntiVersorOddOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for AntiVersorOddOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Motor> for AntiVersorOddOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4, e1, e2, e3
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for AntiVersorOddOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4, e1, e2, e3
            ((swizzle!(other.group0(), 3, 0, 1, 2) * Simd32x4::from([self.group1()[0], self.group0()[3], self.group0()[3], self.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (self.group1()[3] * other.group0()[3]),
                ])),
        );
    }
}
impl AntiWedge<MultiVector> for AntiVersorOddOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       23        0
    //    simd3        1        3        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       17       28        0
    //  no simd       25       40        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other.group9()[3]) + (self.group1()[2] * other.group9()[2]) + (self.group1()[1] * other.group9()[1]) + (self.group1()[0] * other[e45])
                    - (self.group0()[3] * other.group3()[3])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 1, 2, 3, 0) * Simd32x4::from(other.group0()[1]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group8()[2], other.group8()[0], other.group8()[1], other.group6()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group6()[0]) + (self.group0()[2] * other.group8()[1])),
                    ((self.group0()[3] * other.group6()[1]) + (self.group0()[0] * other.group8()[2])),
                    ((self.group0()[3] * other.group6()[2]) + (self.group0()[1] * other.group8()[0])),
                    (-(self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
                ])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e423, e431, e412
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for AntiVersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       16        0
    //  no simd        9       19        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[1]) * -1.0),
                    ((self.group1()[2] * other.group0()[1]) + (self.group1()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[0])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) - (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) - (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) - (self.group0()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                ((self.group1()[3] * other.group0()[2]) + (self.group1()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Sphere> for AntiVersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       16        0
    //  no simd        9       19        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[1]) * -1.0),
                    ((self.group1()[2] * other.group0()[1]) + (self.group1()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[0])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) - (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) - (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) - (self.group0()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[0]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2       15        0
    //  no simd        5       18        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[1]) * -1.0),
                    ((self.group1()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for AntiVersorOddOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4, e1, e2, e3
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group1()[2], other.group2()[2], other.group2()[0], other.group2()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4, e1, e2, e3
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group1()[2], other.group2()[2], other.group2()[0], other.group2()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for AntiVersorOddOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e4, e1, e2, e3
            ((self.group1() * Simd32x4::from(other.group0()[0]))
                - (swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group1()[2], other.group2()[2], other.group2()[0], other.group2()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4, e1, e2, e3
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for AntiVersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group1()[3]]))
                + (swizzle!(other.group3(), 1, 2, 0, 2) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group3()[1]) + (self.group1()[1] * other.group3()[0]) + (self.group1()[0] * other.group3()[3])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group3()[3]) - (self.group0()[3] * other.group3()[0])),
                ((self.group0()[1] * other.group3()[3]) - (self.group0()[3] * other.group3()[1])),
                ((self.group0()[2] * other.group3()[3]) - (self.group0()[3] * other.group3()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group0()[3]]))
                + (swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group2()[1]) + (self.group1()[1] * other.group2()[0]) + (self.group1()[0] * other.group2()[3])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) - (self.group0()[3] * other.group2()[0])),
                ((self.group0()[1] * other.group2()[3]) - (self.group0()[3] * other.group2()[1])),
                ((self.group0()[2] * other.group2()[3]) - (self.group0()[3] * other.group2()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for AntiVersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[3]]))
                + (swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group2()[1]) + (self.group1()[1] * other.group2()[0]) + (self.group1()[0] * other.group2()[3])
                        - (self.group0()[2] * other.group0()[3])
                        - (self.group0()[0] * other.group0()[1])
                        - (self.group0()[1] * other.group0()[2])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) - (self.group0()[3] * other.group2()[0])),
                ((self.group0()[1] * other.group2()[3]) - (self.group0()[3] * other.group2()[1])),
                ((self.group0()[2] * other.group2()[3]) - (self.group0()[3] * other.group2()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group1()[0] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group1()[3], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + (swizzle!(other.group1(), 2, 3, 1, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group1()[2] * other.group1()[2]) + (self.group1()[1] * other.group1()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[1] * -1.0),
                (self.group0()[3] * other.group1()[2] * -1.0),
                (self.group0()[3] * other.group1()[3] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for AntiVersorOddOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            ((self.group1()[0] * other.group1()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        ]));
    }
}
impl InfixAntiWedge for Circle {}
impl AntiWedge<AntiCircleOnOrigin> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[0] * other.group1()[0])
                - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for Circle {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDualNum> for Circle {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[0])),
            // e15, e25, e35
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for Circle {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (Simd32x3::from(other[e321]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for Circle {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlector> for Circle {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for Circle {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])),
        );
    }
}
impl AntiWedge<AntiLine> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for Circle {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for Circle {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Circle::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other[e12345])),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other[e12345])),
            // e235, e315, e125
            (self.group2() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for Circle {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group1()[3]),
                (self.group2()[1] * other.group1()[3]),
                (self.group2()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for Circle {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<Circle> for Circle {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       25       30        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1])
                    + (self.group2()[1] * other.group0()[2])
                    + (self.group1()[3] * other.group1()[0])
                    + (self.group1()[0] * other.group1()[3])
                    - (self.group0()[1] * other.group2()[2])
                    + (self.group0()[2] * other.group2()[1])),
                ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2])
                    + (self.group1()[3] * other.group1()[1])
                    + (self.group1()[1] * other.group1()[3])
                    + (self.group0()[0] * other.group2()[2])
                    - (self.group0()[2] * other.group2()[0])),
                (-(self.group2()[1] * other.group0()[0])
                    + (self.group2()[0] * other.group0()[1])
                    + (self.group1()[3] * other.group1()[2])
                    + (self.group1()[2] * other.group1()[3])
                    - (self.group0()[0] * other.group2()[1])
                    + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group1()[2])
                - (self.group2()[1] * other.group1()[1])
                - (self.group2()[0] * other.group1()[0])
                - (self.group1()[2] * other.group2()[2])
                - (self.group1()[0] * other.group2()[0])
                - (self.group1()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for Circle {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       22       27        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group2()[1] * other.group0()[2]) + (self.group1()[3] * other.group1()[0])
                    - (self.group0()[1] * other.group2()[2])
                    + (self.group0()[2] * other.group2()[1])),
                ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2]) + (self.group1()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])
                    - (self.group0()[2] * other.group2()[0])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group2()[0] * other.group0()[1]) + (self.group1()[3] * other.group1()[2])
                    - (self.group0()[0] * other.group2()[1])
                    + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group1()[2])
                - (self.group2()[1] * other.group1()[1])
                - (self.group2()[0] * other.group1()[0])
                - (self.group1()[2] * other.group2()[2])
                - (self.group1()[0] * other.group2()[0])
                - (self.group1()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for Circle {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       16       21        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group1()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                ((self.group1()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[0] * other.group1()[0])
                - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for Circle {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       18        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group2()[1] * other.group0()[2]) - (self.group0()[1] * other.group1()[2])
                    + (self.group0()[2] * other.group1()[1])),
                ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group2()[0] * other.group0()[1]) - (self.group0()[0] * other.group1()[1])
                    + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for Circle {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       18        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[3] * other.group1()[0]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[3] * other.group1()[2]) + (self.group2()[0] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for Circle {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       13       18        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2])
                        + (self.group0()[2] * other.group1()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])
                        - (self.group0()[2] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1])
                        + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<Dipole> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       10        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[3] * other.group1()[3])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[3] * other.group0()[3])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[3] * other.group0()[3])
                - (self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) - (self.group1()[3] * other.group0()[3]) - (self.group2()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8        9        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for Circle {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return Circle::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125
            (self.group2() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for Circle {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       24        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       14       25        0
    //  no simd       17       28        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[3] * other.group0()[3])
                    - (self.group0()[2] * other.group0()[2])
                    - (self.group0()[0] * other.group0()[0])
                    - (self.group0()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group1(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group0()[0] * other.group1()[3]),
                    (self.group0()[1] * other.group1()[3]),
                    (self.group0()[2] * other.group1()[3]),
                    (-(self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group1()[1]) + (self.group1()[0] * other.group1()[3]) + (self.group2()[1] * other.group1()[2])),
                ((self.group2()[2] * other.group1()[0]) + (self.group1()[1] * other.group1()[3]) - (self.group2()[0] * other.group1()[2])),
                (-(self.group2()[1] * other.group1()[0]) + (self.group1()[2] * other.group1()[3]) + (self.group2()[0] * other.group1()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for Circle {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for Circle {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       23        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[2] * other.group0()[2])),
                ((self.group0()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group0()[1])),
                (self.group1()[3] * other.group0()[0] * -1.0),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                (self.group1()[3] * other.group0()[3] * -1.0),
                (-(self.group1()[2] * other.group0()[3]) - (self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[2])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[3]) - (self.group2()[2] * other.group0()[2])),
                (-(self.group2()[0] * other.group0()[3]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[1] * other.group0()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<Horizon> for Circle {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other[e3215])),
            // e15, e25, e35
            (Simd32x3::from(other[e3215]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])),
        );
    }
}
impl AntiWedge<Line> for Circle {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       18        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[3] * other.group0()[0]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group1()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                ((self.group1()[3] * other.group0()[2]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[0] * other.group1()[0])
                - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for Circle {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for Circle {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for Circle {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       24        0
    //    simd4        0        1        0
    // Totals...
    // yes simd       13       25        0
    //  no simd       13       28        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[3] * other.group0()[0]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group1()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                ((self.group1()[3] * other.group0()[2]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for Circle {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group0()[1]),
                    (self.group0()[0] * other.group0()[2]),
                    (self.group0()[1] * other.group0()[0]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for Circle {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       15        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        4       16        0
    //  no simd        4       19        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for Circle {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       38       48        0
    //    simd3        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       43       58        0
    //  no simd       54       80        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group2()[2] * other.group3()[2])
                    - (self.group2()[1] * other.group3()[1])
                    - (self.group2()[0] * other.group3()[0])
                    - (self.group1()[3] * other.group3()[3])
                    - (self.group1()[2] * other.group5()[2])
                    - (self.group1()[1] * other.group5()[1])
                    - (self.group1()[0] * other.group5()[0])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group7()[1])
                    + (self.group2()[1] * other.group7()[2])
                    + (self.group1()[3] * other.group6()[0])
                    + (self.group1()[0] * other.group6()[3])
                    - (self.group0()[1] * other.group8()[2])
                    + (self.group0()[2] * other.group8()[1])),
                ((self.group2()[2] * other.group7()[0]) - (self.group2()[0] * other.group7()[2])
                    + (self.group1()[3] * other.group6()[1])
                    + (self.group1()[1] * other.group6()[3])
                    + (self.group0()[0] * other.group8()[2])
                    - (self.group0()[2] * other.group8()[0])),
                (-(self.group2()[1] * other.group7()[0])
                    + (self.group2()[0] * other.group7()[1])
                    + (self.group1()[3] * other.group6()[2])
                    + (self.group1()[2] * other.group6()[3])
                    - (self.group0()[0] * other.group8()[1])
                    + (self.group0()[1] * other.group8()[0])),
                (-(self.group1()[2] * other.group7()[2])
                    - (self.group1()[1] * other.group7()[1])
                    - (self.group1()[0] * other.group7()[0])
                    - (self.group0()[2] * other.group6()[2])
                    - (self.group0()[0] * other.group6()[0])
                    - (self.group0()[1] * other.group6()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group6()[2])
                - (self.group2()[1] * other.group6()[1])
                - (self.group2()[0] * other.group6()[0])
                - (self.group1()[2] * other.group8()[2])
                - (self.group1()[0] * other.group8()[0])
                - (self.group1()[1] * other.group8()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group9(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group9()[0]) + (self.group0()[2] * other.group9()[2])),
                    ((self.group1()[1] * other.group9()[0]) + (self.group0()[0] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[0]) + (self.group0()[1] * other.group9()[1])),
                    (-(self.group1()[0] * other.group9()[1]) - (self.group1()[1] * other.group9()[2])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group2(), 2, 0, 1) * Simd32x3::from([other.group9()[2], other.group9()[3], other.group9()[1]]))
                + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                + (swizzle!(self.group2(), 1, 2, 0) * Simd32x3::from([other.group9()[3], other.group9()[1], other.group9()[2]]))),
            // e23, e31, e12
            ((self.group2() * Simd32x3::from(other.group9()[0])) + (self.group0() * Simd32x3::from(other[e45]))
                - (Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e235, e315, e125
            (self.group2() * Simd32x3::from(other.group0()[1])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for Circle {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
            (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
            ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for Circle {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for Circle {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(other[e1234]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])),
            // e23, e31, e12
            (self.group2() * Simd32x3::from(other[e1234])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for Circle {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for Circle {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Plane> for Circle {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        3        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5       11        0
    //  no simd       14       24        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group0(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[3]),
                    (self.group0()[2] * other.group0()[3]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group2(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                + (swizzle!(self.group2(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for Circle {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd3        2        4        0
    // Totals...
    // yes simd        4       13        0
    //  no simd        8       21        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            ((swizzle!(self.group2(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group2(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Sphere> for Circle {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd3        4        6        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       15        0
    //  no simd       20       30        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            ((Simd32x3::from(other[e4315]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                - (swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group0(), 0, 1, 2, 2))
                + Simd32x4::from([
                    ((self.group2()[0] * other[e4315]) + (self.group0()[0] * other.group0()[3])),
                    ((self.group2()[1] * other[e4315]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group2()[2] * other[e4315]) + (self.group0()[2] * other.group0()[3])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group2(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                + (swizzle!(self.group2(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for Circle {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        4        0
    // no simd        3       12        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])),
            // e23, e31, e12
            ((self.group0() * Simd32x3::from(other.group0()[0])) + (self.group2() * Simd32x3::from(other.group0()[1]))),
            // e15, e25, e35
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for Circle {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        3        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5       11        0
    //  no simd       14       24        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                - (swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group0(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group2()[0] * other.group0()[3]),
                    (self.group2()[1] * other.group0()[3]),
                    (self.group2()[2] * other.group0()[3]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35
            ((swizzle!(self.group2(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group2(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEven> for Circle {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       21       32        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       22       34        0
    //  no simd       25       40        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[3] * other.group1()[0]) + (self.group1()[0] * other.group1()[3])
                        - (self.group0()[1] * other.group2()[2])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[0])
                        + (self.group1()[3] * other.group1()[1])
                        + (self.group1()[1] * other.group1()[3])
                        + (self.group0()[0] * other.group2()[2])
                        - (self.group0()[2] * other.group2()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[3] * other.group1()[2]) + (self.group1()[2] * other.group1()[3])
                        - (self.group0()[0] * other.group2()[1])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Circle {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       29        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       19       31        0
    //  no simd       22       37        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[3] * other.group1()[0]) - (self.group0()[1] * other.group2()[2])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])
                        - (self.group0()[2] * other.group2()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[3] * other.group1()[2]) - (self.group0()[0] * other.group2()[1])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Circle {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       27        0
    //    simd4        0        1        0
    // Totals...
    // yes simd       16       28        0
    //  no simd       16       31        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other.group0()[0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[0]),
                (self.group2()[1] * other.group0()[0]),
                (self.group2()[2] * other.group0()[0]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) + (self.group1()[0] * other.group1()[3]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[3] * other.group1()[1]) + (self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group1()[3] * other.group1()[2]) + (self.group1()[2] * other.group1()[3]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for Circle {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       13       18        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[1]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Circle {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       10       22        0
    //  no simd       13       28        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group1()[0]) + (self.group2()[1] * other.group0()[2])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1])),
                    ((self.group1()[3] * other.group1()[2]) + (self.group2()[0] * other.group0()[1])),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for Circle {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       13       18        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2])
                        + (self.group0()[2] * other.group1()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])
                        - (self.group0()[2] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1])
                        + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for Circle {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       25       36        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       26       37        0
    //  no simd       29       40        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[0] * other.group2()[3]) - (self.group0()[1] * other.group3()[2]) + (self.group0()[2] * other.group3()[1])),
                ((self.group1()[1] * other.group2()[3]) + (self.group0()[0] * other.group3()[2]) - (self.group0()[2] * other.group3()[0])),
                ((self.group1()[2] * other.group2()[3]) - (self.group0()[0] * other.group3()[1]) + (self.group0()[1] * other.group3()[0])),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[3] * other.group1()[3])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group3(), 0, 1, 2, 2))
                + Simd32x4::from([
                    ((self.group2()[0] * other.group2()[3]) + (self.group0()[0] * other.group3()[3])),
                    ((self.group2()[1] * other.group2()[3]) + (self.group0()[1] * other.group3()[3])),
                    ((self.group2()[2] * other.group2()[3]) + (self.group0()[2] * other.group3()[3])),
                    (-(self.group1()[0] * other.group3()[0]) - (self.group1()[1] * other.group3()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group3()[1]) + (self.group1()[0] * other.group3()[3]) + (self.group2()[1] * other.group3()[2])),
                ((self.group2()[2] * other.group3()[0]) + (self.group1()[1] * other.group3()[3]) - (self.group2()[0] * other.group3()[2])),
                (-(self.group2()[1] * other.group3()[0]) + (self.group1()[2] * other.group3()[3]) + (self.group2()[0] * other.group3()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for Circle {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       22       33        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       23       34        0
    //  no simd       26       37        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[3]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group1()[2] * other.group1()[3]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[3] * other.group0()[3])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group2(), 0, 1, 2, 2))
                + Simd32x4::from([
                    ((self.group2()[0] * other.group1()[3]) + (self.group0()[0] * other.group2()[3])),
                    ((self.group2()[1] * other.group1()[3]) + (self.group0()[1] * other.group2()[3])),
                    ((self.group2()[2] * other.group1()[3]) + (self.group0()[2] * other.group2()[3])),
                    (-(self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group2()[1]) + (self.group1()[0] * other.group2()[3]) + (self.group2()[1] * other.group2()[2])),
                ((self.group2()[2] * other.group2()[0]) + (self.group1()[1] * other.group2()[3]) - (self.group2()[0] * other.group2()[2])),
                (-(self.group2()[1] * other.group2()[0]) + (self.group1()[2] * other.group2()[3]) + (self.group2()[0] * other.group2()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for Circle {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       27        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       17       28        0
    //  no simd       20       31        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[3] * other.group1()[3])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group0()[3])
                    - (self.group0()[0] * other.group0()[1])
                    - (self.group0()[1] * other.group0()[2])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group2(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group0()[0] * other.group2()[3]),
                    (self.group0()[1] * other.group2()[3]),
                    (self.group0()[2] * other.group2()[3]),
                    (-(self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group2()[1]) + (self.group1()[0] * other.group2()[3]) + (self.group2()[1] * other.group2()[2])),
                ((self.group2()[2] * other.group2()[0]) + (self.group1()[1] * other.group2()[3]) - (self.group2()[0] * other.group2()[2])),
                (-(self.group2()[1] * other.group2()[0]) + (self.group1()[2] * other.group2()[3]) + (self.group2()[0] * other.group2()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for Circle {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group2()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group0()[3]) + (self.group2()[1] * other.group1()[3])),
                ((self.group0()[2] * other.group0()[3]) + (self.group2()[2] * other.group1()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for Circle {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       24        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       14       25        0
    //  no simd       17       28        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[3]) + (self.group0()[2] * other.group1()[2])),
                ((self.group1()[1] * other.group1()[0]) + (self.group0()[0] * other.group1()[3]) - (self.group0()[2] * other.group1()[1])),
                ((self.group1()[2] * other.group1()[0]) - (self.group0()[0] * other.group1()[2]) + (self.group0()[1] * other.group1()[1])),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group1()[3] * other.group0()[3])
                    - (self.group2()[0] * other.group0()[0])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group1(), 1, 2, 3, 3))
                + Simd32x4::from([
                    (self.group2()[0] * other.group1()[0]),
                    (self.group2()[1] * other.group1()[0]),
                    (self.group2()[2] * other.group1()[0]),
                    (-(self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group1()[2])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group1()[3]) - (self.group2()[2] * other.group1()[2])),
                (-(self.group2()[0] * other.group1()[3]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[1] * other.group1()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for Circle {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       21        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group2()[3]),
                (self.group1()[1] * other.group2()[3]),
                (self.group1()[2] * other.group2()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) + (self.group2()[0] * other.group2()[3])),
                ((self.group0()[1] * other.group1()[3]) + (self.group2()[1] * other.group2()[3])),
                ((self.group0()[2] * other.group1()[3]) + (self.group2()[2] * other.group2()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl InfixAntiWedge for CircleAligningOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[0] * other.group1()[0])
                - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for CircleAligningOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDualNum> for CircleAligningOrigin {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[0])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for CircleAligningOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other[e321])));
    }
}
impl AntiWedge<AntiFlatPoint> for CircleAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlector> for CircleAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for CircleAligningOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiLine> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for CircleAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for CircleAligningOrigin {
    type Output = CircleAligningOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return CircleAligningOrigin::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other[e12345])),
            // e415, e425, e435
            (self.group1() * Simd32x3::from(other[e12345])),
            // e235, e315, e125
            (self.group2() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for CircleAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group1()[3]),
                (self.group2()[1] * other.group1()[3]),
                (self.group2()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for CircleAligningOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<Circle> for CircleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       22       27        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group1()[3])
                    - (self.group0()[1] * other.group2()[2])
                    + (self.group0()[2] * other.group2()[1])),
                ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2]) + (self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2])
                    - (self.group0()[2] * other.group2()[0])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group1()[3])
                    - (self.group0()[0] * other.group2()[1])
                    + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group1()[2])
                - (self.group2()[1] * other.group1()[1])
                - (self.group2()[0] * other.group1()[0])
                - (self.group1()[2] * other.group2()[2])
                - (self.group1()[0] * other.group2()[0])
                - (self.group1()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for CircleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       19       24        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group2()[1] * other.group0()[2]) - (self.group0()[1] * other.group2()[2])
                    + (self.group0()[2] * other.group2()[1])),
                ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group2()[0] * other.group0()[1]) - (self.group0()[0] * other.group2()[1])
                    + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group1()[2])
                - (self.group2()[1] * other.group1()[1])
                - (self.group2()[0] * other.group1()[0])
                - (self.group1()[2] * other.group2()[2])
                - (self.group1()[0] * other.group2()[0])
                - (self.group1()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for CircleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       18        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                ((self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[0] * other.group1()[0])
                - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for CircleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       18        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group2()[1] * other.group0()[2]) - (self.group0()[1] * other.group1()[2])
                    + (self.group0()[2] * other.group1()[1])),
                ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group2()[0] * other.group0()[1]) - (self.group0()[0] * other.group1()[1])
                    + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for CircleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for CircleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       13       18        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2])
                        + (self.group0()[2] * other.group1()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])
                        - (self.group0()[2] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1])
                        + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<Dipole> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8        9        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8        9        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for CircleAligningOrigin {
    type Output = CircleAligningOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return CircleAligningOrigin::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e415, e425, e435
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e235, e315, e125
            (self.group2() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPoint> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for CircleAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group1()[1]) + (self.group1()[0] * other.group1()[3]) + (self.group2()[1] * other.group1()[2])),
                ((self.group2()[2] * other.group1()[0]) + (self.group1()[1] * other.group1()[3]) - (self.group2()[0] * other.group1()[2])),
                (-(self.group2()[1] * other.group1()[0]) + (self.group1()[2] * other.group1()[3]) + (self.group2()[0] * other.group1()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for CircleAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for CircleAligningOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        1        2        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        8       15        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(other.group0(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group0()[2]),
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[1]),
                    (-(self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[2])),
                ])),
            // e15, e25, e35
            ((swizzle!(self.group2(), 1, 2, 0) * Simd32x3::from([other.group0()[3], other.group0()[1], other.group0()[2]]))
                - (swizzle!(self.group2(), 2, 0, 1) * Simd32x3::from([other.group0()[2], other.group0()[3], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<Horizon> for CircleAligningOrigin {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other[e3215])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other[e3215])),
        );
    }
}
impl AntiWedge<Line> for CircleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[0] * other.group1()[0])
                - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for CircleAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for CircleAligningOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        2        3        0
    // no simd        4        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (-(Simd32x2::from(other.group0()[2]) * Simd32x2::from([self.group0()[2], self.group2()[2]]))
                - (Simd32x2::from(other.group0()[0]) * Simd32x2::from([self.group0()[0], self.group2()[0]]))
                - (Simd32x2::from(other.group0()[1]) * Simd32x2::from([self.group0()[1], self.group2()[1]]))),
        );
    }
}
impl AntiWedge<Motor> for CircleAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       24        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for CircleAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group0()[1]),
                    (self.group0()[0] * other.group0()[2]),
                    (self.group0()[1] * other.group0()[0]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for CircleAligningOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       15        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for CircleAligningOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       34       47        0
    //    simd3        3        7        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       38       55        0
    //  no simd       47       72        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group2()[2] * other.group3()[2])
                    - (self.group2()[1] * other.group3()[1])
                    - (self.group2()[0] * other.group3()[0])
                    - (self.group1()[2] * other.group5()[2])
                    - (self.group1()[1] * other.group5()[1])
                    - (self.group1()[0] * other.group5()[0])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group7()[1]) + (self.group2()[1] * other.group7()[2]) + (self.group1()[0] * other.group6()[3])
                    - (self.group0()[1] * other.group8()[2])
                    + (self.group0()[2] * other.group8()[1])),
                ((self.group2()[2] * other.group7()[0]) - (self.group2()[0] * other.group7()[2]) + (self.group1()[1] * other.group6()[3]) + (self.group0()[0] * other.group8()[2])
                    - (self.group0()[2] * other.group8()[0])),
                (-(self.group2()[1] * other.group7()[0]) + (self.group2()[0] * other.group7()[1]) + (self.group1()[2] * other.group6()[3])
                    - (self.group0()[0] * other.group8()[1])
                    + (self.group0()[1] * other.group8()[0])),
                (-(self.group1()[2] * other.group7()[2])
                    - (self.group1()[1] * other.group7()[1])
                    - (self.group1()[0] * other.group7()[0])
                    - (self.group0()[2] * other.group6()[2])
                    - (self.group0()[0] * other.group6()[0])
                    - (self.group0()[1] * other.group6()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group6()[2])
                - (self.group2()[1] * other.group6()[1])
                - (self.group2()[0] * other.group6()[0])
                - (self.group1()[2] * other.group8()[2])
                - (self.group1()[0] * other.group8()[0])
                - (self.group1()[1] * other.group8()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group9(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group9()[0]) + (self.group0()[2] * other.group9()[2])),
                    ((self.group1()[1] * other.group9()[0]) + (self.group0()[0] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[0]) + (self.group0()[1] * other.group9()[1])),
                    (-(self.group1()[0] * other.group9()[1]) - (self.group1()[1] * other.group9()[2])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group2(), 2, 0, 1) * Simd32x3::from([other.group9()[2], other.group9()[3], other.group9()[1]]))
                + (self.group1() * Simd32x3::from(other[e45]))
                + (swizzle!(self.group2(), 1, 2, 0) * Simd32x3::from([other.group9()[3], other.group9()[1], other.group9()[2]]))),
            // e23, e31, e12
            ((self.group0() * Simd32x3::from(other[e45])) + (self.group2() * Simd32x3::from(other.group9()[0]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[1] * other.group0()[1]),
                (self.group1()[2] * other.group0()[1]),
                0.0,
            ]),
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e235, e315, e125
            (self.group2() * Simd32x3::from(other.group0()[1])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for CircleAligningOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
            (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
            ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for CircleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for CircleAligningOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (self.group1() * Simd32x3::from(other[e1234])),
            // e23, e31, e12
            (self.group2() * Simd32x3::from(other[e1234])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for CircleAligningOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for CircleAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Plane> for CircleAligningOrigin {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        3        5        0
    // Totals...
    // yes simd        5       11        0
    //  no simd       11       21        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (-(swizzle!(self.group2(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (self.group1() * Simd32x3::from(other.group0()[3]))
                + (swizzle!(self.group2(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for CircleAligningOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        9        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        6       11        0
    //  no simd        8       15        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            ((swizzle!(self.group2(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group2(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Sphere> for CircleAligningOrigin {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        9        0
    //    simd3        4        6        0
    // Totals...
    // yes simd        9       15        0
    //  no simd       17       27        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            ((self.group1() * Simd32x3::from(other[e4315])) - (swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group2()[0] * other[e4315])),
                ((self.group0()[1] * other.group0()[3]) + (self.group2()[1] * other[e4315])),
                ((self.group0()[2] * other.group0()[3]) + (self.group2()[2] * other[e4315])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (-(swizzle!(self.group2(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (self.group1() * Simd32x3::from(other.group0()[3]))
                + (swizzle!(self.group2(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for CircleAligningOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        4        0
    // no simd        3       12        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            ((self.group0() * Simd32x3::from(other.group0()[0])) + (self.group2() * Simd32x3::from(other.group0()[1]))),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for CircleAligningOrigin {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        3        5        0
    // Totals...
    // yes simd        5       11        0
    //  no simd       11       21        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            ((self.group1() * Simd32x3::from(other.group0()[3])) - (swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            ((swizzle!(self.group2(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group2(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEven> for CircleAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       32        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       19       33        0
    //  no simd       22       36        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group1()[3]) - (self.group0()[1] * other.group2()[2])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2])
                        - (self.group0()[2] * other.group2()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group1()[3]) - (self.group0()[0] * other.group2()[1])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for CircleAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       29        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       16       30        0
    //  no simd       19       33        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                    ((self.group2()[0] * other.group0()[1]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for CircleAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       27        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[0]),
                (self.group2()[1] * other.group0()[0]),
                (self.group2()[2] * other.group0()[0]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[3]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group1()[2] * other.group1()[3]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for CircleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       13       18        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[1]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for CircleAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       20        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       21        0
    //  no simd       10       24        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for CircleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       13       18        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2])
                        + (self.group0()[2] * other.group1()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])
                        - (self.group0()[2] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1])
                        + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for CircleAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       25       36        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[0] * other.group2()[3]) - (self.group0()[1] * other.group3()[2]) + (self.group0()[2] * other.group3()[1])),
                ((self.group1()[1] * other.group2()[3]) + (self.group0()[0] * other.group3()[2]) - (self.group0()[2] * other.group3()[0])),
                ((self.group1()[2] * other.group2()[3]) - (self.group0()[0] * other.group3()[1]) + (self.group0()[1] * other.group3()[0])),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                ((self.group0()[0] * other.group3()[3]) + (self.group2()[0] * other.group2()[3])),
                ((self.group0()[1] * other.group3()[3]) + (self.group2()[1] * other.group2()[3])),
                ((self.group0()[2] * other.group3()[3]) + (self.group2()[2] * other.group2()[3])),
                (-(self.group1()[2] * other.group3()[2]) - (self.group1()[0] * other.group3()[0]) - (self.group1()[1] * other.group3()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group3()[1]) + (self.group1()[0] * other.group3()[3]) + (self.group2()[1] * other.group3()[2])),
                ((self.group2()[2] * other.group3()[0]) + (self.group1()[1] * other.group3()[3]) - (self.group2()[0] * other.group3()[2])),
                (-(self.group2()[1] * other.group3()[0]) + (self.group1()[2] * other.group3()[3]) + (self.group2()[0] * other.group3()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for CircleAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       22       33        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[3]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group1()[2] * other.group1()[3]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) + (self.group2()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group2()[3]) + (self.group2()[1] * other.group1()[3])),
                ((self.group0()[2] * other.group2()[3]) + (self.group2()[2] * other.group1()[3])),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group2()[1]) + (self.group1()[0] * other.group2()[3]) + (self.group2()[1] * other.group2()[2])),
                ((self.group2()[2] * other.group2()[0]) + (self.group1()[1] * other.group2()[3]) - (self.group2()[0] * other.group2()[2])),
                (-(self.group2()[1] * other.group2()[0]) + (self.group1()[2] * other.group2()[3]) + (self.group2()[0] * other.group2()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for CircleAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       16       27        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group0()[3])
                    - (self.group0()[0] * other.group0()[1])
                    - (self.group0()[1] * other.group0()[2])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group2()[1]) + (self.group1()[0] * other.group2()[3]) + (self.group2()[1] * other.group2()[2])),
                ((self.group2()[2] * other.group2()[0]) + (self.group1()[1] * other.group2()[3]) - (self.group2()[0] * other.group2()[2])),
                (-(self.group2()[1] * other.group2()[0]) + (self.group1()[2] * other.group2()[3]) + (self.group2()[0] * other.group2()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for CircleAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group2()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group0()[3]) + (self.group2()[1] * other.group1()[3])),
                ((self.group0()[2] * other.group0()[3]) + (self.group2()[2] * other.group1()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for CircleAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[3]) + (self.group0()[2] * other.group1()[2])),
                ((self.group1()[1] * other.group1()[0]) + (self.group0()[0] * other.group1()[3]) - (self.group0()[2] * other.group1()[1])),
                ((self.group1()[2] * other.group1()[0]) - (self.group0()[0] * other.group1()[2]) + (self.group0()[1] * other.group1()[1])),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group2()[0] * other.group1()[0]),
                (self.group2()[1] * other.group1()[0]),
                (self.group2()[2] * other.group1()[0]),
                (-(self.group1()[2] * other.group1()[3]) - (self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group1()[2])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group1()[3]) - (self.group2()[2] * other.group1()[2])),
                (-(self.group2()[0] * other.group1()[3]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[1] * other.group1()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for CircleAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       21        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group2()[3]),
                (self.group1()[1] * other.group2()[3]),
                (self.group1()[2] * other.group2()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) + (self.group2()[0] * other.group2()[3])),
                ((self.group0()[1] * other.group1()[3]) + (self.group2()[1] * other.group2()[3])),
                ((self.group0()[2] * other.group1()[3]) + (self.group2()[2] * other.group2()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl InfixAntiWedge for CircleAtInfinity {}
impl AntiWedge<AntiCircleOnOrigin> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for CircleAtInfinity {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDualNum> for CircleAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for CircleAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (Simd32x3::from(other[e321]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for CircleAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for CircleAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for CircleAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiLine> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for CircleAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for CircleAtInfinity {
    type Output = CircleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return CircleAtInfinity::from_groups(
            // e415, e425, e435, e321
            (self.group0() * Simd32x4::from(other[e12345])),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for CircleAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for CircleAtInfinity {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<Circle> for CircleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       16       21        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1])
                    + (self.group1()[1] * other.group0()[2])
                    + (self.group0()[0] * other.group1()[3])
                    + (self.group0()[3] * other.group1()[0])),
                ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group1()[1])),
                (-(self.group1()[1] * other.group0()[0])
                    + (self.group1()[0] * other.group0()[1])
                    + (self.group0()[2] * other.group1()[3])
                    + (self.group0()[3] * other.group1()[2])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for CircleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       18        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group0()[3] * other.group1()[0]) + (self.group1()[1] * other.group0()[2])),
                ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[1]) - (self.group1()[0] * other.group0()[2])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group0()[3] * other.group1()[2]) + (self.group1()[0] * other.group0()[1])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for CircleAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       12        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleAtOrigin> for CircleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for CircleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group0()[3] * other.group1()[0]) + (self.group1()[1] * other.group0()[2])),
                ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[1]) - (self.group1()[0] * other.group0()[2])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group0()[3] * other.group1()[2]) + (self.group1()[0] * other.group0()[1])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for CircleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<Dipole> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[3] * other.group1()[3])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for CircleAtInfinity {
    type Output = CircleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return CircleAtInfinity::from_groups(
            // e415, e425, e435, e321
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Flector> for CircleAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       20        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (self.group0()[3] * other.group0()[3] * -1.0),
                (-(self.group1()[2] * other.group1()[1]) + (self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group1()[2])),
                ((self.group1()[2] * other.group1()[0]) + (self.group0()[1] * other.group1()[3]) - (self.group1()[0] * other.group1()[2])),
                (-(self.group1()[1] * other.group1()[0]) + (self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0] * -1.0),
                (self.group0()[3] * other.group1()[1] * -1.0),
                (self.group0()[3] * other.group1()[2] * -1.0),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for CircleAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for CircleAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       17        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                ((self.group1()[1] * other.group0()[3]) - (self.group1()[2] * other.group0()[2])),
                (-(self.group1()[0] * other.group0()[3]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<Horizon> for CircleAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (Simd32x3::from(other[e3215]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<Line> for CircleAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<LineAtInfinity> for CircleAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for CircleAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Motor> for CircleAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       13        0
    //  no simd        5       16        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
            ]),
            // e415, e425, e435, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for CircleAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for CircleAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2       10        0
    //  no simd        2       13        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
            ]),
            // e415, e425, e435, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for CircleAtInfinity {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       24       34        0
    //    simd3        3        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd       27       41        0
    //  no simd       33       56        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group1()[2] * other.group3()[2])
                    - (self.group1()[1] * other.group3()[1])
                    - (self.group1()[0] * other.group3()[0])
                    - (self.group0()[3] * other.group3()[3])
                    - (self.group0()[2] * other.group5()[2])
                    - (self.group0()[0] * other.group5()[0])
                    - (self.group0()[1] * other.group5()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group7()[1])
                    + (self.group1()[1] * other.group7()[2])
                    + (self.group0()[0] * other.group6()[3])
                    + (self.group0()[3] * other.group6()[0])),
                ((self.group1()[2] * other.group7()[0]) - (self.group1()[0] * other.group7()[2]) + (self.group0()[1] * other.group6()[3]) + (self.group0()[3] * other.group6()[1])),
                (-(self.group1()[1] * other.group7()[0])
                    + (self.group1()[0] * other.group7()[1])
                    + (self.group0()[2] * other.group6()[3])
                    + (self.group0()[3] * other.group6()[2])),
                (-(self.group0()[2] * other.group7()[2]) - (self.group0()[0] * other.group7()[0]) - (self.group0()[1] * other.group7()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group6()[2])
                - (self.group1()[1] * other.group6()[1])
                - (self.group1()[0] * other.group6()[0])
                - (self.group0()[2] * other.group8()[2])
                - (self.group0()[0] * other.group8()[0])
                - (self.group0()[1] * other.group8()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other.group9()[0]),
                (self.group0()[1] * other.group9()[0]),
                (self.group0()[2] * other.group9()[0]),
                (-(self.group0()[2] * other.group9()[3]) - (self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
            ]),
            // e15, e25, e35
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group9()[2], other.group9()[3], other.group9()[1]]))
                + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group9()[3], other.group9()[1], other.group9()[2]]))),
            // e23, e31, e12
            (-(Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]])) + (self.group1() * Simd32x3::from(other.group9()[0]))),
            // e415, e425, e435, e321
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for CircleAtInfinity {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for CircleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for CircleAtInfinity {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(other[e1234]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other[e1234])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for CircleAtInfinity {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for CircleAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Plane> for CircleAtInfinity {
    type Output = DipoleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd3        2        3        0
    // Totals...
    // yes simd        4       12        0
    //  no simd        8       18        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return DipoleAtInfinity::from_groups(
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for CircleAtInfinity {
    type Output = DipoleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        3       11        0
    //  no simd        5       15        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return DipoleAtInfinity::from_groups(
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group1(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Sphere> for CircleAtInfinity {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        2        4        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4       10        0
    //  no simd       11       21        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            (Simd32x3::from(other[e4315]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 3, 3, 3, 2) * swizzle!(other.group0(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group1()[0] * other[e4315]),
                    (self.group1()[1] * other[e4315]),
                    (self.group1()[2] * other[e4315]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for CircleAtInfinity {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e15, e25, e35
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for CircleAtInfinity {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        1        3        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3        9        0
    //  no simd        8       18        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 3, 3, 3, 2) * swizzle!(other.group0(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEven> for CircleAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       16       28        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group1()[1] * other.group0()[2]) + (self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group1()[0])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group1()[1])),
                    ((self.group1()[0] * other.group0()[1]) + (self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group1()[2])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for CircleAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       17        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       10       19        0
    //  no simd       13       25        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[1])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for CircleAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       15        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        8       16        0
    //  no simd        8       19        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group1()[2])),
            ]),
            // e415, e425, e435, e321
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for CircleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4        9        0
    //  no simd        7       12        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for CircleAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       14        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        7       16        0
    //  no simd       10       22        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[1])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for CircleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for CircleAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       24        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       14       25        0
    //  no simd       17       28        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 3, 3, 3, 2) * swizzle!(other.group3(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group1()[0] * other.group2()[3]),
                    (self.group1()[1] * other.group2()[3]),
                    (self.group1()[2] * other.group2()[3]),
                    (-(self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group3()[1]) + (self.group0()[0] * other.group3()[3]) + (self.group1()[1] * other.group3()[2])),
                ((self.group1()[2] * other.group3()[0]) + (self.group0()[1] * other.group3()[3]) - (self.group1()[0] * other.group3()[2])),
                (-(self.group1()[1] * other.group3()[0]) + (self.group0()[2] * other.group3()[3]) + (self.group1()[0] * other.group3()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for CircleAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       21        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       11       22        0
    //  no simd       14       25        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group0()[3] * other.group0()[3])
                    - (self.group1()[0] * other.group0()[0])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 3, 3, 3, 2) * swizzle!(other.group2(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group1()[0] * other.group1()[3]),
                    (self.group1()[1] * other.group1()[3]),
                    (self.group1()[2] * other.group1()[3]),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group2()[1]) + (self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3]) - (self.group1()[0] * other.group2()[2])),
                (-(self.group1()[1] * other.group2()[0]) + (self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for CircleAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       22        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
                (-(self.group1()[2] * other.group2()[1]) + (self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3]) - (self.group1()[0] * other.group2()[2])),
                (-(self.group1()[1] * other.group2()[0]) + (self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0] * -1.0),
                (self.group0()[3] * other.group2()[1] * -1.0),
                (self.group0()[3] * other.group2()[2] * -1.0),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for CircleAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       12        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for CircleAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       19        0
    //  no simd       11       22        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[2] * other.group1()[0]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group0()[3] * other.group0()[3])
                    - (self.group1()[0] * other.group0()[0])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 3, 3, 3, 2) * swizzle!(other.group1(), 1, 2, 3, 3))
                + Simd32x4::from([
                    (self.group1()[0] * other.group1()[0]),
                    (self.group1()[1] * other.group1()[0]),
                    (self.group1()[2] * other.group1()[0]),
                    (-(self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[3]) - (self.group1()[2] * other.group1()[2])),
                (-(self.group1()[0] * other.group1()[3]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[1] * other.group1()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for CircleAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group2()[3]),
                (self.group1()[1] * other.group2()[3]),
                (self.group1()[2] * other.group2()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl InfixAntiWedge for CircleAtOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<AntiDualNum> for CircleAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatPoint> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<AntiFlector> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<AntiLine> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for CircleAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for CircleAtOrigin {
    type Output = CircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return CircleAtOrigin::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other[e12345])),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for CircleAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[0] * other.group1()[3]),
            (self.group1()[1] * other.group1()[3]),
            (self.group1()[2] * other.group1()[3]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<Circle> for CircleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       18        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[1] * other.group2()[2])
                    + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1]) - (self.group0()[0] * other.group2()[1])
                    + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for CircleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       18        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[1] * other.group2()[2])
                    + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1]) - (self.group0()[0] * other.group2()[1])
                    + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for CircleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        3        4        0
    // no simd        9       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group1(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0)) + (swizzle!(self.group1(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1))
                - (swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group1(), 2, 0, 1))
                + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group1(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for CircleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        3        4        0
    // no simd        9       12        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group1(), 2, 0, 1))
                + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group1(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Dipole> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for CircleAtOrigin {
    type Output = CircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return CircleAtOrigin::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPoint> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for CircleAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[2] * other.group1()[1])),
                (-(self.group1()[0] * other.group1()[2]) + (self.group1()[2] * other.group1()[0])),
                ((self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for CircleAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for CircleAtOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        4        0
    // no simd        6       12        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[3], other.group0()[1], other.group0()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[2], other.group0()[3], other.group0()[1]]))),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[3], other.group0()[1], other.group0()[2]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[2], other.group0()[3], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<Horizon> for CircleAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group0() * Simd32x3::from(other[e3215])));
    }
}
impl AntiWedge<Line> for CircleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for CircleAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        2        3        0
    // no simd        4        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (-(Simd32x2::from(other.group0()[2]) * Simd32x2::from([self.group0()[2], self.group1()[2]]))
                - (Simd32x2::from(other.group0()[0]) * Simd32x2::from([self.group0()[0], self.group1()[0]]))
                - (Simd32x2::from(other.group0()[1]) * Simd32x2::from([self.group0()[1], self.group1()[1]]))),
        );
    }
}
impl AntiWedge<Motor> for CircleAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for CircleAtOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for CircleAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       21       30        0
    //    simd3        2        6        0
    // Totals...
    // yes simd       23       36        0
    //  no simd       27       48        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group1()[2] * other.group3()[2])
                    - (self.group1()[1] * other.group3()[1])
                    - (self.group1()[0] * other.group3()[0])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group7()[1]) + (self.group1()[1] * other.group7()[2]) - (self.group0()[1] * other.group8()[2])
                    + (self.group0()[2] * other.group8()[1])),
                ((self.group1()[2] * other.group7()[0]) - (self.group1()[0] * other.group7()[2]) + (self.group0()[0] * other.group8()[2]) - (self.group0()[2] * other.group8()[0])),
                (-(self.group1()[1] * other.group7()[0]) + (self.group1()[0] * other.group7()[1]) - (self.group0()[0] * other.group8()[1])
                    + (self.group0()[1] * other.group8()[0])),
                (-(self.group0()[2] * other.group6()[2]) - (self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group6()[2]) - (self.group1()[0] * other.group6()[0]) - (self.group1()[1] * other.group6()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group9()[3], other.group9()[1], other.group9()[2]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group9()[2], other.group9()[3], other.group9()[1]]))),
            // e23, e31, e12
            ((self.group0() * Simd32x3::from(other[e45])) + (self.group1() * Simd32x3::from(other.group9()[0]))),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group1(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group1(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for CircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for CircleAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group1() * Simd32x3::from(other[e1234])));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for CircleAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[0] * other.group0()[3]),
            (self.group1()[1] * other.group0()[3]),
            (self.group1()[2] * other.group0()[3]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Plane> for CircleAtOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        5        0
    // no simd        6       15        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[3])),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for CircleAtOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        4        0
    // no simd        6       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group1(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Sphere> for CircleAtOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        3        6        0
    // no simd        9       18        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12
            ((self.group0() * Simd32x3::from(other.group0()[3])) + (self.group1() * Simd32x3::from(other[e4315]))),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for CircleAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            ((self.group0() * Simd32x3::from(other.group0()[0])) + (self.group1() * Simd32x3::from(other.group0()[1]))),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for CircleAtOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        5        0
    // no simd        6       15        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other.group0()[3])),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEven> for CircleAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[1] * other.group2()[2])
                    + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1]) - (self.group0()[0] * other.group2()[1])
                    + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for CircleAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[1] * other.group2()[2])
                    + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1]) - (self.group0()[0] * other.group2()[1])
                    + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for CircleAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        3        4        0
    // no simd        9       12        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group1()[2], other.group1()[0], other.group1()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[0]]))),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for CircleAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for CircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        3        4        0
    // no simd        9       12        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group1()[2], other.group1()[0], other.group1()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[0]]))),
        );
    }
}
impl AntiWedge<VersorOdd> for CircleAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       14       24        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group3()[2]) + (self.group0()[2] * other.group3()[1])),
                ((self.group0()[0] * other.group3()[2]) - (self.group0()[2] * other.group3()[0])),
                (-(self.group0()[0] * other.group3()[1]) + (self.group0()[1] * other.group3()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group3()[3]) + (self.group1()[0] * other.group2()[3])),
                ((self.group0()[1] * other.group3()[3]) + (self.group1()[1] * other.group2()[3])),
                ((self.group0()[2] * other.group3()[3]) + (self.group1()[2] * other.group2()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group3()[2]) - (self.group1()[2] * other.group3()[1])),
                (-(self.group1()[0] * other.group3()[2]) + (self.group1()[2] * other.group3()[0])),
                ((self.group1()[0] * other.group3()[1]) - (self.group1()[1] * other.group3()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for CircleAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       14       24        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group2()[3]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[2] * other.group2()[3]) + (self.group1()[2] * other.group1()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[2] * other.group2()[1])),
                (-(self.group1()[0] * other.group2()[2]) + (self.group1()[2] * other.group2()[0])),
                ((self.group1()[0] * other.group2()[1]) - (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for CircleAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[2] * other.group2()[1])),
                (-(self.group1()[0] * other.group2()[2]) + (self.group1()[2] * other.group2()[0])),
                ((self.group1()[0] * other.group2()[1]) - (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for CircleAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       12        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group1()[0] * other.group1()[3])),
            ((self.group0()[1] * other.group0()[3]) + (self.group1()[1] * other.group1()[3])),
            ((self.group0()[2] * other.group0()[3]) + (self.group1()[2] * other.group1()[3])),
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for CircleAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[3]) + (self.group0()[2] * other.group1()[2])),
                ((self.group0()[0] * other.group1()[3]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[1] * other.group1()[1])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[1] * other.group1()[0]),
                (self.group1()[2] * other.group1()[0]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[3]) - (self.group1()[2] * other.group1()[2])),
                (-(self.group1()[0] * other.group1()[3]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[1] * other.group1()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for CircleAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       12        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            ((self.group0()[0] * other.group1()[3]) + (self.group1()[0] * other.group2()[3])),
            ((self.group0()[1] * other.group1()[3]) + (self.group1()[1] * other.group2()[3])),
            ((self.group0()[2] * other.group1()[3]) + (self.group1()[2] * other.group2()[3])),
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        ]));
    }
}
impl InfixAntiWedge for CircleOnOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for CircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group1()[0] * other.group0()[3]),
            (self.group1()[1] * other.group0()[3]),
            (self.group1()[2] * other.group0()[3]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiDualNum> for CircleOnOrigin {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[0])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for CircleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other[e321])));
    }
}
impl AntiWedge<AntiFlatPoint> for CircleOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlector> for CircleOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for CircleOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiLine> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for CircleOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for CircleOnOrigin {
    type Output = CircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return CircleOnOrigin::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other[e12345])),
            // e415, e425, e435
            (self.group1() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for CircleOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for CircleOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group1()[0] * other.group0()[3]),
            (self.group1()[1] * other.group0()[3]),
            (self.group1()[2] * other.group0()[3]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Circle> for CircleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       18        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[3]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group1()[2] * other.group1()[3]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for CircleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for CircleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                ((self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for CircleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for CircleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for CircleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                ((self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<Dipole> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for CircleOnOrigin {
    type Output = CircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return CircleOnOrigin::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e415, e425, e435
            (self.group1() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPoint> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for CircleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for CircleOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for CircleOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for CircleOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(other.group0(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group0()[2]),
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[1]),
                    (-(self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[2])),
                ])),
        );
    }
}
impl AntiWedge<Horizon> for CircleOnOrigin {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other[e3215])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other[e3215])),
        );
    }
}
impl AntiWedge<Line> for CircleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for CircleOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for CircleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for CircleOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for CircleOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group0()[1]),
                    (self.group0()[0] * other.group0()[2]),
                    (self.group0()[1] * other.group0()[0]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for CircleOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for CircleOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       22       35        0
    //    simd3        0        3        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       23       39        0
    //  no simd       26       48        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group1()[2] * other.group5()[2])
                    - (self.group1()[1] * other.group5()[1])
                    - (self.group1()[0] * other.group5()[0])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group6()[3]) - (self.group0()[1] * other.group8()[2]) + (self.group0()[2] * other.group8()[1])),
                ((self.group1()[1] * other.group6()[3]) + (self.group0()[0] * other.group8()[2]) - (self.group0()[2] * other.group8()[0])),
                ((self.group1()[2] * other.group6()[3]) - (self.group0()[0] * other.group8()[1]) + (self.group0()[1] * other.group8()[0])),
                (-(self.group1()[2] * other.group7()[2])
                    - (self.group1()[1] * other.group7()[1])
                    - (self.group1()[0] * other.group7()[0])
                    - (self.group0()[2] * other.group6()[2])
                    - (self.group0()[0] * other.group6()[0])
                    - (self.group0()[1] * other.group6()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group8()[2]) - (self.group1()[0] * other.group8()[0]) - (self.group1()[1] * other.group8()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group9(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group9()[0]) + (self.group0()[2] * other.group9()[2])),
                    ((self.group1()[1] * other.group9()[0]) + (self.group0()[0] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[0]) + (self.group0()[1] * other.group9()[1])),
                    (-(self.group1()[0] * other.group9()[1]) - (self.group1()[1] * other.group9()[2])),
                ])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other[e45])),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other[e45])),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[1] * other.group0()[1]),
                (self.group1()[2] * other.group0()[1]),
                0.0,
            ]),
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for CircleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for CircleOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (self.group1() * Simd32x3::from(other[e1234])));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for CircleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for CircleOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (self.group1() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<Plane> for CircleOnOrigin {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        1        3        0
    // Totals...
    // yes simd        3        9        0
    //  no simd        5       15        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for CircleOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ Simd32x4::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Sphere> for CircleOnOrigin {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        2        4        0
    // Totals...
    // yes simd        4       10        0
    //  no simd        8       18        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            ((self.group1() * Simd32x3::from(other[e4315])) - (swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for CircleOnOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[0])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for CircleOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEven> for CircleOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[3]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group1()[2] * other.group1()[3]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for CircleOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       21        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for CircleOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       21        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[3]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group1()[2] * other.group1()[3]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for CircleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for CircleOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for CircleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                ((self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for CircleOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[0] * other.group2()[3]) - (self.group0()[1] * other.group3()[2]) + (self.group0()[2] * other.group3()[1])),
                ((self.group1()[1] * other.group2()[3]) + (self.group0()[0] * other.group3()[2]) - (self.group0()[2] * other.group3()[0])),
                ((self.group1()[2] * other.group2()[3]) - (self.group0()[0] * other.group3()[1]) + (self.group0()[1] * other.group3()[0])),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group3()[3]),
                (self.group0()[1] * other.group3()[3]),
                (self.group0()[2] * other.group3()[3]),
                (-(self.group1()[2] * other.group3()[2]) - (self.group1()[0] * other.group3()[0]) - (self.group1()[1] * other.group3()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group3()[3]),
                (self.group1()[1] * other.group3()[3]),
                (self.group1()[2] * other.group3()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for CircleOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       21        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[3]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group1()[2] * other.group1()[3]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group2()[3]),
                (self.group1()[1] * other.group2()[3]),
                (self.group1()[2] * other.group2()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for CircleOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       21        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group0()[3])
                    - (self.group0()[0] * other.group0()[1])
                    - (self.group0()[1] * other.group0()[2])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group2()[3]),
                (self.group1()[1] * other.group2()[3]),
                (self.group1()[2] * other.group2()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for CircleOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       12        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for CircleOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(other.group1(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group1()[0]) + (self.group0()[2] * other.group1()[2])),
                    ((self.group1()[1] * other.group1()[0]) + (self.group0()[0] * other.group1()[3])),
                    ((self.group1()[2] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
                    (-(self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group1()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for CircleOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group2()[3]),
                (self.group1()[1] * other.group2()[3]),
                (self.group1()[2] * other.group2()[3]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl InfixAntiWedge for CircleOrthogonalOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<AntiDualNum> for CircleOrthogonalOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiLine> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for CircleOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for CircleOrthogonalOrigin {
    type Output = CircleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return CircleOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other[e12345])),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for CircleOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[0] * other.group1()[3]),
            (self.group1()[1] * other.group1()[3]),
            (self.group1()[2] * other.group1()[3]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<Circle> for CircleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       13       18        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1])
                        + (self.group1()[1] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[0])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[1])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0])
                        + (self.group1()[0] * other.group0()[1])
                        + (self.group0()[3] * other.group1()[2])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for CircleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       13       18        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1])
                        + (self.group1()[1] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[0])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[1])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0])
                        + (self.group1()[0] * other.group0()[1])
                        + (self.group0()[3] * other.group1()[2])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for CircleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        3        4        0
    // no simd        9       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group1(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0)) + (swizzle!(self.group1(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1))
                - (swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for CircleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group0()[3] * other.group1()[0]) + (self.group1()[1] * other.group0()[2])),
                ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[1]) - (self.group1()[0] * other.group0()[2])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group0()[3] * other.group1()[2]) + (self.group1()[0] * other.group0()[1])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        3        4        0
    // no simd        9       12        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Dipole> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[3] * other.group1()[3])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[3] * other.group0()[3])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for CircleOrthogonalOrigin {
    type Output = CircleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return CircleOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for CircleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       12       22        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[1]),
                    (self.group0()[0] * other.group1()[2]),
                    (self.group0()[1] * other.group1()[0]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) - (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group1()[3]) - (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group1()[3]) - (self.group0()[3] * other.group1()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[2] * other.group1()[1])),
                (-(self.group1()[0] * other.group1()[2]) + (self.group1()[2] * other.group1()[0])),
                ((self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for CircleOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for CircleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       20        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[2] * other.group0()[2])),
                ((self.group0()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0] * -1.0),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[3]) - (self.group1()[2] * other.group0()[2])),
                (-(self.group1()[0] * other.group0()[3]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[1] * other.group0()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Horizon> for CircleOrthogonalOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other[e3215]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<Line> for CircleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for CircleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for CircleOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       14        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        7       16        0
    //  no simd       10       22        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for CircleOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        4       13        0
    //  no simd        4       16        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for CircleOrthogonalOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       21       31        0
    //    simd3        3        7        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       25       39        0
    //  no simd       34       56        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group1()[2] * other.group3()[2])
                    - (self.group1()[1] * other.group3()[1])
                    - (self.group1()[0] * other.group3()[0])
                    - (self.group0()[3] * other.group3()[3])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group8()[2], other.group8()[0], other.group8()[1], other.group6()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group7()[1])
                        + (self.group1()[1] * other.group7()[2])
                        + (self.group0()[3] * other.group6()[0])
                        + (self.group0()[2] * other.group8()[1])),
                    ((self.group1()[2] * other.group7()[0]) - (self.group1()[0] * other.group7()[2])
                        + (self.group0()[3] * other.group6()[1])
                        + (self.group0()[0] * other.group8()[2])),
                    (-(self.group1()[1] * other.group7()[0])
                        + (self.group1()[0] * other.group7()[1])
                        + (self.group0()[3] * other.group6()[2])
                        + (self.group0()[1] * other.group8()[0])),
                    (-(self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group6()[2]) - (self.group1()[0] * other.group6()[0]) - (self.group1()[1] * other.group6()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group9()[3], other.group9()[1], other.group9()[2]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group9()[2], other.group9()[3], other.group9()[1]]))),
            // e23, e31, e12
            ((self.group1() * Simd32x3::from(other.group9()[0])) + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e423, e431, e412
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group1(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group1(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for CircleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for CircleOrthogonalOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group1() * Simd32x3::from(other[e1234])));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for CircleOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[0] * other.group0()[3]),
            (self.group1()[1] * other.group0()[3]),
            (self.group1()[2] * other.group0()[3]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Plane> for CircleOrthogonalOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        2        4        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        9       18        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for CircleOrthogonalOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        6        0
    // no simd        6       18        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group1(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Sphere> for CircleOrthogonalOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        3        5        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       21        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            ((self.group1() * Simd32x3::from(other[e4315])) + (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for CircleOrthogonalOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            ((Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])) + (self.group1() * Simd32x3::from(other.group0()[1]))),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for CircleOrthogonalOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        2        4        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        9       18        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            (-(Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])) + (self.group1() * Simd32x3::from(other.group0()[3]))),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEven> for CircleOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       16       28        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1])
                        + (self.group1()[1] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[0])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[1])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0])
                        + (self.group1()[0] * other.group0()[1])
                        + (self.group0()[3] * other.group1()[2])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for CircleOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       16       28        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1])
                        + (self.group1()[1] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[0])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[1])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0])
                        + (self.group1()[0] * other.group0()[1])
                        + (self.group0()[3] * other.group1()[2])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for CircleOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       14        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        7       16        0
    //  no simd       10       22        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        2        2        0
    // Totals...
    // yes simd        5        8        0
    //  no simd        9       12        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + Simd32x3::from([
                    (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                    (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for CircleOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       18        0
    //    simd4        0        1        0
    // Totals...
    // yes simd       10       19        0
    //  no simd       10       22        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group0()[3] * other.group1()[0]) + (self.group1()[1] * other.group0()[2])),
                ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[1]) - (self.group1()[0] * other.group0()[2])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group0()[3] * other.group1()[2]) + (self.group1()[0] * other.group0()[1])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for CircleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        2        2        0
    // Totals...
    // yes simd        5        8        0
    //  no simd        9       12        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + Simd32x3::from([
                    (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                    (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOdd> for CircleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       24        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       15       25        0
    //  no simd       18       28        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group3()[1]),
                    (self.group0()[0] * other.group3()[2]),
                    (self.group0()[1] * other.group3()[0]),
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group2()[3]) + (self.group0()[0] * other.group3()[3]) - (self.group0()[3] * other.group3()[0])),
                ((self.group1()[1] * other.group2()[3]) + (self.group0()[1] * other.group3()[3]) - (self.group0()[3] * other.group3()[1])),
                ((self.group1()[2] * other.group2()[3]) + (self.group0()[2] * other.group3()[3]) - (self.group0()[3] * other.group3()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group3()[2]) - (self.group1()[2] * other.group3()[1])),
                (-(self.group1()[0] * other.group3()[2]) + (self.group1()[2] * other.group3()[0])),
                ((self.group1()[0] * other.group3()[1]) - (self.group1()[1] * other.group3()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for CircleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       24        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       15       25        0
    //  no simd       18       28        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group2()[1]),
                    (self.group0()[0] * other.group2()[2]),
                    (self.group0()[1] * other.group2()[0]),
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[3]) + (self.group0()[0] * other.group2()[3]) - (self.group0()[3] * other.group2()[0])),
                ((self.group1()[1] * other.group1()[3]) + (self.group0()[1] * other.group2()[3]) - (self.group0()[3] * other.group2()[1])),
                ((self.group1()[2] * other.group1()[3]) + (self.group0()[2] * other.group2()[3]) - (self.group0()[3] * other.group2()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[2] * other.group2()[1])),
                (-(self.group1()[0] * other.group2()[2]) + (self.group1()[2] * other.group2()[0])),
                ((self.group1()[0] * other.group2()[1]) - (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for CircleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       12       22        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group2()[1]),
                    (self.group0()[0] * other.group2()[2]),
                    (self.group0()[1] * other.group2()[0]),
                    (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) - (self.group0()[3] * other.group2()[0])),
                ((self.group0()[1] * other.group2()[3]) - (self.group0()[3] * other.group2()[1])),
                ((self.group0()[2] * other.group2()[3]) - (self.group0()[3] * other.group2()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[2] * other.group2()[1])),
                (-(self.group1()[0] * other.group2()[2]) + (self.group1()[2] * other.group2()[0])),
                ((self.group1()[0] * other.group2()[1]) - (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for CircleOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       12        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group1()[0] * other.group1()[3])),
            ((self.group0()[1] * other.group0()[3]) + (self.group1()[1] * other.group1()[3])),
            ((self.group0()[2] * other.group0()[3]) + (self.group1()[2] * other.group1()[3])),
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for CircleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       12       22        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group1()[3], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[2]),
                    (self.group0()[0] * other.group1()[3]),
                    (self.group0()[1] * other.group1()[1]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (-(self.group0()[3] * other.group1()[1]) + (self.group1()[0] * other.group1()[0])),
                (-(self.group0()[3] * other.group1()[2]) + (self.group1()[1] * other.group1()[0])),
                (-(self.group0()[3] * other.group1()[3]) + (self.group1()[2] * other.group1()[0])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[3]) - (self.group1()[2] * other.group1()[2])),
                (-(self.group1()[0] * other.group1()[3]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[1] * other.group1()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for CircleOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       12        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            ((self.group0()[0] * other.group1()[3]) + (self.group1()[0] * other.group2()[3])),
            ((self.group0()[1] * other.group1()[3]) + (self.group1()[1] * other.group2()[3])),
            ((self.group0()[2] * other.group1()[3]) + (self.group1()[2] * other.group2()[3])),
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        ]));
    }
}
impl InfixAntiWedge for Dipole {}
impl AntiWedge<AntiDipoleOnOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) - (self.group1()[3] * other.group0()[3]) - (self.group2()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiDualNum> for Dipole {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other[e321] * -1.0));
    }
}
impl AntiWedge<AntiFlatPoint> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiFlector> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other.group0()[0] * -1.0));
    }
}
impl AntiWedge<AntiMotor> for Dipole {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
        );
    }
}
impl AntiWedge<AntiScalar> for Dipole {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other[e12345])),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other[e12345])),
            // e15, e25, e35
            (self.group2() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for Dipole {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) - (self.group1()[3] * other.group0()[3]) - (self.group2()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<Circle> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       10        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[3] * other.group1()[3])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8        9        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[3] * other.group0()[3])
                - (self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[0] * other.group1()[0])
                - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[3] * other.group0()[3])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for Dipole {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e15, e25, e35
            (self.group2() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       13        0
    //  no simd       11       16        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group1(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group1()[2])),
                    ((self.group1()[2] * other.group1()[0]) + (self.group0()[1] * other.group1()[3])),
                    ((self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group1()[2]) + (self.group2()[1] * other.group1()[1]) + (self.group1()[3] * other.group1()[3]) + (self.group2()[0] * other.group1()[0])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for Dipole {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4        9        0
    //  no simd        7       12        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[1]),
                    (self.group1()[0] * other.group0()[2]),
                    (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
                ])),
            // e5
            ((self.group2()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Horizon> for Dipole {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
        );
    }
}
impl AntiWedge<Line> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for Dipole {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       13        0
    //  no simd        5       16        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for Dipole {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2       10        0
    //  no simd        2       13        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<MultiVector> for Dipole {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       22        0
    //    simd3        0        2        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       18       27        0
    //  no simd       24       40        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group2()[2] * other.group7()[2])
                    - (self.group2()[1] * other.group7()[1])
                    - (self.group2()[0] * other.group7()[0])
                    - (self.group1()[3] * other.group6()[3])
                    - (self.group1()[2] * other.group6()[2])
                    - (self.group1()[1] * other.group6()[1])
                    - (self.group1()[0] * other.group6()[0])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group9()[0]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group9(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other[e45]) + (self.group1()[1] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[1]) + (self.group0()[1] * other[e45])),
                    ((self.group0()[2] * other[e45]) + (self.group1()[0] * other.group9()[2])),
                    (-(self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
                ])),
            // e5
            ((self.group2()[2] * other.group9()[3]) + (self.group2()[1] * other.group9()[2]) + (self.group1()[3] * other[e45]) + (self.group2()[0] * other.group9()[1])),
            // e41, e42, e43, e45
            (Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
            // e15, e25, e35
            (self.group2() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for Dipole {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for Dipole {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Plane> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       13        0
    //  no simd       11       16        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       14        0
    //  no simd       15       20        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(Simd32x4::from(other[e4315]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) - (self.group2()[0] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[0]) - (self.group2()[2] * other.group0()[1])),
                (self.group1()[3] * other.group0()[1] * -1.0),
            ]),
            // e5
            (self.group1()[3] * other.group0()[0]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd       11       16        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for Dipole {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        9       17        0
    //  no simd        9       20        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[3] * other.group1()[3])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Dipole {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       15        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        8       16        0
    //  no simd        8       19        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Dipole {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       13        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        6       14        0
    //  no simd        6       17        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                (-(self.group1()[3] * other.group1()[3])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other.group0()[0])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[0]),
                (self.group2()[1] * other.group0()[0]),
                (self.group2()[2] * other.group0()[0]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Dipole {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       13        0
    //  no simd        5       16        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for Dipole {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[3] * other.group0()[3])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       14        0
    //  no simd       15       20        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group3(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group3()[3]) + (self.group1()[1] * other.group3()[2])),
                    ((self.group1()[2] * other.group3()[0]) + (self.group0()[1] * other.group3()[3])),
                    ((self.group0()[2] * other.group3()[3]) + (self.group1()[0] * other.group3()[1])),
                    (-(self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group3()[2]) + (self.group2()[1] * other.group3()[1]) + (self.group1()[3] * other.group3()[3]) + (self.group2()[0] * other.group3()[0])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       14        0
    //  no simd       15       20        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                    ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3])),
                    ((self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group2()[2]) + (self.group2()[1] * other.group2()[1]) + (self.group1()[3] * other.group2()[3]) + (self.group2()[0] * other.group2()[0])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       13        0
    //  no simd       11       16        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                    ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3])),
                    ((self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group2()[2]) + (self.group2()[1] * other.group2()[1]) + (self.group1()[3] * other.group2()[3]) + (self.group2()[0] * other.group2()[0])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) - (self.group2()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group0()[3]) - (self.group2()[1] * other.group1()[3])),
                ((self.group0()[2] * other.group0()[3]) - (self.group2()[2] * other.group1()[3])),
                (self.group1()[3] * other.group1()[3] * -1.0),
            ]),
            // e5
            (self.group1()[3] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd       11       16        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group1()[0]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group1(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group1()[3]),
                    (self.group1()[2] * other.group1()[1]),
                    (self.group1()[0] * other.group1()[2]),
                    (-(self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
                ])),
            // e5
            ((self.group2()[2] * other.group1()[3]) + (self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for Dipole {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) - (self.group2()[0] * other.group2()[3])),
                ((self.group0()[1] * other.group1()[3]) - (self.group2()[1] * other.group2()[3])),
                ((self.group0()[2] * other.group1()[3]) - (self.group2()[2] * other.group2()[3])),
                (self.group1()[3] * other.group2()[3] * -1.0),
            ]),
            // e5
            (self.group1()[3] * other.group1()[3]),
        );
    }
}
impl InfixAntiWedge for DipoleAligningOrigin {}
impl AntiWedge<AntiDipoleOnOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiDualNum> for DipoleAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e321] * -1.0));
    }
}
impl AntiWedge<AntiFlatPoint> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiFlector> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[0] * -1.0));
    }
}
impl AntiWedge<AntiMotor> for DipoleAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group1()[3])));
    }
}
impl AntiWedge<AntiScalar> for DipoleAligningOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (self.group0() * Simd32x4::from(other[e12345])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for DipoleAligningOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<Circle> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[3] * other.group1()[3])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[3] * other.group0()[3])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for DipoleAligningOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       10        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group1()[2]) + (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group1()[3]) + (self.group1()[0] * other.group1()[0])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for DipoleAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<FlectorOnOrigin> for DipoleAligningOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        6        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ Simd32x2::from([
            (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
            ((self.group1()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<Horizon> for DipoleAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other[e3215])));
    }
}
impl AntiWedge<Line> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for DipoleAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       10        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for DipoleAligningOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for DipoleAligningOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       17        0
    //    simd3        0        1        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       12       20        0
    //  no simd       15       28        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group1()[2] * other.group7()[2])
                    - (self.group1()[1] * other.group7()[1])
                    - (self.group1()[0] * other.group7()[0])
                    - (self.group0()[3] * other.group6()[3])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group9()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other[e45]),
                    (self.group0()[1] * other[e45]),
                    (self.group0()[2] * other[e45]),
                    (-(self.group0()[2] * other.group9()[3]) - (self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
                ])),
            // e5
            ((self.group1()[2] * other.group9()[3]) + (self.group1()[1] * other.group9()[2]) + (self.group0()[3] * other[e45]) + (self.group1()[0] * other.group9()[1])),
            // e41, e42, e43, e45
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for DipoleAligningOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for DipoleAligningOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Plane> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       10        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for DipoleAligningOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ Simd32x2::from([
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Sphere> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       10        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       11        0
    //  no simd        9       14        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(Simd32x4::from(other[e4315]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[3]),
                    (self.group0()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) - (self.group1()[0] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[0]) - (self.group1()[2] * other.group0()[1])),
                (self.group0()[3] * other.group0()[1] * -1.0),
            ]),
            // e5
            (self.group0()[3] * other.group0()[0]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       13        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3] * -1.0),
                (self.group1()[1] * other.group0()[3] * -1.0),
                (self.group1()[2] * other.group0()[3] * -1.0),
                (-(self.group0()[3] * other.group0()[3])
                    - (self.group0()[2] * other.group0()[2])
                    - (self.group0()[0] * other.group0()[0])
                    - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for DipoleAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for DipoleAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       13        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for DipoleAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       11        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                (-(self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[0])]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for DipoleAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       10        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for DipoleAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[3] * other.group0()[3])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       10        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       11        0
    //  no simd        9       14        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group3()[3]),
                    (self.group0()[1] * other.group3()[3]),
                    (self.group0()[2] * other.group3()[3]),
                    (-(self.group0()[2] * other.group3()[2]) - (self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                ])),
            // e5
            ((self.group1()[2] * other.group3()[2]) + (self.group1()[1] * other.group3()[1]) + (self.group0()[3] * other.group3()[3]) + (self.group1()[0] * other.group3()[0])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       10        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       11        0
    //  no simd        9       14        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group2()[3]),
                    (self.group0()[1] * other.group2()[3]),
                    (self.group0()[2] * other.group2()[3]),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e5
            ((self.group1()[2] * other.group2()[2]) + (self.group1()[1] * other.group2()[1]) + (self.group0()[3] * other.group2()[3]) + (self.group1()[0] * other.group2()[0])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       10        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group2()[2]) + (self.group1()[1] * other.group2()[1]) + (self.group0()[3] * other.group2()[3]) + (self.group1()[0] * other.group2()[0])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) - (self.group1()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group0()[3]) - (self.group1()[1] * other.group1()[3])),
                ((self.group0()[2] * other.group0()[3]) - (self.group1()[2] * other.group1()[3])),
                (self.group0()[3] * other.group1()[3] * -1.0),
            ]),
            // e5
            (self.group0()[3] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       13        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0] * -1.0),
                (self.group1()[1] * other.group1()[0] * -1.0),
                (self.group1()[2] * other.group1()[0] * -1.0),
                (-(self.group0()[3] * other.group1()[0])
                    - (self.group0()[2] * other.group1()[3])
                    - (self.group0()[0] * other.group1()[1])
                    - (self.group0()[1] * other.group1()[2])),
            ]),
            // e5
            ((self.group1()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for DipoleAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) - (self.group1()[0] * other.group2()[3])),
                ((self.group0()[1] * other.group1()[3]) - (self.group1()[1] * other.group2()[3])),
                ((self.group0()[2] * other.group1()[3]) - (self.group1()[2] * other.group2()[3])),
                (self.group0()[3] * other.group2()[3] * -1.0),
            ]),
            // e5
            (self.group0()[3] * other.group1()[3]),
        );
    }
}
impl InfixAntiWedge for DipoleAtInfinity {}
impl AntiWedge<AntiDipoleOnOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiDualNum> for DipoleAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other.group0()[0]));
    }
}
impl AntiWedge<AntiFlatOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e321] * -1.0));
    }
}
impl AntiWedge<AntiFlatPoint> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<AntiFlector> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[0] * -1.0));
    }
}
impl AntiWedge<AntiMotor> for DipoleAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other.group1()[3]));
    }
}
impl AntiWedge<AntiScalar> for DipoleAtInfinity {
    type Output = DipoleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return DipoleAtInfinity::from_groups(
            // e23, e31, e12, e45
            (self.group0() * Simd32x4::from(other[e12345])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for DipoleAtInfinity {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<Circle> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[3] * other.group1()[3])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<DualNum> for DipoleAtInfinity {
    type Output = DipoleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return DipoleAtInfinity::from_groups(
            // e23, e31, e12, e45
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for DipoleAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        6       13        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group1(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group1()[1]) * -1.0),
                    ((self.group0()[0] * other.group1()[2]) * -1.0),
                    ((self.group0()[1] * other.group1()[0]) * -1.0),
                    ((self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group1()[3]) + (self.group1()[0] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for DipoleAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<FlectorOnOrigin> for DipoleAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        9        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group0(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[2]) * -1.0),
                    ((self.group0()[0] * other.group0()[3]) * -1.0),
                    ((self.group0()[1] * other.group0()[1]) * -1.0),
                    ((self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[2])),
                ])),
        );
    }
}
impl AntiWedge<Horizon> for DipoleAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other[e3215]));
    }
}
impl AntiWedge<Line> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for DipoleAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for DipoleAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<MultiVector> for DipoleAtInfinity {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       23        0
    //    simd3        0        2        0
    // Totals...
    // yes simd       15       25        0
    //  no simd       15       29        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group1()[2] * other.group7()[2])
                    - (self.group1()[1] * other.group7()[1])
                    - (self.group1()[0] * other.group7()[0])
                    - (self.group0()[3] * other.group6()[3])
                    - (self.group0()[2] * other.group6()[2])
                    - (self.group0()[0] * other.group6()[0])
                    - (self.group0()[1] * other.group6()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[0] * other.group9()[0]) + (self.group0()[1] * other.group9()[3]) - (self.group0()[2] * other.group9()[2])),
                (-(self.group1()[1] * other.group9()[0]) - (self.group0()[0] * other.group9()[3]) + (self.group0()[2] * other.group9()[1])),
                (-(self.group1()[2] * other.group9()[0]) + (self.group0()[0] * other.group9()[2]) - (self.group0()[1] * other.group9()[1])),
                (self.group0()[3] * other.group9()[0] * -1.0),
            ]),
            // e5
            ((self.group1()[2] * other.group9()[3]) + (self.group1()[1] * other.group9()[2]) + (self.group0()[3] * other[e45]) + (self.group1()[0] * other.group9()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for DipoleAtInfinity {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for DipoleAtInfinity {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Plane> for DipoleAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        6       13        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for DipoleAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Sphere> for DipoleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       15        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[0] * other[e4315]) + (self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group1()[1] * other[e4315]) - (self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                (-(self.group1()[2] * other[e4315]) + (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other[e4315] * -1.0),
            ]),
            // e5
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for DipoleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
            // e5
            (self.group0()[3] * other.group0()[0]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for DipoleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       14        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[0] * other.group0()[3]) + (self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group1()[1] * other.group0()[3]) - (self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3] * -1.0),
            ]),
            // e5
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for DipoleAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       10        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        6       11        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for DipoleAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        9        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        5       13        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for DipoleAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        7        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       11        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
            ]),
            // e23, e31, e12, e45
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for DipoleAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        9        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        5       13        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for DipoleAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<VersorOdd> for DipoleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       15        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[0] * other.group2()[3]) + (self.group0()[1] * other.group3()[2]) - (self.group0()[2] * other.group3()[1])),
                (-(self.group1()[1] * other.group2()[3]) - (self.group0()[0] * other.group3()[2]) + (self.group0()[2] * other.group3()[0])),
                (-(self.group1()[2] * other.group2()[3]) + (self.group0()[0] * other.group3()[1]) - (self.group0()[1] * other.group3()[0])),
                (self.group0()[3] * other.group2()[3] * -1.0),
            ]),
            // e5
            ((self.group1()[2] * other.group3()[2]) + (self.group1()[1] * other.group3()[1]) + (self.group0()[3] * other.group3()[3]) + (self.group1()[0] * other.group3()[0])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for DipoleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       15        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[0] * other.group1()[3]) + (self.group0()[1] * other.group2()[2]) - (self.group0()[2] * other.group2()[1])),
                (-(self.group1()[1] * other.group1()[3]) - (self.group0()[0] * other.group2()[2]) + (self.group0()[2] * other.group2()[0])),
                (-(self.group1()[2] * other.group1()[3]) + (self.group0()[0] * other.group2()[1]) - (self.group0()[1] * other.group2()[0])),
                (self.group0()[3] * other.group1()[3] * -1.0),
            ]),
            // e5
            ((self.group1()[2] * other.group2()[2]) + (self.group1()[1] * other.group2()[1]) + (self.group0()[3] * other.group2()[3]) + (self.group1()[0] * other.group2()[0])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for DipoleAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        6       13        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            ((swizzle!(other.group2(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group2()[1]) * -1.0),
                    ((self.group0()[0] * other.group2()[2]) * -1.0),
                    ((self.group0()[1] * other.group2()[0]) * -1.0),
                    ((self.group1()[1] * other.group2()[1]) + (self.group0()[3] * other.group2()[3]) + (self.group1()[0] * other.group2()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for DipoleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
            // e5
            (self.group0()[3] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for DipoleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       14        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[3]) - (self.group0()[2] * other.group1()[2])),
                (-(self.group1()[1] * other.group1()[0]) - (self.group0()[0] * other.group1()[3]) + (self.group0()[2] * other.group1()[1])),
                (-(self.group1()[2] * other.group1()[0]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[1] * other.group1()[1])),
                (self.group0()[3] * other.group1()[0] * -1.0),
            ]),
            // e5
            ((self.group1()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for DipoleAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
            // e5
            (self.group0()[3] * other.group1()[3]),
        );
    }
}
impl InfixAntiWedge for DipoleAtOrigin {}
impl AntiWedge<AntiDipoleOnOrigin> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDualNum> for DipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatPoint> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiFlector> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for DipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group1()[3])));
    }
}
impl AntiWedge<AntiScalar> for DipoleAtOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other[e12345])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for DipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other.group1()[3]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Circle> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for DipoleAtOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for DipoleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group1()[2]) + (self.group1()[0] * other.group1()[0]) + (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for DipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<FlectorOnOrigin> for DipoleAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        6        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ Simd32x2::from([
            (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
            ((self.group1()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<Horizon> for DipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other[e3215])));
    }
}
impl AntiWedge<Line> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for DipoleAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for DipoleAtOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other.group0()[3])),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for DipoleAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       17        0
    //    simd3        0        1        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       12       24        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group1()[2] * other.group7()[2])
                    - (self.group1()[1] * other.group7()[1])
                    - (self.group1()[0] * other.group7()[0])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group9(), 0, 0, 0, 3) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[0] * other[e45]),
                    (self.group0()[1] * other[e45]),
                    (self.group0()[2] * other[e45]),
                    (-(self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
                ])),
            // e5
            ((self.group1()[2] * other.group9()[3]) + (self.group1()[0] * other.group9()[1]) + (self.group1()[1] * other.group9()[2])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                0.0,
            ]),
            // e15, e25, e35
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for DipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other[e1234]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for DipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group1() * Simd32x3::from(other.group0()[3]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<Plane> for DipoleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for DipoleAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ Simd32x2::from([
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Sphere> for DipoleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) - (self.group1()[0] * other[e4315])),
                ((self.group0()[1] * other.group0()[3]) - (self.group1()[1] * other[e4315])),
                ((self.group0()[2] * other.group0()[3]) - (self.group1()[2] * other[e4315])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for DipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((self.group0() * Simd32x3::from(other.group0()[0])) - (self.group1() * Simd32x3::from(other.group0()[1]))),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for DipoleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3] * -1.0),
                (self.group1()[1] * other.group0()[3] * -1.0),
                (self.group1()[2] * other.group0()[3] * -1.0),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for DipoleAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for DipoleAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for DipoleAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for DipoleAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for DipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for DipoleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group3()[3]) - (self.group1()[0] * other.group2()[3])),
                ((self.group0()[1] * other.group3()[3]) - (self.group1()[1] * other.group2()[3])),
                ((self.group0()[2] * other.group3()[3]) - (self.group1()[2] * other.group2()[3])),
                (-(self.group0()[2] * other.group3()[2]) - (self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group3()[2]) + (self.group1()[0] * other.group3()[0]) + (self.group1()[1] * other.group3()[1])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for DipoleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) - (self.group1()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group2()[3]) - (self.group1()[1] * other.group1()[3])),
                ((self.group0()[2] * other.group2()[3]) - (self.group1()[2] * other.group1()[3])),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group2()[2]) + (self.group1()[0] * other.group2()[0]) + (self.group1()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for DipoleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e5
            ((self.group1()[2] * other.group2()[2]) + (self.group1()[0] * other.group2()[0]) + (self.group1()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for DipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((self.group0() * Simd32x3::from(other.group0()[3])) - (self.group1() * Simd32x3::from(other.group1()[3]))),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for DipoleAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0] * -1.0),
                (self.group1()[1] * other.group1()[0] * -1.0),
                (self.group1()[2] * other.group1()[0] * -1.0),
                (-(self.group0()[2] * other.group1()[3]) - (self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
            ]),
            // e5
            ((self.group1()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for DipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((self.group0() * Simd32x3::from(other.group1()[3])) - (self.group1() * Simd32x3::from(other.group2()[3]))),
        );
    }
}
impl InfixAntiWedge for DipoleOnOrigin {}
impl AntiWedge<AntiDipoleOnOrigin> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<AntiDualNum> for DipoleOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatOrigin> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e321] * -1.0));
    }
}
impl AntiWedge<AntiFlatPoint> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiFlector> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[0] * -1.0));
    }
}
impl AntiWedge<AntiMotor> for DipoleOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group1()[3])));
    }
}
impl AntiWedge<AntiScalar> for DipoleOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for DipoleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other.group1()[3] * -1.0));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Circle> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for DipoleOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for DipoleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (self.group0()[3] * other.group1()[3]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for DipoleOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<FlectorOnOrigin> for DipoleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Horizon> for DipoleOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (self.group0() * Simd32x4::from(other[e3215])));
    }
}
impl AntiWedge<Line> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for DipoleOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from(0.0),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for DipoleOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<MultiVector> for DipoleOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       12        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        6       16        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group0()[3] * other.group6()[3])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other[e45]),
                (self.group0()[1] * other[e45]),
                (self.group0()[2] * other[e45]),
                (-(self.group0()[3] * other.group9()[0])
                    - (self.group0()[2] * other.group9()[3])
                    - (self.group0()[0] * other.group9()[1])
                    - (self.group0()[1] * other.group9()[2])),
            ]),
            // e5
            (self.group0()[3] * other[e45]),
            // e41, e42, e43, e45
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for DipoleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other[e1234] * -1.0));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for DipoleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Plane> for DipoleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (self.group0()[3] * other.group0()[3]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for DipoleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for DipoleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[3] * other[e4315]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (self.group0()[3] * other.group0()[3]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for DipoleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from([other.group0()[0], other.group0()[0], other.group0()[0], other.group0()[1]]) * Simd32x4::from([1.0, 1.0, 1.0, -1.0])),
            // e5
            (self.group0()[3] * other.group0()[0]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for DipoleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for DipoleOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from(0.0),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for DipoleOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from(0.0),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for DipoleOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                (-(self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[0])]),
            // e15, e25, e35, e1234
            Simd32x4::from(0.0),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for DipoleOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for DipoleOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for DipoleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group3()[3]),
                (self.group0()[1] * other.group3()[3]),
                (self.group0()[2] * other.group3()[3]),
                (-(self.group0()[3] * other.group2()[3])
                    - (self.group0()[2] * other.group3()[2])
                    - (self.group0()[0] * other.group3()[0])
                    - (self.group0()[1] * other.group3()[1])),
            ]),
            // e5
            (self.group0()[3] * other.group3()[3]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for DipoleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e5
            (self.group0()[3] * other.group2()[3]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for DipoleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e5
            (self.group0()[3] * other.group2()[3]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for DipoleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group1()[3]]) * Simd32x4::from([1.0, 1.0, 1.0, -1.0])),
            // e5
            (self.group0()[3] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for DipoleOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[3] * other.group1()[0]) - (self.group0()[2] * other.group1()[3]) - (self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for DipoleOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group2()[3]]) * Simd32x4::from([1.0, 1.0, 1.0, -1.0])),
            // e5
            (self.group0()[3] * other.group1()[3]),
        );
    }
}
impl InfixAntiWedge for DipoleOrthogonalOrigin {}
impl AntiWedge<AntiDipoleOnOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDualNum> for DipoleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatPoint> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiFlector> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for DipoleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group1()[3])));
    }
}
impl AntiWedge<AntiScalar> for DipoleOrthogonalOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other[e12345])),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other[e12345])),
            // e15, e25, e35
            (self.group2() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for DipoleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group2() * Simd32x3::from(other.group1()[3]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Circle> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8        9        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8        9        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group1()[2] * other.group1()[2])
                - (self.group1()[0] * other.group1()[0])
                - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for DipoleOrthogonalOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e15, e25, e35
            (self.group2() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for DipoleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group1(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group1()[2])),
                    ((self.group1()[2] * other.group1()[0]) + (self.group0()[1] * other.group1()[3])),
                    ((self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group1()[2]) + (self.group2()[0] * other.group1()[0]) + (self.group2()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for DipoleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<FlectorOnOrigin> for DipoleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4        9        0
    //  no simd        7       12        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[1]),
                    (self.group1()[0] * other.group0()[2]),
                    (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
                ])),
            // e5
            ((self.group2()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Horizon> for DipoleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other[e3215])));
    }
}
impl AntiWedge<Line> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for DipoleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for DipoleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       12        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for DipoleOrthogonalOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       23        0
    //    simd3        0        2        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       15       27        0
    //  no simd       21       37        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group2()[2] * other.group7()[2])
                    - (self.group2()[1] * other.group7()[1])
                    - (self.group2()[0] * other.group7()[0])
                    - (self.group1()[2] * other.group6()[2])
                    - (self.group1()[1] * other.group6()[1])
                    - (self.group1()[0] * other.group6()[0])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group9(), 0, 0, 0, 3) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[2]]))
                - (swizzle!(other.group9(), 2, 3, 1, 1) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[0]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other[e45]) + (self.group1()[1] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[1]) + (self.group0()[1] * other[e45])),
                    ((self.group0()[2] * other[e45]) + (self.group1()[0] * other.group9()[2])),
                    ((self.group0()[1] * other.group9()[2]) * -1.0),
                ])),
            // e5
            ((self.group2()[2] * other.group9()[3]) + (self.group2()[0] * other.group9()[1]) + (self.group2()[1] * other.group9()[2])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                0.0,
            ]),
            // e15, e25, e35
            (self.group2() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for DipoleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group2() * Simd32x3::from(other[e1234]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for DipoleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group2() * Simd32x3::from(other.group0()[3]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<Plane> for DipoleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for DipoleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for DipoleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       13       18        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[0] * other[e4315]) + (self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    (-(self.group2()[1] * other[e4315]) + (self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    (-(self.group2()[2] * other[e4315]) + (self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for DipoleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((self.group0() * Simd32x3::from(other.group0()[0])) - (self.group2() * Simd32x3::from(other.group0()[1]))),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for DipoleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        3        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        4        7        0
    //  no simd       10       19        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 3, 3, 3, 2) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[2]]))
                - (swizzle!(other.group0(), 1, 2, 0, 0) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[0]]))
                + (Simd32x4::from([1.0, 1.0, 1.0, -1.0])
                    * swizzle!(other.group0(), 2, 0, 1, 1)
                    * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[0], self.group0()[1]]))),
            // e5
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for DipoleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for DipoleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for DipoleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[0]),
                (self.group2()[1] * other.group0()[0]),
                (self.group2()[2] * other.group0()[0]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for DipoleOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for DipoleOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for DipoleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       13       18        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group3(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[0] * other.group2()[3]) + (self.group0()[0] * other.group3()[3]) + (self.group1()[1] * other.group3()[2])),
                    (-(self.group2()[1] * other.group2()[3]) + (self.group1()[2] * other.group3()[0]) + (self.group0()[1] * other.group3()[3])),
                    (-(self.group2()[2] * other.group2()[3]) + (self.group0()[2] * other.group3()[3]) + (self.group1()[0] * other.group3()[1])),
                    (-(self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group3()[2]) + (self.group2()[0] * other.group3()[0]) + (self.group2()[1] * other.group3()[1])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for DipoleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       13       18        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[0] * other.group1()[3]) + (self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                    (-(self.group2()[1] * other.group1()[3]) + (self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3])),
                    (-(self.group2()[2] * other.group1()[3]) + (self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group2()[2]) + (self.group2()[0] * other.group2()[0]) + (self.group2()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for DipoleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                    ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3])),
                    ((self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e5
            ((self.group2()[2] * other.group2()[2]) + (self.group2()[0] * other.group2()[0]) + (self.group2()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for DipoleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((self.group0() * Simd32x3::from(other.group0()[3])) - (self.group2() * Simd32x3::from(other.group1()[3]))),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for DipoleOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        3        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        4        7        0
    //  no simd       10       19        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group1(), 0, 0, 0, 3) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[2]]))
                - (swizzle!(other.group1(), 2, 3, 1, 1) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[0]]))
                + (Simd32x4::from([1.0, 1.0, 1.0, -1.0])
                    * swizzle!(other.group1(), 3, 1, 2, 2)
                    * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[0], self.group0()[1]]))),
            // e5
            ((self.group2()[2] * other.group1()[3]) + (self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for DipoleOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((self.group0() * Simd32x3::from(other.group1()[3])) - (self.group2() * Simd32x3::from(other.group2()[3]))),
        );
    }
}
impl InfixAntiWedge for DualNum {}
impl AntiWedge<AntiCircleOnOrigin> for DualNum {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for DualNum {
    type Output = AntiDipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiDipoleOnOrigin::from_groups(/* e423, e431, e412, e321 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<AntiDualNum> for DualNum {
    type Output = AntiDualNum;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiDualNum::from_groups(/* e3215, scalar */ (Simd32x2::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<AntiFlatOrigin> for DualNum {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self.group0()[1] * other[e321]));
    }
}
impl AntiWedge<AntiFlatPoint> for DualNum {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiFlatPoint::from_groups(/* e235, e315, e125, e321 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<AntiFlector> for DualNum {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e1, e2, e3, e5
            (Simd32x4::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for DualNum {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<AntiLine> for DualNum {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for DualNum {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<AntiMotor> for DualNum {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e15, e25, e35, e3215
            (Simd32x4::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for DualNum {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<AntiPlane> for DualNum {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for DualNum {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<AntiScalar> for DualNum {
    type Output = DualNum;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return DualNum::from_groups(/* e5, e12345 */ (self.group0() * Simd32x2::from(other[e12345])));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for DualNum {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for DualNum {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        1        6        0
    //  no simd        1        9        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other.group0()[3])),
            ]),
            // e23, e31, e12, e1234
            (Simd32x4::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for DualNum {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e4, e1, e2, e3
            (Simd32x4::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<Circle> for DualNum {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Circle::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group2()),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for DualNum {
    type Output = CircleAligningOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return CircleAligningOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e415, e425, e435
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group2()),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for DualNum {
    type Output = CircleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return CircleAtInfinity::from_groups(
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for DualNum {
    type Output = CircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return CircleAtOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for DualNum {
    type Output = CircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return CircleOnOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e415, e425, e435
            (Simd32x3::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for DualNum {
    type Output = CircleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return CircleOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<Dipole> for DualNum {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group2()),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for DualNum {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for DualNum {
    type Output = DipoleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return DipoleAtInfinity::from_groups(
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for DualNum {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for DualNum {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for DualNum {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group2()),
        );
    }
}
impl AntiWedge<DualNum> for DualNum {
    type Output = DualNum;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        3        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return DualNum::from_groups(/* e5, e12345 */ Simd32x2::from([
            ((self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            (self.group0()[1] * other.group0()[1]),
        ]));
    }
}
impl AntiWedge<FlatOrigin> for DualNum {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self.group0()[1] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for DualNum {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<FlatPointAtInfinity> for DualNum {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<Flector> for DualNum {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for DualNum {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<FlectorOnOrigin> for DualNum {
    type Output = FlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return FlectorOnOrigin::from_groups(/* e45, e4235, e4315, e4125 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<Horizon> for DualNum {
    type Output = Horizon;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return Horizon::from_groups(/* e3215 */ (self.group0()[1] * other[e3215]));
    }
}
impl AntiWedge<Infinity> for DualNum {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[1] * other[e5]));
    }
}
impl AntiWedge<Line> for DualNum {
    type Output = Line;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Line::from_groups(
            // e415, e425, e435
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<LineAtInfinity> for DualNum {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return LineAtInfinity::from_groups(/* e235, e315, e125 */ (Simd32x3::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<LineOnOrigin> for DualNum {
    type Output = LineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return LineOnOrigin::from_groups(/* e415, e425, e435 */ (Simd32x3::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<Motor> for DualNum {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        1        6        0
    //  no simd        1        9        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[1] * other.group1()[1]),
                (self.group0()[1] * other.group1()[2]),
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other.group1()[3])),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for DualNum {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<MotorOnOrigin> for DualNum {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[3])]),
        );
    }
}
impl AntiWedge<MultiVector> for DualNum {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        0        4        0
    //    simd4        0        4        0
    // Totals...
    // yes simd        2       14        0
    //  no simd        2       34        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[0] * other.group9()[0]) + (self.group0()[1] * other.group0()[0])),
                (self.group0()[1] * other.group0()[1]),
            ]),
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e5
            ((self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other[e1])),
            // e41, e42, e43, e45
            (Simd32x4::from(self.group0()[1]) * other.group3()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group4()),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * other.group5()),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[1]) * other.group6()),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group7()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group8()),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[1]) * other.group9()),
            // e3215
            (self.group0()[1] * other[e45]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for DualNum {
    type Output = NullCircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return NullCircleAtOrigin::from_groups(/* e423, e431, e412 */ (Simd32x3::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for DualNum {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (Simd32x3::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<NullSphereAtOrigin> for DualNum {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other[e1234])]),
            // e23, e31, e12, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other[e1234])]),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for DualNum {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for DualNum {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (other.group0() * Simd32x4::from([self.group0()[1], self.group0()[1], self.group0()[1], self.group0()[0]])),
            // e23, e31, e12, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
        );
    }
}
impl AntiWedge<Origin> for DualNum {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group0()[1] * other[e4]));
    }
}
impl AntiWedge<Plane> for DualNum {
    type Output = Plane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Plane::from_groups(/* e4235, e4315, e4125, e3215 */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<PlaneOnOrigin> for DualNum {
    type Output = PlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return PlaneOnOrigin::from_groups(/* e4235, e4315, e4125 */ (Simd32x3::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<RoundPoint> for DualNum {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e5
            (self.group0()[1] * other[e2]),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for DualNum {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (Simd32x2::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<Scalar> for DualNum {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Scalar) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[1] * other[scalar]));
    }
}
impl AntiWedge<Sphere> for DualNum {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        6        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other[e4315])]),
            // e23, e31, e12, e45
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other[e4315])]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[1]) * other.group0()),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for DualNum {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        3        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[1])]),
            // e23, e31, e12, e3215
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[0])]),
            // e15, e25, e35, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[1])]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for DualNum {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        5        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[3])]),
            // e23, e31, e12, e45
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for DualNum {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        3        0
    // Totals...
    // yes simd        1        8        0
    //  no simd        1       17        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[1] * other.group2()[0]),
                (self.group0()[1] * other.group2()[1]),
                (self.group0()[1] * other.group2()[2]),
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other.group2()[3])),
            ]),
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[1]) * other.group3()),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for DualNum {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        1        7        0
    //  no simd        1       13        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e415, e425, e435, e4
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[1] * other.group2()[0]),
                (self.group0()[1] * other.group2()[1]),
                (self.group0()[1] * other.group2()[2]),
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other.group2()[3])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for DualNum {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        1        7        0
    //  no simd        1       13        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[1] * other.group2()[0]),
                (self.group0()[1] * other.group2()[1]),
                (self.group0()[1] * other.group2()[2]),
                ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group2()[3])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for DualNum {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for DualNum {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e415, e425, e435, e4
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[3])]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for DualNum {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[1]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOdd> for DualNum {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        3        0
    // Totals...
    // yes simd        1        8        0
    //  no simd        1       17        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                ((self.group0()[0] * other.group2()[3]) + (self.group0()[1] * other.group0()[3])),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e15, e25, e35, e1234
            (Simd32x4::from(self.group0()[1]) * other.group2()),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[1]) * other.group3()),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for DualNum {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        5        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        7        0
    //  no simd        0       13        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                (self.group0()[0] * other.group1()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e15, e25, e35, e1234
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[1]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for DualNum {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[1]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for DualNum {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        6        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                (self.group0()[0] * other.group1()[3]),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e15, e25, e35, e1234
            (Simd32x4::from(self.group0()[1]) * other.group1()),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for DualNum {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        9        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                (self.group0()[0] * other.group1()[0]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group1()[0])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[1] * other.group1()[1]),
                (self.group0()[1] * other.group1()[2]),
                (self.group0()[1] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for DualNum {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        1        7        0
    //  no simd        1       13        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                ((self.group0()[0] * other.group2()[3]) + (self.group0()[1] * other.group0()[3])),
            ]),
            // e23, e31, e12, e3215
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e15, e25, e35, e1234
            (Simd32x4::from(self.group0()[1]) * other.group2()),
        );
    }
}
impl InfixAntiWedge for FlatOrigin {}
impl AntiWedge<AntiDipoleOnOrigin> for FlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e45] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<AntiDualNum> for FlatOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e45] * other.group0()[0]));
    }
}
impl AntiWedge<AntiFlatOrigin> for FlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e45] * other[e321] * -1.0));
    }
}
impl AntiWedge<AntiFlatPoint> for FlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e45] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<AntiFlector> for FlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e45] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for FlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e45] * other.group0()[0] * -1.0));
    }
}
impl AntiWedge<AntiMotor> for FlatOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e45] * other.group1()[3]));
    }
}
impl AntiWedge<AntiScalar> for FlatOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self[e45] * other[e12345]));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for FlatOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e45] * other.group1()[3] * -1.0));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for FlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e45] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Circle> for FlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e45] * other.group1()[3] * -1.0));
    }
}
impl AntiWedge<CircleAtInfinity> for FlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e45] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for FlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e45] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DualNum> for FlatOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self[e45] * other.group0()[1]));
    }
}
impl AntiWedge<Flector> for FlatOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e45] * other.group1()[3]));
    }
}
impl AntiWedge<FlectorAtInfinity> for FlatOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e45] * other.group0()[3]));
    }
}
impl AntiWedge<Horizon> for FlatOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e45] * other[e3215]));
    }
}
impl AntiWedge<Motor> for FlatOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self[e45] * other.group0()[3]));
    }
}
impl AntiWedge<MotorOnOrigin> for FlatOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self[e45] * other.group0()[3]));
    }
}
impl AntiWedge<MultiVector> for FlatOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        6        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self[e45] * other.group6()[3] * -1.0), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([0.0, 0.0, 0.0, (self[e45] * other.group9()[0] * -1.0)]),
            // e5
            (self[e45] * other[e45]),
            // e41, e42, e43, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self[e45] * other.group0()[1])]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for FlatOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e45] * other[e1234] * -1.0));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for FlatOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e45] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Plane> for FlatOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e45] * other.group0()[3]));
    }
}
impl AntiWedge<Sphere> for FlatOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (Simd32x2::from(self[e45]) * Simd32x2::from([other[e4315], other.group0()[3]]) * Simd32x2::from([-1.0, 1.0])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for FlatOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (Simd32x2::from(self[e45]) * swizzle!(other.group0(), 1, 0) * Simd32x2::from([-1.0, 1.0])));
    }
}
impl AntiWedge<SphereOnOrigin> for FlatOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e45] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<VersorEven> for FlatOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        3        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([(self[e45] * other.group1()[3] * -1.0), 0.0, 0.0, 0.0]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self[e45] * other.group0()[3])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for FlatOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self[e45] * other.group0()[3]));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for FlatOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        3        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([(self[e45] * other.group1()[3] * -1.0), 0.0, 0.0, 0.0]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self[e45] * other.group0()[0])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for FlatOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self[e45] * other.group0()[3]));
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for FlatOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e45] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<VersorOdd> for FlatOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (Simd32x2::from(self[e45]) * Simd32x2::from([other.group2()[3], other.group3()[3]]) * Simd32x2::from([-1.0, 1.0])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for FlatOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (Simd32x2::from(self[e45]) * Simd32x2::from([other.group1()[3], other.group2()[3]]) * Simd32x2::from([-1.0, 1.0])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for FlatOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e45] * other.group2()[3]));
    }
}
impl AntiWedge<VersorOddAtOrigin> for FlatOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (Simd32x2::from(self[e45]) * Simd32x2::from([other.group1()[3], other.group0()[3]]) * Simd32x2::from([-1.0, 1.0])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for FlatOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e45] * other.group1()[0] * -1.0));
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for FlatOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (Simd32x2::from(self[e45]) * Simd32x2::from([other.group2()[3], other.group1()[3]]) * Simd32x2::from([-1.0, 1.0])),
        );
    }
}
impl InfixAntiWedge for FlatPoint {}
impl AntiWedge<AntiDipoleOnOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDualNum> for FlatPoint {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other.group0()[0]));
    }
}
impl AntiWedge<AntiFlatOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e321] * -1.0));
    }
}
impl AntiWedge<AntiFlatPoint> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<AntiFlector> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[0] * -1.0));
    }
}
impl AntiWedge<AntiMotor> for FlatPoint {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other.group1()[3]));
    }
}
impl AntiWedge<AntiScalar> for FlatPoint {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for FlatPoint {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Circle> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<CircleAtOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DualNum> for FlatPoint {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for FlatPoint {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Infinity::from_groups(
            // e5
            ((self.group0()[3] * other.group1()[3]) + (self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for FlatPoint {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<FlectorOnOrigin> for FlatPoint {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Infinity::from_groups(
            // e5
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Horizon> for FlatPoint {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other[e3215]));
    }
}
impl AntiWedge<Motor> for FlatPoint {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<MotorOnOrigin> for FlatPoint {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ (self.group0() * Simd32x4::from(other.group0()[3])));
    }
}
impl AntiWedge<MultiVector> for FlatPoint {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6        9        0
    //    simd3        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        6       12        0
    //  no simd        6       20        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group0()[3] * other.group6()[3])
                    - (self.group0()[2] * other.group7()[2])
                    - (self.group0()[0] * other.group7()[0])
                    - (self.group0()[1] * other.group7()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group9()[0]) * Simd32x4::from(-1.0)),
            // e5
            ((self.group0()[3] * other[e45]) + (self.group0()[2] * other.group9()[3]) + (self.group0()[0] * other.group9()[1]) + (self.group0()[1] * other.group9()[2])),
            // e41, e42, e43, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e15, e25, e35
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for FlatPoint {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for FlatPoint {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other.group0()[3]) * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<Plane> for FlatPoint {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Infinity::from_groups(
            // e5
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for FlatPoint {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Infinity::from_groups(
            // e5
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for FlatPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        4        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3        6        0
    //  no simd        3       12        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other[e4315]) * Simd32x4::from(-1.0)),
            // e5
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for FlatPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[1]) * Simd32x4::from(-1.0)),
            // e5
            (self.group0()[3] * other.group0()[0]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for FlatPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        3        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        2        5        0
    //  no simd        2       11        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[3]) * Simd32x4::from(-1.0)),
            // e5
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for FlatPoint {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group0()[2])
                    - (self.group0()[0] * other.group0()[0])
                    - (self.group0()[1] * other.group0()[1])),
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for FlatPoint {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for FlatPoint {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            (swizzle!(self.group0(), 3, 0, 1, 2)
                * Simd32x4::from([other.group1()[3], other.group0()[0], other.group0()[0], other.group0()[0]])
                * Simd32x4::from([-1.0, 1.0, 1.0, 1.0])),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[0])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for FlatPoint {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for FlatPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for FlatPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        4        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3        6        0
    //  no simd        3       12        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group2()[3]) * Simd32x4::from(-1.0)),
            // e5
            ((self.group0()[3] * other.group3()[3]) + (self.group0()[2] * other.group3()[2]) + (self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for FlatPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        4        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3        6        0
    //  no simd        3       12        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e5
            ((self.group0()[3] * other.group2()[3]) + (self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for FlatPoint {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            ((self.group0()[3] * other.group2()[3]) + (self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for FlatPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e5
            (self.group0()[3] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for FlatPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        3        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        2        5        0
    //  no simd        2       11        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group1()[0]) * Simd32x4::from(-1.0)),
            // e5
            ((self.group0()[2] * other.group1()[3]) + (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for FlatPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group2()[3]) * Simd32x4::from(-1.0)),
            // e5
            (self.group0()[3] * other.group1()[3]),
        );
    }
}
impl InfixAntiWedge for FlatPointAtInfinity {}
impl AntiWedge<AntiDipoleOnOrigin> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for FlatPointAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other[e12345])));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for FlatPointAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group1()[3]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Circle> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DualNum> for FlatPointAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for FlatPointAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Infinity::from_groups(
            // e5
            ((self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for FlatPointAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Infinity::from_groups(
            // e5
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Motor> for FlatPointAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<MotorOnOrigin> for FlatPointAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<MultiVector> for FlatPointAtInfinity {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        4       13        0
    //  no simd        4       15        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group0()[2] * other.group7()[2]) - (self.group0()[0] * other.group7()[0]) - (self.group0()[1] * other.group7()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group9()[0] * -1.0),
                (self.group0()[1] * other.group9()[0] * -1.0),
                (self.group0()[2] * other.group9()[0] * -1.0),
                0.0,
            ]),
            // e5
            ((self.group0()[2] * other.group9()[3]) + (self.group0()[0] * other.group9()[1]) + (self.group0()[1] * other.group9()[2])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for FlatPointAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other[e1234]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for FlatPointAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<Plane> for FlatPointAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Infinity::from_groups(
            // e5
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for FlatPointAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Infinity::from_groups(
            // e5
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for FlatPointAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other[e4315] * -1.0),
            (self.group0()[1] * other[e4315] * -1.0),
            (self.group0()[2] * other[e4315] * -1.0),
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<SphereAtOrigin> for FlatPointAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[1]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<SphereOnOrigin> for FlatPointAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3] * -1.0),
            (self.group0()[1] * other.group0()[3] * -1.0),
            (self.group0()[2] * other.group0()[3] * -1.0),
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorEven> for FlatPointAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for FlatPointAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for FlatPointAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<VersorEvenAtOrigin> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for FlatPointAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for FlatPointAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for FlatPointAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group2()[3] * -1.0),
            (self.group0()[1] * other.group2()[3] * -1.0),
            (self.group0()[2] * other.group2()[3] * -1.0),
            ((self.group0()[2] * other.group3()[2]) + (self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddAligningOrigin> for FlatPointAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3] * -1.0),
            (self.group0()[1] * other.group1()[3] * -1.0),
            (self.group0()[2] * other.group1()[3] * -1.0),
            ((self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddAtInfinity> for FlatPointAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            ((self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for FlatPointAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group1()[3]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<VersorOddOnOrigin> for FlatPointAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group1()[0] * -1.0),
            (self.group0()[1] * other.group1()[0] * -1.0),
            (self.group0()[2] * other.group1()[0] * -1.0),
            ((self.group0()[2] * other.group1()[3]) + (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
        ]));
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for FlatPointAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group2()[3]) * Simd32x3::from(-1.0)));
    }
}
impl InfixAntiWedge for Flector {}
impl AntiWedge<AntiCircleOnOrigin> for Flector {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group0()[0]) - (self.group1()[2] * other.group1()[1])),
                    (-(self.group1()[3] * other.group0()[1]) - (self.group1()[0] * other.group1()[2])),
                    (-(self.group1()[3] * other.group0()[2]) - (self.group1()[1] * other.group1()[0])),
                    ((self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for Flector {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (-(self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDualNum> for Flector {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return MotorAtInfinity::from_groups(
            // e235, e315, e125, e5
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for Flector {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e321]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for Flector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       14        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group1()[1] * other.group0()[2]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[2] * other.group0()[0])),
                (-(self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for Flector {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3] * -1.0),
                (self.group1()[1] * other.group0()[3] * -1.0),
                (self.group1()[2] * other.group0()[3] * -1.0),
                ((self.group1()[2] * other.group1()[2]) + (self.group1()[1] * other.group1()[1]) - (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group1()[0])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group1()[1] * other.group0()[2]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[2] * other.group0()[0])),
                (-(self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for Flector {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[0] * other.group0()[0] * -1.0),
            (self.group1()[1] * other.group0()[0] * -1.0),
            (self.group1()[2] * other.group0()[0] * -1.0),
            ((self.group1()[2] * other.group0()[3]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiLine> for Flector {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group1(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for Flector {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group1()[2], self.group1()[0], self.group1()[1]]))),
        );
    }
}
impl AntiWedge<AntiMotor> for Flector {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        6       13        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            (-(swizzle!(self.group1(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group1()[3]) - (self.group1()[0] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for Flector {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for Flector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for Flector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for Flector {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Flector::from_groups(
            // e15, e25, e35, e45
            (self.group0() * Simd32x4::from(other[e12345])),
            // e4235, e4315, e4125, e3215
            (self.group1() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for Flector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group1()[3] * other.group0()[3]) + (self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for Flector {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        6       12        0
    //  no simd       12       24        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group1() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            ((swizzle!(self.group1(), 2, 1, 2, 0) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                - (swizzle!(other.group1(), 3, 1, 3, 0) * Simd32x4::from([self.group0()[3], self.group1()[2], self.group0()[1], self.group1()[1]]))
                + Simd32x4::from([
                    ((self.group1()[1] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
                    (-(self.group1()[3] * other.group0()[0]) - (self.group0()[0] * other.group1()[3])),
                    (-(self.group1()[3] * other.group0()[1]) - (self.group1()[0] * other.group1()[2])),
                    (-(self.group1()[3] * other.group0()[2]) - (self.group0()[2] * other.group1()[3])),
                ])),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for Flector {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group1()[3]) + (self.group1()[1] * other.group1()[2]) + (self.group1()[0] * other.group1()[1])
                        - (self.group0()[2] * other.group0()[2])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (-(self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Circle> for Flector {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       24        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       14       25        0
    //  no simd       17       28        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group0()[2])
                    - (self.group0()[0] * other.group0()[0])
                    - (self.group0()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group1()[3] * other.group0()[0]),
                    (self.group1()[3] * other.group0()[1]),
                    (self.group1()[3] * other.group0()[2]),
                    (-(self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) - (self.group1()[1] * other.group2()[2]) + (self.group1()[2] * other.group2()[1])),
                ((self.group1()[3] * other.group1()[1]) + (self.group1()[0] * other.group2()[2]) - (self.group1()[2] * other.group2()[0])),
                ((self.group1()[3] * other.group1()[2]) - (self.group1()[0] * other.group2()[1]) + (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for Flector {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) - (self.group1()[1] * other.group2()[2]) + (self.group1()[2] * other.group2()[1])),
                ((self.group1()[3] * other.group1()[1]) + (self.group1()[0] * other.group2()[2]) - (self.group1()[2] * other.group2()[0])),
                ((self.group1()[3] * other.group1()[2]) - (self.group1()[0] * other.group2()[1]) + (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for Flector {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       20        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (self.group0()[3] * other.group0()[3] * -1.0),
                ((self.group1()[3] * other.group0()[0]) - (self.group1()[1] * other.group1()[2]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[3] * other.group0()[1]) + (self.group1()[0] * other.group1()[2]) - (self.group1()[2] * other.group1()[0])),
                ((self.group1()[3] * other.group0()[2]) - (self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[0])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3] * -1.0),
                (self.group1()[1] * other.group0()[3] * -1.0),
                (self.group1()[2] * other.group0()[3] * -1.0),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for Flector {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[1] * other.group1()[2]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[2] * other.group1()[0])),
                (-(self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for Flector {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for Flector {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       12       22        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (-(self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[1] * other.group1()[2]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[2] * other.group1()[0])),
                (-(self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Dipole> for Flector {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       13        0
    //  no simd       11       16        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group0()[0]) - (self.group1()[2] * other.group1()[1])),
                    (-(self.group1()[3] * other.group0()[1]) - (self.group1()[0] * other.group1()[2])),
                    (-(self.group1()[3] * other.group0()[2]) - (self.group1()[1] * other.group1()[0])),
                    ((self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[3] * other.group1()[3]) - (self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for Flector {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       13        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for Flector {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        6       10        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group1(), 2, 0, 1, 3) * swizzle!(other.group0(), 1, 2, 0, 3))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for Flector {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for Flector {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       11        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
            // e5
            (self.group1()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for Flector {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group0()[0]) - (self.group1()[2] * other.group1()[1])),
                    (-(self.group1()[3] * other.group0()[1]) - (self.group1()[0] * other.group1()[2])),
                    (-(self.group1()[3] * other.group0()[2]) - (self.group1()[1] * other.group1()[0])),
                    ((self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for Flector {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e4235, e4315, e4125, e3215
            (self.group1() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for Flector {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group1()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for Flector {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for Flector {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for Flector {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (-(self.group1()[1] * other.group1()[2]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[2] * other.group1()[0])),
                (-(self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                - (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])
                        + (self.group0()[2] * other.group1()[2])
                        + (self.group0()[0] * other.group1()[0])
                        + (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for Flector {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group1()[0] * other.group0()[3]),
            (self.group1()[1] * other.group0()[3]),
            (self.group1()[2] * other.group0()[3]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for Flector {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (-(self.group1()[1] * other.group0()[3]) + (self.group1()[2] * other.group0()[2])),
                ((self.group1()[0] * other.group0()[3]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[1] * other.group0()[1])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                (self.group1()[3] * other.group0()[3] * -1.0),
                (-(self.group1()[3] * other.group0()[0])
                    + (self.group0()[2] * other.group0()[3])
                    + (self.group0()[0] * other.group0()[1])
                    + (self.group0()[1] * other.group0()[2])),
            ]),
        );
    }
}
impl AntiWedge<Horizon> for Flector {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return MotorAtInfinity::from_groups(
            // e235, e315, e125, e5
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<Line> for Flector {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return FlatPoint::from_groups(
            // e15, e25, e35, e45
            (-(swizzle!(self.group1(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group0()[0]) + (self.group1()[2] * other.group1()[1])),
                    ((self.group1()[3] * other.group0()[1]) + (self.group1()[0] * other.group1()[2])),
                    ((self.group1()[3] * other.group0()[2]) + (self.group1()[1] * other.group1()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for Flector {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group1()[2], self.group1()[0], self.group1()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for Flector {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Motor> for Flector {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            ((other.group0() * Simd32x4::from([self.group1()[3], self.group1()[3], self.group1()[3], self.group0()[3]]))
                - (swizzle!(self.group1(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group1()[2] * other.group1()[1]) + (self.group0()[0] * other.group0()[3])),
                    ((self.group0()[1] * other.group0()[3]) + (self.group1()[0] * other.group1()[2])),
                    ((self.group1()[1] * other.group1()[0]) + (self.group0()[2] * other.group0()[3])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
            // e4235, e4315, e4125, e3215
            (self.group1() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for Flector {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ Simd32x3::from([
            (-(self.group1()[1] * other.group0()[2]) + (self.group1()[2] * other.group0()[1])),
            ((self.group1()[0] * other.group0()[2]) - (self.group1()[2] * other.group0()[0])),
            (-(self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for Flector {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group1()[3] * other.group0()[0]),
                    (self.group1()[3] * other.group0()[1]),
                    (self.group1()[3] * other.group0()[2]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
            // e4235, e4315, e4125, e3215
            (self.group1() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for Flector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       23       42        0
    //    simd3        5       10        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       31       55        0
    //  no simd       50       84        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other.group1()[3]) + (self.group1()[2] * other.group1()[2]) + (self.group1()[1] * other.group1()[1]) + (self.group1()[0] * other.group1()[0])
                    - (self.group0()[3] * other.group6()[3])
                    - (self.group0()[2] * other.group7()[2])
                    - (self.group0()[0] * other.group7()[0])
                    - (self.group0()[1] * other.group7()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(self.group0() * Simd32x4::from(other.group9()[0]))
                + (swizzle!(self.group1(), 1, 2, 0, 2) * Simd32x4::from([other.group5()[2], other.group5()[0], other.group5()[1], other.group3()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group3()[0]) - (self.group1()[2] * other.group5()[1])),
                    (-(self.group1()[3] * other.group3()[1]) - (self.group1()[0] * other.group5()[2])),
                    (-(self.group1()[3] * other.group3()[2]) - (self.group1()[1] * other.group5()[0])),
                    ((self.group1()[1] * other.group3()[1]) + (self.group1()[0] * other.group3()[0])),
                ])),
            // e5
            (-(self.group1()[3] * other.group3()[3]) - (self.group1()[2] * other.group4()[2]) - (self.group1()[1] * other.group4()[1]) - (self.group1()[0] * other.group4()[0])
                + (self.group0()[3] * other[e45])
                + (self.group0()[2] * other.group9()[3])
                + (self.group0()[0] * other.group9()[1])
                + (self.group0()[1] * other.group9()[2])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group1(), 2, 0, 1, 2) * Simd32x4::from([other.group7()[1], other.group7()[2], other.group7()[0], other.group6()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group7()[2]),
                    (self.group1()[2] * other.group7()[0]),
                    (self.group1()[0] * other.group7()[1]),
                    (-(self.group1()[1] * other.group6()[1]) + (self.group0()[3] * other.group0()[1]) - (self.group1()[0] * other.group6()[0])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))
                + (swizzle!(other.group8(), 1, 2, 0) * Simd32x3::from([self.group1()[2], self.group1()[0], self.group1()[1]]))
                + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (swizzle!(other.group8(), 2, 0, 1) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[0]]))),
            // e23, e31, e12
            (-(Simd32x3::from(other.group6()[3]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])) + (Simd32x3::from(self.group1()[3]) * other.group7())),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[1] * other.group9()[3]) + (self.group1()[2] * other.group9()[2])),
                ((self.group1()[0] * other.group9()[3]) - (self.group1()[2] * other.group9()[1])),
                (-(self.group1()[0] * other.group9()[2]) + (self.group1()[1] * other.group9()[1])),
                (self.group1()[3] * other.group9()[0] * -1.0),
            ]),
            // e423, e431, e412
            (Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                - (Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[1] * other.group0()[1]),
                (self.group1()[2] * other.group0()[1]),
            ]),
            // e3215
            (self.group1()[3] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for Flector {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for Flector {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0] * -1.0),
            (self.group1()[3] * other.group0()[1] * -1.0),
            (self.group1()[3] * other.group0()[2] * -1.0),
            ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullSphereAtOrigin> for Flector {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group1() * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for Flector {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for Flector {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        3       12        0
    //  no simd        6       21        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group1() * Simd32x4::from(other.group0()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            (-(swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    ((self.group1()[2] * other.group0()[2]) + (self.group1()[1] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
                    ((self.group1()[3] * other.group0()[0]) * -1.0),
                    ((self.group1()[3] * other.group0()[1]) * -1.0),
                    ((self.group1()[3] * other.group0()[2]) * -1.0),
                ])),
        );
    }
}
impl AntiWedge<Origin> for Flector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for Flector {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       16        0
    //  no simd        9       19        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (-(self.group1()[1] * other.group0()[2]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[2] * other.group0()[0])),
                (-(self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group0()[0]) * -1.0),
                    ((self.group1()[3] * other.group0()[1]) * -1.0),
                    ((self.group1()[3] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for Flector {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (-(self.group1()[1] * other.group0()[2]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[2] * other.group0()[0])),
                (-(self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for Flector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group1()[3] * other.group0()[3]) + (self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for Flector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other.group0()[0]));
    }
}
impl AntiWedge<Sphere> for Flector {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       23        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        6       26        0
    //  no simd        9       35        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[0] * other[e4315] * -1.0),
                (self.group1()[1] * other[e4315] * -1.0),
                (self.group1()[2] * other[e4315] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[1] * other.group0()[2]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[2] * other.group0()[0])),
                (-(self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[0])),
                (self.group1()[3] * other[e4315] * -1.0),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group0()[0]) * -1.0),
                    ((self.group1()[3] * other.group0()[1]) * -1.0),
                    ((self.group1()[3] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other[e4315]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for Flector {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        5        0
    // no simd        0       20        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group1() * Simd32x4::from(other.group0()[1]) * Simd32x4::from(-1.0)),
            // e235, e315, e125, e5
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[1]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for Flector {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       23        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        5       25        0
    //  no simd        5       31        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3] * -1.0),
                (self.group1()[1] * other.group0()[3] * -1.0),
                (self.group1()[2] * other.group0()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[1] * other.group0()[2]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[2] * other.group0()[0])),
                (-(self.group1()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[0])),
                (self.group1()[3] * other.group0()[3] * -1.0),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[3]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorEven> for Flector {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       20        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       19       25        0
    //  no simd       31       40        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group3()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group3()[2]) + (self.group1()[1] * other.group3()[1]) + (self.group1()[0] * other.group3()[0])
                        - (self.group0()[3] * other.group1()[3])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + (other.group0() * Simd32x4::from([self.group1()[3], self.group1()[3], self.group1()[3], self.group0()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[1] * other.group1()[1]) - (self.group1()[0] * other.group1()[0]))])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) + (self.group1()[2] * other.group2()[1]) + (self.group0()[0] * other.group0()[3]) - (self.group1()[1] * other.group2()[2])),
                ((self.group1()[3] * other.group1()[1]) - (self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group0()[3]) + (self.group1()[0] * other.group2()[2])),
                ((self.group1()[3] * other.group1()[2]) + (self.group1()[1] * other.group2()[0]) + (self.group0()[2] * other.group0()[3]) - (self.group1()[0] * other.group2()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (self.group1() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Flector {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       21        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       15       24        0
    //  no simd       21       33        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1]))])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) + (self.group1()[2] * other.group2()[1]) + (self.group0()[0] * other.group0()[3]) - (self.group1()[1] * other.group2()[2])),
                ((self.group1()[3] * other.group1()[1]) - (self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group0()[3]) + (self.group1()[0] * other.group2()[2])),
                ((self.group1()[3] * other.group1()[2]) + (self.group1()[1] * other.group2()[0]) + (self.group0()[2] * other.group0()[3]) - (self.group1()[0] * other.group2()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (self.group1() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Flector {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3       14        0
    //    simd4        3        4        0
    // Totals...
    // yes simd        6       18        0
    //  no simd       15       30        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((swizzle!(self.group1(), 2, 3, 3, 3) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (swizzle!(self.group1(), 1, 2, 0, 1) * Simd32x4::from([other.group0()[2], other.group2()[1], other.group2()[2], other.group2()[0]]))
                + (swizzle!(other.group0(), 1, 0, 0, 0) * Simd32x4::from([self.group1()[0], self.group0()[0], self.group0()[1], self.group0()[2]]))
                - Simd32x4::from([
                    (self.group0()[3] * other.group1()[3]),
                    (self.group1()[1] * other.group2()[2]),
                    (self.group1()[2] * other.group2()[0]),
                    (self.group1()[0] * other.group2()[1]),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3] * -1.0),
                (self.group1()[1] * other.group1()[3] * -1.0),
                (self.group1()[2] * other.group1()[3] * -1.0),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group0()[0])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group1() * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for Flector {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       11        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       13        0
    //  no simd       12       19        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1]))])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[1] * other.group1()[2]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[2] * other.group1()[0])),
                (-(self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Flector {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       15        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        9       18        0
    //  no simd       15       27        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1]))])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group1()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group1()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group1()[3] * other.group1()[2])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (self.group1() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for Flector {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group2()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group2()[2]) + (self.group1()[1] * other.group2()[1]) + (self.group1()[0] * other.group2()[0])
                        - (self.group0()[2] * other.group0()[2])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (-(self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[1] * other.group1()[2]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[2] * other.group1()[0])),
                (-(self.group1()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOdd> for Flector {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       28        0
    //    simd4        4        4        0
    // Totals...
    // yes simd       16       32        0
    //  no simd       28       44        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[0] * other.group2()[3] * -1.0),
                (self.group1()[1] * other.group2()[3] * -1.0),
                (self.group1()[2] * other.group2()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[1] * other.group3()[2]) + (self.group1()[2] * other.group3()[1])),
                ((self.group1()[0] * other.group3()[2]) - (self.group1()[2] * other.group3()[0])),
                (-(self.group1()[0] * other.group3()[1]) + (self.group1()[1] * other.group3()[0])),
                (self.group1()[3] * other.group2()[3] * -1.0),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group3()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                - (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group2()[2]) - (self.group1()[1] * other.group2()[1]) - (self.group1()[0] * other.group2()[0])
                        + (self.group0()[2] * other.group3()[2])
                        + (self.group0()[0] * other.group3()[0])
                        + (self.group0()[1] * other.group3()[1])),
                ])),
            // e1, e2, e3, e4
            (-(self.group0() * Simd32x4::from(other.group2()[3]))
                + (swizzle!(self.group1(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group0()[0]) - (self.group1()[2] * other.group1()[1])),
                    (-(self.group1()[3] * other.group0()[1]) - (self.group1()[0] * other.group1()[2])),
                    (-(self.group1()[3] * other.group0()[2]) - (self.group1()[1] * other.group1()[0])),
                    ((self.group1()[1] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for Flector {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       29        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       13       32        0
    //  no simd       22       41        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3] * -1.0),
                (self.group1()[1] * other.group1()[3] * -1.0),
                (self.group1()[2] * other.group1()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[1] * other.group2()[2]) + (self.group1()[2] * other.group2()[1])),
                ((self.group1()[0] * other.group2()[2]) - (self.group1()[2] * other.group2()[0])),
                (-(self.group1()[0] * other.group2()[1]) + (self.group1()[1] * other.group2()[0])),
                (self.group1()[3] * other.group1()[3] * -1.0),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                - (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) - (self.group1()[0] * other.group1()[0])
                        + (self.group0()[2] * other.group2()[2])
                        + (self.group0()[0] * other.group2()[0])
                        + (self.group0()[1] * other.group2()[1])),
                ])),
            // e1, e2, e3, e4
            (-(self.group0() * Simd32x4::from(other.group1()[3]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group0()[0]) * -1.0),
                    ((self.group1()[3] * other.group0()[1]) * -1.0),
                    ((self.group1()[3] * other.group0()[2]) * -1.0),
                    ((self.group1()[2] * other.group0()[2]) + (self.group1()[1] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for Flector {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[2] * other.group1()[1])),
                (-(self.group1()[0] * other.group1()[2]) + (self.group1()[2] * other.group1()[0])),
                ((self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group1()[0])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[1] * other.group2()[2]) + (self.group1()[2] * other.group2()[1])),
                ((self.group1()[0] * other.group2()[2]) - (self.group1()[2] * other.group2()[0])),
                (-(self.group1()[0] * other.group2()[1]) + (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                - (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[3]) - (self.group1()[1] * other.group0()[2]) - (self.group1()[0] * other.group0()[1])
                        + (self.group0()[2] * other.group2()[2])
                        + (self.group0()[0] * other.group2()[0])
                        + (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for Flector {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        6       19        0
    //  no simd        9       28        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group1() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            (-(self.group0() * Simd32x4::from(other.group1()[3]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group0()[0]) * -1.0),
                    ((self.group1()[3] * other.group0()[1]) * -1.0),
                    ((self.group1()[3] * other.group0()[2]) * -1.0),
                    ((self.group1()[2] * other.group0()[2]) + (self.group1()[1] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for Flector {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       33        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       34        0
    //  no simd       12       37        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0] * -1.0),
                (self.group1()[1] * other.group1()[0] * -1.0),
                (self.group1()[2] * other.group1()[0] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[1] * other.group1()[3]) + (self.group1()[2] * other.group1()[2])),
                ((self.group1()[0] * other.group1()[3]) - (self.group1()[2] * other.group1()[1])),
                (-(self.group1()[0] * other.group1()[2]) + (self.group1()[1] * other.group1()[1])),
                (self.group1()[3] * other.group1()[0] * -1.0),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group1()[1] * -1.0),
                (self.group1()[3] * other.group1()[2] * -1.0),
                (self.group1()[3] * other.group1()[3] * -1.0),
                (-(self.group1()[3] * other.group0()[3])
                    + (self.group0()[2] * other.group1()[3])
                    + (self.group0()[0] * other.group1()[1])
                    + (self.group0()[1] * other.group1()[2])),
            ]),
            // e1, e2, e3, e4
            (-(self.group0() * Simd32x4::from(other.group1()[0]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group0()[0]) * -1.0),
                    ((self.group1()[3] * other.group0()[1]) * -1.0),
                    ((self.group1()[3] * other.group0()[2]) * -1.0),
                    ((self.group1()[2] * other.group0()[2]) + (self.group1()[1] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for Flector {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       15        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       15       31        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group1() * Simd32x4::from(other.group2()[3]) * Simd32x4::from(-1.0)),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[1] * other.group2()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group1()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            (-(self.group0() * Simd32x4::from(other.group2()[3]))
                + (swizzle!(self.group1(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group0()[0]) - (self.group1()[2] * other.group1()[1])),
                    (-(self.group1()[3] * other.group0()[1]) - (self.group1()[0] * other.group1()[2])),
                    (-(self.group1()[3] * other.group0()[2]) - (self.group1()[1] * other.group1()[0])),
                    ((self.group1()[1] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl InfixAntiWedge for FlectorAtInfinity {}
impl AntiWedge<AntiCircleOnOrigin> for FlectorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for FlectorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for FlectorAtInfinity {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for FlectorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for FlectorAtInfinity {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group1()[3] * -1.0),
            (-(self.group0()[0] * other.group1()[3]) - (self.group0()[3] * other.group0()[0])),
            (-(self.group0()[1] * other.group1()[3]) - (self.group0()[3] * other.group0()[1])),
            (-(self.group0()[2] * other.group1()[3]) - (self.group0()[3] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for FlectorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group1()[0]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Circle> for FlectorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for FlectorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for FlectorAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for FlectorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<CircleOnOrigin> for FlectorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for FlectorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Dipole> for FlectorAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for FlectorAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[3]) * other.group0() * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<DipoleAtInfinity> for FlectorAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleAtOrigin> for FlectorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<DipoleOnOrigin> for FlectorAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[3]) * other.group0() * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for FlectorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<DualNum> for FlectorAtInfinity {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<FlatOrigin> for FlectorAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for FlectorAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Flector> for FlectorAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group1()[0] * -1.0),
            (self.group0()[3] * other.group1()[1] * -1.0),
            (self.group0()[3] * other.group1()[2] * -1.0),
            (-(self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for FlectorAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[1] * -1.0),
            (self.group0()[3] * other.group0()[2] * -1.0),
            (self.group0()[3] * other.group0()[3] * -1.0),
            (-(self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<Line> for FlectorAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<LineOnOrigin> for FlectorAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Motor> for FlectorAtInfinity {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for FlectorAtInfinity {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<MultiVector> for FlectorAtInfinity {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       17        0
    //    simd3        1        5        0
    // Totals...
    // yes simd       10       22        0
    //  no simd       12       32        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group7()[2]) - (self.group0()[0] * other.group7()[0]) - (self.group0()[1] * other.group7()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[0] * other.group9()[0]) - (self.group0()[3] * other.group3()[0])),
                (-(self.group0()[1] * other.group9()[0]) - (self.group0()[3] * other.group3()[1])),
                (-(self.group0()[2] * other.group9()[0]) - (self.group0()[3] * other.group3()[2])),
                0.0,
            ]),
            // e5
            (-(self.group0()[3] * other.group3()[3]) + (self.group0()[2] * other.group9()[3]) + (self.group0()[0] * other.group9()[1]) + (self.group0()[1] * other.group9()[2])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            ((Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group7()),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group9()[0] * -1.0)]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]) * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            (self.group0()[3] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for FlectorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for FlectorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullSphereAtOrigin> for FlectorAtInfinity {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for FlectorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for FlectorAtInfinity {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[3] * -1.0),
            (-(self.group0()[0] * other.group0()[3]) - (self.group0()[3] * other.group0()[0])),
            (-(self.group0()[1] * other.group0()[3]) - (self.group0()[3] * other.group0()[1])),
            (-(self.group0()[2] * other.group0()[3]) - (self.group0()[3] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<Origin> for FlectorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for FlectorAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0] * -1.0),
            (self.group0()[3] * other.group0()[1] * -1.0),
            (self.group0()[3] * other.group0()[2] * -1.0),
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<PlaneOnOrigin> for FlectorAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0] * -1.0),
            (self.group0()[3] * other.group0()[1] * -1.0),
            (self.group0()[3] * other.group0()[2] * -1.0),
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<RoundPoint> for FlectorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<RoundPointAtOrigin> for FlectorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[0]));
    }
}
impl AntiWedge<Sphere> for FlectorAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        2       11        0
    //  no simd        2       17        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other[e4315]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            Simd32x4::from([
                (self.group0()[0] * other[e4315] * -1.0),
                (self.group0()[1] * other[e4315] * -1.0),
                (self.group0()[2] * other[e4315] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for FlectorAtInfinity {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other.group0()[1]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for FlectorAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        2       11        0
    //  no simd        2       17        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0() * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3] * -1.0),
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for FlectorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                ((self.group0()[3] * other.group3()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for FlectorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for FlectorAtInfinity {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[0]) + (self.group0()[3] * other.group1()[0])),
            ((self.group0()[1] * other.group0()[0]) + (self.group0()[3] * other.group1()[1])),
            ((self.group0()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[2])),
            (self.group0()[3] * other.group0()[0]),
        ]));
    }
}
impl AntiWedge<VersorEvenAtOrigin> for FlectorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for FlectorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for FlectorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group2()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for FlectorAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        3       12        0
    //  no simd        6       21        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group2()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (-(self.group0() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) * -1.0),
                    ((self.group0()[3] * other.group0()[1]) * -1.0),
                    ((self.group0()[3] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group3()[2]) + (self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for FlectorAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        3       12        0
    //  no simd        6       21        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (-(self.group0() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[3]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) * -1.0),
                    ((self.group0()[3] * other.group0()[1]) * -1.0),
                    ((self.group0()[3] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for FlectorAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group2()[0] * -1.0),
            (self.group0()[3] * other.group2()[1] * -1.0),
            (self.group0()[3] * other.group2()[2] * -1.0),
            (-(self.group0()[3] * other.group1()[3]) + (self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddAtOrigin> for FlectorAtInfinity {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group1()[3] * -1.0),
            (-(self.group0()[0] * other.group1()[3]) - (self.group0()[3] * other.group0()[0])),
            (-(self.group0()[1] * other.group1()[3]) - (self.group0()[3] * other.group0()[1])),
            (-(self.group0()[2] * other.group1()[3]) - (self.group0()[3] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for FlectorAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        3       12        0
    //  no simd        6       21        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * swizzle!(other.group1(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (-(self.group0() * Simd32x4::from([other.group1()[0], other.group1()[0], other.group1()[0], other.group0()[3]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) * -1.0),
                    ((self.group0()[3] * other.group0()[1]) * -1.0),
                    ((self.group0()[3] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group1()[3]) + (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for FlectorAtInfinity {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group2()[3] * -1.0),
            (-(self.group0()[0] * other.group2()[3]) - (self.group0()[3] * other.group0()[0])),
            (-(self.group0()[1] * other.group2()[3]) - (self.group0()[3] * other.group0()[1])),
            (-(self.group0()[2] * other.group2()[3]) - (self.group0()[3] * other.group0()[2])),
        ]));
    }
}
impl InfixAntiWedge for FlectorOnOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for FlectorOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        9        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[1]) * -1.0),
                    ((self.group0()[1] * other.group1()[2]) * -1.0),
                    ((self.group0()[2] * other.group1()[0]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for FlectorOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       14        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[2] * other.group0()[2]) - (self.group0()[3] * other.group0()[1])),
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[1]) - (self.group0()[2] * other.group0()[0])),
                (self.group0()[0] * other.group0()[3] * -1.0),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDualNum> for FlectorOnOrigin {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatOrigin> for FlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from(other[e321]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for FlectorOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       14        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from(other.group0()[3]) * Simd32x4::from(-1.0)),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[3] * other.group0()[0])),
                (-(self.group0()[1] * other.group0()[1]) + (self.group0()[2] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for FlectorOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                ((self.group0()[3] * other.group1()[2]) + (self.group0()[2] * other.group1()[1]) - (self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other.group1()[0])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[3] * other.group0()[0])),
                (-(self.group0()[1] * other.group0()[1]) + (self.group0()[2] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for FlectorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[1] * other.group0()[0] * -1.0),
            (self.group0()[2] * other.group0()[0] * -1.0),
            (self.group0()[3] * other.group0()[0] * -1.0),
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiLine> for FlectorOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group0()[2]),
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[1] * other.group0()[1]),
                    (-(self.group0()[1] * other.group1()[0]) - (self.group0()[2] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for FlectorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[2], self.group0()[3], self.group0()[1]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[3], self.group0()[1], self.group0()[2]]))),
        );
    }
}
impl AntiWedge<AntiMotor> for FlectorOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (self.group0()[3] * other.group1()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            ((swizzle!(self.group0(), 2, 3, 1, 0) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                - (swizzle!(self.group0(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[2] * other.group1()[1]) - (self.group0()[1] * other.group1()[0]))])),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for FlectorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[2] * other.group0()[2]) - (self.group0()[3] * other.group0()[1])),
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[1]) - (self.group0()[2] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for FlectorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for FlectorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for FlectorOnOrigin {
    type Output = FlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return FlectorOnOrigin::from_groups(/* e45, e4235, e4315, e4125 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for FlectorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for FlectorOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (self.group0()[3] * other.group1()[3] * -1.0),
                0.0,
            ]),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 2, 3, 1) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                - (swizzle!(self.group0(), 0, 3, 1, 2) * swizzle!(other.group1(), 3, 1, 2, 0))
                + Simd32x4::from([((self.group0()[2] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for FlectorOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                - (swizzle!(self.group0(), 3, 1, 2, 0) * swizzle!(other.group0(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group0()[2] * other.group1()[2]) + (self.group0()[1] * other.group1()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Circle> for FlectorOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       23        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[2] * other.group0()[2]) - (self.group0()[3] * other.group0()[1])),
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[1]) - (self.group0()[2] * other.group0()[0])),
                (self.group0()[0] * other.group1()[3] * -1.0),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (self.group0()[3] * other.group1()[3] * -1.0),
                (-(self.group0()[3] * other.group1()[2]) - (self.group0()[1] * other.group1()[0]) - (self.group0()[2] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[2] * other.group2()[2]) + (self.group0()[3] * other.group2()[1])),
                ((self.group0()[1] * other.group2()[2]) - (self.group0()[3] * other.group2()[0])),
                (-(self.group0()[1] * other.group2()[1]) + (self.group0()[2] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for FlectorOnOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        1        2        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        8       15        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(self.group0(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group0()[2]),
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[1] * other.group0()[1]),
                    (-(self.group0()[1] * other.group1()[0]) - (self.group0()[2] * other.group1()[1])),
                ])),
            // e15, e25, e35
            (-(swizzle!(other.group2(), 2, 0, 1) * Simd32x3::from([self.group0()[2], self.group0()[3], self.group0()[1]]))
                + (swizzle!(other.group2(), 1, 2, 0) * Simd32x3::from([self.group0()[3], self.group0()[1], self.group0()[2]]))),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for FlectorOnOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       17        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3] * -1.0),
                (-(self.group0()[2] * other.group1()[2]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[3] * other.group1()[0])),
                (-(self.group0()[1] * other.group1()[1]) + (self.group0()[2] * other.group1()[0])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for FlectorOnOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        4        0
    // no simd        6       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[2], self.group0()[3], self.group0()[1]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[3], self.group0()[1], self.group0()[2]]))),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[2], self.group0()[3], self.group0()[1]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[3], self.group0()[1], self.group0()[2]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for FlectorOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(self.group0(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group0()[2]),
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[1] * other.group0()[1]),
                    (-(self.group0()[1] * other.group1()[0]) - (self.group0()[2] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for FlectorOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       20        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[2] * other.group0()[2]) - (self.group0()[3] * other.group0()[1])),
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[1]) - (self.group0()[2] * other.group0()[0])),
                (self.group0()[0] * other.group0()[3] * -1.0),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[2] * other.group1()[2]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[3] * other.group1()[0])),
                (-(self.group0()[1] * other.group1()[1]) + (self.group0()[2] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Dipole> for FlectorOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4       12        0
    //  no simd        7       15        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[1]) * -1.0),
                    ((self.group0()[1] * other.group1()[2]) * -1.0),
                    ((self.group0()[2] * other.group1()[0]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[3] * other.group2()[2]) - (self.group0()[1] * other.group2()[0]) - (self.group0()[2] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for FlectorOnOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        6        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ Simd32x2::from([
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[3] * other.group1()[2]) - (self.group0()[1] * other.group1()[0]) - (self.group0()[2] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<DipoleAtInfinity> for FlectorOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group0()[2]),
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[1] * other.group0()[1]),
                    (-(self.group0()[1] * other.group1()[0]) - (self.group0()[2] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for FlectorOnOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ Simd32x2::from([
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[3] * other.group1()[2]) - (self.group0()[1] * other.group1()[0]) - (self.group0()[2] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<DipoleOnOrigin> for FlectorOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for FlectorOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4       12        0
    //  no simd        7       15        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[1]) * -1.0),
                    ((self.group0()[1] * other.group1()[2]) * -1.0),
                    ((self.group0()[2] * other.group1()[0]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[3] * other.group2()[2]) - (self.group0()[1] * other.group2()[0]) - (self.group0()[2] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for FlectorOnOrigin {
    type Output = FlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return FlectorOnOrigin::from_groups(/* e45, e4235, e4315, e4125 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<FlatPoint> for FlectorOnOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for FlectorOnOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for FlectorOnOrigin {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       13        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (-(self.group0()[2] * other.group1()[2]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[3] * other.group1()[0])),
                (-(self.group0()[1] * other.group1()[1]) + (self.group0()[2] * other.group1()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (self.group0()[3] * other.group1()[3]),
                (-(self.group0()[3] * other.group0()[2]) - (self.group0()[2] * other.group0()[1]) + (self.group0()[0] * other.group1()[3])
                    - (self.group0()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for FlectorOnOrigin {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (self.group0()[3] * other.group0()[3]),
            (-(self.group0()[3] * other.group0()[2]) - (self.group0()[2] * other.group0()[1]) + (self.group0()[0] * other.group0()[3]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for FlectorOnOrigin {
    type Output = LineOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return LineOnOrigin::from_groups(/* e415, e425, e435 */ Simd32x3::from([
            (-(self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
            ((self.group0()[1] * other.group0()[3]) - (self.group0()[3] * other.group0()[1])),
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Horizon> for FlectorOnOrigin {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from(other[e3215])));
    }
}
impl AntiWedge<Line> for FlectorOnOrigin {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return FlatPoint::from_groups(
            // e15, e25, e35, e45
            (-(swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[1] * other.group1()[2]),
                    (self.group0()[2] * other.group1()[0]),
                    (-(self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for FlectorOnOrigin {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[2], self.group0()[3], self.group0()[1]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[3], self.group0()[1], self.group0()[2]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for FlectorOnOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return FlatOrigin::from_groups(
            // e45
            (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for FlectorOnOrigin {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            (-(swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(self.group0(), 3, 1, 2, 0) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[0], other.group0()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[2] * other.group0()[1]) - (self.group0()[1] * other.group0()[0]))])),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (self.group0()[3] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for FlectorOnOrigin {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ Simd32x3::from([
            (-(self.group0()[2] * other.group0()[2]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[3] * other.group0()[0])),
            (-(self.group0()[1] * other.group0()[1]) + (self.group0()[2] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for FlectorOnOrigin {
    type Output = FlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return FlectorOnOrigin::from_groups(/* e45, e4235, e4315, e4125 */ Simd32x4::from([
            (-(self.group0()[3] * other.group0()[2]) - (self.group0()[2] * other.group0()[1]) + (self.group0()[0] * other.group0()[3]) - (self.group0()[1] * other.group0()[0])),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<MultiVector> for FlectorOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       21        0
    //    simd3        1        7        0
    //    simd4        4        4        0
    // Totals...
    // yes simd       16       32        0
    //  no simd       30       58        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other.group1()[2]) + (self.group0()[2] * other.group1()[1]) - (self.group0()[0] * other.group6()[3]) + (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group5()[2], other.group5()[0], other.group5()[1], other.group3()[2]]))
                - (swizzle!(self.group0(), 3, 1, 2, 0) * Simd32x4::from([other.group5()[1], other.group5()[2], other.group5()[0], other.group9()[0]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group0()[2] * other.group3()[1]) + (self.group0()[1] * other.group3()[0]))])),
            // e5
            (-(self.group0()[3] * other.group4()[2]) - (self.group0()[2] * other.group4()[1]) + (self.group0()[0] * other[e45]) - (self.group0()[1] * other.group4()[0])),
            // e41, e42, e43, e45
            ((swizzle!(self.group0(), 2, 3, 1, 0) * Simd32x4::from([other.group7()[2], other.group7()[0], other.group7()[1], other.group0()[1]]))
                - (swizzle!(self.group0(), 3, 1, 2, 3) * Simd32x4::from([other.group7()[1], other.group7()[2], other.group7()[0], other.group6()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[2] * other.group6()[1]) - (self.group0()[1] * other.group6()[0]))])),
            // e15, e25, e35
            (-(swizzle!(other.group8(), 2, 0, 1) * Simd32x3::from([self.group0()[2], self.group0()[3], self.group0()[1]]))
                + (swizzle!(other.group8(), 1, 2, 0) * Simd32x3::from([self.group0()[3], self.group0()[1], self.group0()[2]]))),
            // e23, e31, e12
            (Simd32x3::from(other.group6()[3]) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[3]]) * Simd32x3::from(-1.0)),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[2] * other.group9()[3]) + (self.group0()[3] * other.group9()[2])),
                ((self.group0()[1] * other.group9()[3]) - (self.group0()[3] * other.group9()[1])),
                (-(self.group0()[1] * other.group9()[2]) + (self.group0()[2] * other.group9()[1])),
                0.0,
            ]),
            // e423, e431, e412
            (Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[3]]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            (Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[3]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                (self.group0()[3] * other.group0()[1]),
            ]),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for FlectorOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[2], self.group0()[3], self.group0()[1]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[3], self.group0()[1], self.group0()[2]]))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for FlectorOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for FlectorOnOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullVersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for FlectorOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ Simd32x3::from([
            ((self.group0()[2] * other.group0()[2]) - (self.group0()[3] * other.group0()[1])),
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[1]) - (self.group0()[2] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for FlectorOnOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[1] * other.group0()[3] * -1.0),
            (self.group0()[2] * other.group0()[3] * -1.0),
            (self.group0()[3] * other.group0()[3] * -1.0),
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[2] * other.group0()[1]) - (self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Plane> for FlectorOnOrigin {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        3       10        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[3] * other.group0()[0])),
                (-(self.group0()[1] * other.group0()[1]) + (self.group0()[2] * other.group0()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for FlectorOnOrigin {
    type Output = LineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return LineOnOrigin::from_groups(
            // e415, e425, e435
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[2], self.group0()[3], self.group0()[1]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[3], self.group0()[1], self.group0()[2]]))),
        );
    }
}
impl AntiWedge<RoundPoint> for FlectorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for FlectorOnOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3       14        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3       15        0
    //  no simd        3       18        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[1] * other[e4315] * -1.0),
                (self.group0()[2] * other[e4315] * -1.0),
                (self.group0()[3] * other[e4315] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[3] * other.group0()[0])),
                (-(self.group0()[1] * other.group0()[1]) + (self.group0()[2] * other.group0()[0])),
                (self.group0()[0] * other[e4315] * -1.0),
            ]),
            // e235, e315, e125, e5
            (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for FlectorOnOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from(other.group0()[1]) * Simd32x4::from(-1.0)),
            // e235, e315, e125, e5
            (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for FlectorOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       14        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[3] * other.group0()[0])),
                (-(self.group0()[1] * other.group0()[1]) + (self.group0()[2] * other.group0()[0])),
                (self.group0()[0] * other.group0()[3] * -1.0),
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for FlectorOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       21        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       23        0
    //  no simd       15       29        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group3()[2]]))
                - (swizzle!(self.group0(), 3, 1, 2, 0) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group0()[2] * other.group3()[1]) + (self.group0()[1] * other.group3()[0]))])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (self.group0()[3] * other.group1()[3] * -1.0),
                (-(self.group0()[3] * other.group1()[2]) - (self.group0()[2] * other.group1()[1]) + (self.group0()[0] * other.group0()[3])
                    - (self.group0()[1] * other.group1()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[2] * other.group2()[2]) + (self.group0()[3] * other.group2()[1])),
                ((self.group0()[1] * other.group2()[2]) - (self.group0()[3] * other.group2()[0])),
                (-(self.group0()[1] * other.group2()[1]) + (self.group0()[2] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (self.group0()[3] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for FlectorOnOrigin {
    type Output = VersorOddAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       11        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       13        0
    //  no simd       12       19        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOddAligningOrigin::from_groups(
            // e41, e42, e43, e45
            ((swizzle!(self.group0(), 2, 3, 1, 0) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(self.group0(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[2] * other.group1()[1]) - (self.group0()[1] * other.group1()[0]))])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[2] * other.group2()[2]) + (self.group0()[3] * other.group2()[1])),
                ((self.group0()[1] * other.group2()[2]) - (self.group0()[3] * other.group2()[0])),
                (-(self.group0()[1] * other.group2()[1]) + (self.group0()[2] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (self.group0()[3] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for FlectorOnOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       15        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       17        0
    //  no simd       12       23        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((swizzle!(self.group0(), 3, 3, 1, 2) * Simd32x4::from([other.group0()[3], other.group2()[1], other.group2()[2], other.group2()[0]]))
                - (swizzle!(self.group0(), 0, 2, 3, 1) * Simd32x4::from([other.group1()[3], other.group2()[2], other.group2()[0], other.group2()[1]]))
                + Simd32x4::from([((self.group0()[2] * other.group0()[2]) + (self.group0()[1] * other.group0()[1])), 0.0, 0.0, 0.0])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (self.group0()[3] * other.group1()[3] * -1.0),
                (-(self.group0()[3] * other.group1()[2]) - (self.group0()[2] * other.group1()[1]) + (self.group0()[0] * other.group0()[0])
                    - (self.group0()[1] * other.group1()[0])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                (self.group0()[3] * other.group0()[0]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for FlectorOnOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       12        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                ((self.group0()[2] * other.group0()[2]) - (self.group0()[3] * other.group0()[1])),
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[1]) - (self.group0()[2] * other.group0()[0])),
            ]),
            // e15, e25, e35
            Simd32x3::from([
                (-(self.group0()[2] * other.group1()[2]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[3] * other.group1()[0])),
                (-(self.group0()[1] * other.group1()[1]) + (self.group0()[2] * other.group1()[0])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for FlectorOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            ((swizzle!(self.group0(), 2, 3, 1, 0) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(self.group0(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[2] * other.group1()[1]) - (self.group0()[1] * other.group1()[0]))])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (self.group0()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for FlectorOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       14        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       16        0
    //  no simd       12       22        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group2()[2]]))
                - (swizzle!(self.group0(), 3, 1, 2, 0) * swizzle!(other.group0(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group0()[2] * other.group2()[1]) + (self.group0()[1] * other.group2()[0]))])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[2] * other.group1()[2]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[3] * other.group1()[0])),
                (-(self.group0()[1] * other.group1()[1]) + (self.group0()[2] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOdd> for FlectorOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       21        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       23        0
    //  no simd       15       29        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[1] * other.group2()[3] * -1.0),
                (self.group0()[2] * other.group2()[3] * -1.0),
                (self.group0()[3] * other.group2()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[2] * other.group3()[2]) + (self.group0()[3] * other.group3()[1])),
                ((self.group0()[1] * other.group3()[2]) - (self.group0()[3] * other.group3()[0])),
                (-(self.group0()[1] * other.group3()[1]) + (self.group0()[2] * other.group3()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[1] * other.group3()[3]),
                (self.group0()[2] * other.group3()[3]),
                (self.group0()[3] * other.group3()[3]),
                (-(self.group0()[3] * other.group2()[2]) - (self.group0()[2] * other.group2()[1]) + (self.group0()[0] * other.group3()[3])
                    - (self.group0()[1] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                - (swizzle!(self.group0(), 3, 1, 2, 0) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[0], other.group2()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group0()[2] * other.group0()[1]) + (self.group0()[1] * other.group0()[0]))])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for FlectorOnOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       15        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       17        0
    //  no simd       12       23        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (self.group0()[3] * other.group1()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e4
            (-(swizzle!(self.group0(), 2, 3, 1, 0) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[3]]))
                + (swizzle!(self.group0(), 3, 1, 2, 3) * Simd32x4::from([other.group2()[1], other.group2()[2], other.group2()[0], other.group0()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group0()[2] * other.group0()[1]) + (self.group0()[1] * other.group0()[0]))])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (self.group0()[3] * other.group2()[3]),
                (-(self.group0()[3] * other.group1()[2]) - (self.group0()[2] * other.group1()[1]) + (self.group0()[0] * other.group2()[3])
                    - (self.group0()[1] * other.group1()[0])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for FlectorOnOrigin {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       19        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                ((self.group0()[2] * other.group1()[2]) - (self.group0()[3] * other.group1()[1])),
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group1()[1]) - (self.group0()[2] * other.group1()[0])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[2] * other.group2()[2]) + (self.group0()[3] * other.group2()[1])),
                ((self.group0()[1] * other.group2()[2]) - (self.group0()[3] * other.group2()[0])),
                (-(self.group0()[1] * other.group2()[1]) + (self.group0()[2] * other.group2()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (self.group0()[3] * other.group2()[3]),
                (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group2()[3])
                    - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for FlectorOnOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       17        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            Simd32x4::from([
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (self.group0()[3] * other.group1()[3] * -1.0),
                ((self.group0()[3] * other.group0()[2]) + (self.group0()[2] * other.group0()[1]) - (self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other.group0()[0])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (self.group0()[3] * other.group0()[3]),
                (-(self.group0()[3] * other.group1()[2]) - (self.group0()[2] * other.group1()[1]) + (self.group0()[0] * other.group0()[3])
                    - (self.group0()[1] * other.group1()[0])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for FlectorOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[1] * other.group1()[0] * -1.0),
                (self.group0()[2] * other.group1()[0] * -1.0),
                (self.group0()[3] * other.group1()[0] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e4
            (-(swizzle!(self.group0(), 2, 3, 1, 0) * swizzle!(other.group1(), 3, 1, 2, 0))
                + (swizzle!(self.group0(), 3, 1, 2, 3) * Simd32x4::from([other.group1()[2], other.group1()[3], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group0()[2] * other.group0()[1]) + (self.group0()[1] * other.group0()[0]))])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for FlectorOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       15        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       17        0
    //  no simd       12       23        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[1] * other.group2()[3] * -1.0),
                (self.group0()[2] * other.group2()[3] * -1.0),
                (self.group0()[3] * other.group2()[3] * -1.0),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (self.group0()[3] * other.group1()[3]),
                (-(self.group0()[3] * other.group2()[2]) - (self.group0()[2] * other.group2()[1]) + (self.group0()[0] * other.group1()[3])
                    - (self.group0()[1] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                - (swizzle!(self.group0(), 3, 1, 2, 0) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[0], other.group2()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group0()[2] * other.group0()[1]) + (self.group0()[1] * other.group0()[0]))])),
        );
    }
}
impl InfixAntiWedge for Horizon {}
impl AntiWedge<AntiCircleOnOrigin> for Horizon {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e3215]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for Horizon {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e3215]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiScalar> for Horizon {
    type Output = Horizon;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Horizon::from_groups(/* e3215 */ (self[e3215] * other[e12345]));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for Horizon {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e3215] * other.group0()[3]));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for Horizon {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group1()[3], other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for Horizon {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[0]])),
        );
    }
}
impl AntiWedge<Circle> for Horizon {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e3215]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self[e3215]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for Horizon {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e3215]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self[e3215]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for Horizon {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (Simd32x3::from(self[e3215]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for Horizon {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self[e3215]) * other.group0()));
    }
}
impl AntiWedge<CircleOnOrigin> for Horizon {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e3215]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self[e3215]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for Horizon {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e3215]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<Dipole> for Horizon {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for Horizon {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self[e3215]) * other.group0() * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<DipoleAtInfinity> for Horizon {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e3215] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<DipoleAtOrigin> for Horizon {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e3215]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<DipoleOnOrigin> for Horizon {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self[e3215]) * other.group0() * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for Horizon {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e3215]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<DualNum> for Horizon {
    type Output = Horizon;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return Horizon::from_groups(/* e3215 */ (self[e3215] * other.group0()[1]));
    }
}
impl AntiWedge<FlatOrigin> for Horizon {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e3215] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for Horizon {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e3215] * other.group0()[3] * -1.0));
    }
}
impl AntiWedge<Flector> for Horizon {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        use crate::elements::*;
        return MotorAtInfinity::from_groups(
            // e235, e315, e125, e5
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for Horizon {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MotorAtInfinity::from_groups(
            // e235, e315, e125, e5
            (Simd32x4::from(self[e3215]) * swizzle!(other.group0(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Line> for Horizon {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        use crate::elements::*;
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self[e3215]) * other.group0()));
    }
}
impl AntiWedge<LineOnOrigin> for Horizon {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        use crate::elements::*;
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self[e3215]) * other.group0()));
    }
}
impl AntiWedge<Motor> for Horizon {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ (Simd32x4::from(self[e3215]) * other.group0()));
    }
}
impl AntiWedge<MotorOnOrigin> for Horizon {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ (Simd32x4::from(self[e3215]) * other.group0()));
    }
}
impl AntiWedge<MultiVector> for Horizon {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0       12        0
    //    simd3        0        4        0
    // Totals...
    // yes simd        0       16        0
    //  no simd        0       24        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self[e3215] * other.group1()[3]), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self[e3215] * other.group3()[0] * -1.0),
                (self[e3215] * other.group3()[1] * -1.0),
                (self[e3215] * other.group3()[2] * -1.0),
                0.0,
            ]),
            // e5
            (self[e3215] * other.group3()[3] * -1.0),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            (Simd32x3::from(self[e3215]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]])),
            // e23, e31, e12
            (Simd32x3::from(self[e3215]) * other.group7()),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self[e3215] * other.group9()[0] * -1.0)]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(self[e3215]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]) * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            (self[e3215] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for Horizon {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self[e3215]) * other.group0()));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for Horizon {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e3215]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullSphereAtOrigin> for Horizon {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self[e3215] * other[e1234] * -1.0));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for Horizon {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self[e3215]) * other.group0()));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for Horizon {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (Simd32x4::from(self[e3215]) * swizzle!(other.group0(), 3, 0, 1, 2) * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<Origin> for Horizon {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e3215] * other[e4]));
    }
}
impl AntiWedge<Plane> for Horizon {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        use crate::elements::*;
        return LineAtInfinity::from_groups(
            // e235, e315, e125
            (Simd32x3::from(self[e3215]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for Horizon {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        use crate::elements::*;
        return LineAtInfinity::from_groups(/* e235, e315, e125 */ (Simd32x3::from(self[e3215]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<RoundPoint> for Horizon {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e3215] * other.group0()[3]));
    }
}
impl AntiWedge<RoundPointAtOrigin> for Horizon {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e3215] * other.group0()[0]));
    }
}
impl AntiWedge<Sphere> for Horizon {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiFlatPoint::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other[e4315]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for Horizon {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self[e3215] * other.group0()[1] * -1.0));
    }
}
impl AntiWedge<SphereOnOrigin> for Horizon {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlatPoint::from_groups(/* e235, e315, e125, e321 */ (Simd32x4::from(self[e3215]) * other.group0() * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<VersorEven> for Horizon {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group3()[3]])),
            // e15, e25, e35, e3215
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Horizon {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]])),
            // e15, e25, e35, e3215
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Horizon {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return FlectorAtInfinity::from_groups(
            // e15, e25, e35, e3215
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[0]])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for Horizon {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self[e3215]) * other.group0()));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Horizon {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]])),
            // e15, e25, e35, e3215
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for Horizon {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group2()[3]])),
        );
    }
}
impl AntiWedge<VersorOdd> for Horizon {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group2()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for Horizon {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (Simd32x4::from(self[e3215]) * other.group0() * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for Horizon {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        use crate::elements::*;
        return MotorAtInfinity::from_groups(
            // e235, e315, e125, e5
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for Horizon {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group1()[3], other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for Horizon {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self[e3215]) * swizzle!(other.group1(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e5
            (Simd32x4::from(self[e3215]) * other.group0() * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for Horizon {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self[e3215]) * Simd32x4::from([other.group2()[3], other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl InfixAntiWedge for Infinity {}
impl AntiWedge<AntiScalar> for Infinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e5] * other[e12345]));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other.group1()[3]));
    }
}
impl AntiWedge<DualNum> for Infinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e5] * other.group0()[1]));
    }
}
impl AntiWedge<Motor> for Infinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e5] * other.group0()[3]));
    }
}
impl AntiWedge<MotorOnOrigin> for Infinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e5] * other.group0()[3]));
    }
}
impl AntiWedge<MultiVector> for Infinity {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self[e5] * other.group9()[0]), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from(0.0),
            // e5
            (self[e5] * other.group0()[1]),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other[e1234]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other.group0()[3]));
    }
}
impl AntiWedge<Sphere> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other[e4315]));
    }
}
impl AntiWedge<SphereAtOrigin> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other.group0()[1]));
    }
}
impl AntiWedge<SphereOnOrigin> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other.group0()[3]));
    }
}
impl AntiWedge<VersorEven> for Infinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e5] * other.group0()[3]));
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Infinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e5] * other.group0()[3]));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Infinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e5] * other.group0()[0]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Infinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self[e5] * other.group0()[3]));
    }
}
impl AntiWedge<VersorOdd> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other.group2()[3]));
    }
}
impl AntiWedge<VersorOddAligningOrigin> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other.group1()[3]));
    }
}
impl AntiWedge<VersorOddAtOrigin> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other.group1()[3]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other.group1()[0]));
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for Infinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e5] * other.group2()[3]));
    }
}
impl InfixAntiWedge for Line {}
impl AntiWedge<AntiCircleOnOrigin> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for Line {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDualNum> for Line {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatOrigin> for Line {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other[e321])));
    }
}
impl AntiWedge<AntiFlatPoint> for Line {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for Line {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for Line {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiLine> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for Line {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for Line {
    type Output = Line;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Line::from_groups(
            // e415, e425, e435
            (self.group0() * Simd32x3::from(other[e12345])),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for Line {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for Line {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<Circle> for Line {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       18        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group0()[2])),
                ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group1()[3]) - (self.group1()[0] * other.group0()[2])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group0()[1])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for Line {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2])
                - (self.group1()[1] * other.group1()[1])
                - (self.group1()[0] * other.group1()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for Line {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleAtOrigin> for Line {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for Line {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for Line {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<Dipole> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for Line {
    type Output = Line;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return Line::from_groups(
            // e415, e425, e435
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for Line {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return FlatPoint::from_groups(
            // e15, e25, e35, e45
            (-(swizzle!(other.group1(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group1()[2])),
                    ((self.group1()[2] * other.group1()[0]) + (self.group0()[1] * other.group1()[3])),
                    ((self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for Line {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<FlectorOnOrigin> for Line {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return FlatPoint::from_groups(
            // e15, e25, e35, e45
            (-(swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[1]),
                    (self.group1()[0] * other.group0()[2]),
                    (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
                ])),
        );
    }
}
impl AntiWedge<Horizon> for Line {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other[e3215])));
    }
}
impl AntiWedge<Line> for Line {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for Line {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for Line {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for Line {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for Line {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for Line {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for Line {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       20       33        0
    //    simd3        2        5        0
    // Totals...
    // yes simd       22       38        0
    //  no simd       26       48        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group1()[2] * other.group3()[2])
                    - (self.group1()[1] * other.group3()[1])
                    - (self.group1()[0] * other.group3()[0])
                    - (self.group0()[2] * other.group5()[2])
                    - (self.group0()[0] * other.group5()[0])
                    - (self.group0()[1] * other.group5()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group7()[1]) + (self.group0()[0] * other.group6()[3]) + (self.group1()[1] * other.group7()[2])),
                ((self.group1()[2] * other.group7()[0]) + (self.group0()[1] * other.group6()[3]) - (self.group1()[0] * other.group7()[2])),
                (-(self.group1()[1] * other.group7()[0]) + (self.group0()[2] * other.group6()[3]) + (self.group1()[0] * other.group7()[1])),
                (-(self.group0()[2] * other.group7()[2]) - (self.group0()[0] * other.group7()[0]) - (self.group0()[1] * other.group7()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group6()[2])
                - (self.group1()[1] * other.group6()[1])
                - (self.group1()[0] * other.group6()[0])
                - (self.group0()[2] * other.group8()[2])
                - (self.group0()[0] * other.group8()[0])
                - (self.group0()[1] * other.group8()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other.group9()[0]),
                (self.group0()[1] * other.group9()[0]),
                (self.group0()[2] * other.group9()[0]),
                (-(self.group0()[2] * other.group9()[3]) - (self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
            ]),
            // e15, e25, e35
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group9()[2], other.group9()[3], other.group9()[1]]))
                + (self.group0() * Simd32x3::from(other[e45]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group9()[3], other.group9()[1], other.group9()[2]]))),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other.group9()[0])),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                0.0,
            ]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for Line {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for Line {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for Line {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other[e1234])),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other[e1234])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for Line {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for Line {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Plane> for Line {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return FlatPoint::from_groups(
            // e15, e25, e35, e45
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for Line {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ Simd32x4::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Sphere> for Line {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        2        4        0
    // Totals...
    // yes simd        4       10        0
    //  no simd        8       18        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other[e4315])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other[e4315]),
                (self.group1()[1] * other[e4315]),
                (self.group1()[2] * other[e4315]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (-(swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (self.group0() * Simd32x3::from(other.group0()[3]))
                + (swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for Line {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            (self.group1() * Simd32x3::from(other.group0()[1])),
            // e15, e25, e35
            (self.group0() * Simd32x3::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for Line {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        1        3        0
    // Totals...
    // yes simd        3        9        0
    //  no simd        5       15        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other.group0()[3])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            ((swizzle!(self.group1(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group1(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEven> for Line {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       20        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       21        0
    //  no simd       13       24        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group1()[3])),
                    ((self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Line {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       18        0
    //  no simd       10       21        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Line {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for Line {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4        9        0
    //  no simd        7       12        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Line {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4       15        0
    //  no simd        7       18        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for Line {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for Line {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group2()[3]),
                (self.group1()[1] * other.group2()[3]),
                (self.group1()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group3()[2]) - (self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group3()[1]) + (self.group0()[0] * other.group3()[3]) + (self.group1()[1] * other.group3()[2])),
                ((self.group1()[2] * other.group3()[0]) + (self.group0()[1] * other.group3()[3]) - (self.group1()[0] * other.group3()[2])),
                (-(self.group1()[1] * other.group3()[0]) + (self.group0()[2] * other.group3()[3]) + (self.group1()[0] * other.group3()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for Line {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       21        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group2()[1]) + (self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3]) - (self.group1()[0] * other.group2()[2])),
                (-(self.group1()[1] * other.group2()[0]) + (self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for Line {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                (-(self.group1()[2] * other.group2()[1]) + (self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3]) - (self.group1()[0] * other.group2()[2])),
                (-(self.group1()[1] * other.group2()[0]) + (self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for Line {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       12        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for Line {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[2] * other.group1()[0]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[1] * other.group1()[0]),
                (self.group1()[2] * other.group1()[0]),
                (-(self.group0()[2] * other.group1()[3]) - (self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[3]) - (self.group1()[2] * other.group1()[2])),
                (-(self.group1()[0] * other.group1()[3]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[1] * other.group1()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for Line {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group2()[3]),
                (self.group1()[1] * other.group2()[3]),
                (self.group1()[2] * other.group2()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl InfixAntiWedge for LineAtInfinity {}
impl AntiWedge<AntiCircleOnOrigin> for LineAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for LineAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<AntiScalar> for LineAtInfinity {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return LineAtInfinity::from_groups(/* e235, e315, e125 */ (self.group0() * Simd32x3::from(other[e12345])));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for LineAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for LineAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<Circle> for LineAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleAligningOrigin> for LineAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleAtInfinity> for LineAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for LineAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for LineAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for LineAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<Dipole> for LineAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for LineAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for LineAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for LineAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for LineAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DualNum> for LineAtInfinity {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return LineAtInfinity::from_groups(/* e235, e315, e125 */ (self.group0() * Simd32x3::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for LineAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group1()[2], other.group1()[0], other.group1()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[0]]))),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for LineAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[3], other.group0()[1], other.group0()[2]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[2], other.group0()[3], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<Line> for LineAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for LineAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for LineAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for LineAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MultiVector> for LineAtInfinity {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd3        1        4        0
    // Totals...
    // yes simd        8       16        0
    //  no simd       10       24        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group0()[2] * other.group3()[2]) - (self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[1] * other.group7()[2]) - (self.group0()[2] * other.group7()[1])),
                (-(self.group0()[0] * other.group7()[2]) + (self.group0()[2] * other.group7()[0])),
                ((self.group0()[0] * other.group7()[1]) - (self.group0()[1] * other.group7()[0])),
                0.0,
            ]),
            // e5
            (-(self.group0()[2] * other.group6()[2]) - (self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group9()[3], other.group9()[1], other.group9()[2]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group9()[2], other.group9()[3], other.group9()[1]]))),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group9()[0])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for LineAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for LineAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for LineAtInfinity {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group0() * Simd32x3::from(other[e1234])));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for LineAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for LineAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Plane> for LineAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for LineAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            ((swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Sphere> for LineAtInfinity {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        3        0
    // no simd        3        9        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other[e4315])),
            // e15, e25, e35
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for LineAtInfinity {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group0() * Simd32x3::from(other.group0()[1])));
    }
}
impl AntiWedge<SphereOnOrigin> for LineAtInfinity {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        3        0
    // no simd        3        9        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[3])),
            // e15, e25, e35
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEven> for LineAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for LineAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for LineAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0]),
            (self.group0()[1] * other.group0()[0]),
            (self.group0()[2] * other.group0()[0]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenAtOrigin> for LineAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for LineAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for LineAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorOdd> for LineAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group3()[2]) - (self.group0()[2] * other.group3()[1])),
                (-(self.group0()[0] * other.group3()[2]) + (self.group0()[2] * other.group3()[0])),
                ((self.group0()[0] * other.group3()[1]) - (self.group0()[1] * other.group3()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for LineAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group2()[2]) - (self.group0()[2] * other.group2()[1])),
                (-(self.group0()[0] * other.group2()[2]) + (self.group0()[2] * other.group2()[0])),
                ((self.group0()[0] * other.group2()[1]) - (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for LineAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group2()[2], other.group2()[0], other.group2()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group2()[1], other.group2()[2], other.group2()[0]]))),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for LineAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for LineAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[2] * other.group1()[0]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[3]) - (self.group0()[2] * other.group1()[2])),
                (-(self.group0()[0] * other.group1()[3]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[1] * other.group1()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for LineAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group2()[3]),
            (self.group0()[1] * other.group2()[3]),
            (self.group0()[2] * other.group2()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl InfixAntiWedge for LineOnOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for LineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for LineOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiDualNum> for LineOnOrigin {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatOrigin> for LineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other[e321])));
    }
}
impl AntiWedge<AntiFlatPoint> for LineOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for LineOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for LineOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiLine> for LineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for LineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for LineOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for LineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for LineOnOrigin {
    type Output = LineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return LineOnOrigin::from_groups(/* e415, e425, e435 */ (self.group0() * Simd32x3::from(other[e12345])));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for LineOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for LineOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Circle> for LineOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for LineOnOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        2        3        0
    // no simd        4        6        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (-(Simd32x2::from(self.group0()[2]) * Simd32x2::from([other.group0()[2], other.group2()[2]]))
                - (Simd32x2::from(self.group0()[0]) * Simd32x2::from([other.group0()[0], other.group2()[0]]))
                - (Simd32x2::from(self.group0()[1]) * Simd32x2::from([other.group0()[1], other.group2()[1]]))),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for LineOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleAtOrigin> for LineOnOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        2        3        0
    // no simd        4        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (-(Simd32x2::from(self.group0()[2]) * Simd32x2::from([other.group0()[2], other.group1()[2]]))
                - (Simd32x2::from(self.group0()[0]) * Simd32x2::from([other.group0()[0], other.group1()[0]]))
                - (Simd32x2::from(self.group0()[1]) * Simd32x2::from([other.group0()[1], other.group1()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for LineOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for LineOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<Dipole> for LineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for LineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for LineOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for LineOnOrigin {
    type Output = LineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return LineOnOrigin::from_groups(/* e415, e425, e435 */ (self.group0() * Simd32x3::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for LineOnOrigin {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<FlectorAtInfinity> for LineOnOrigin {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<FlectorOnOrigin> for LineOnOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return FlatOrigin::from_groups(
            // e45
            (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Horizon> for LineOnOrigin {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (self.group0() * Simd32x3::from(other[e3215])));
    }
}
impl AntiWedge<Line> for LineOnOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for LineOnOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for LineOnOrigin {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for LineOnOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for LineOnOrigin {
    type Output = LineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return LineOnOrigin::from_groups(/* e415, e425, e435 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<MultiVector> for LineOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       21        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        8       22        0
    //  no simd        8       24        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group0()[2] * other.group5()[2]) - (self.group0()[0] * other.group5()[0]) - (self.group0()[1] * other.group5()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group6()[3]),
                (self.group0()[1] * other.group6()[3]),
                (self.group0()[2] * other.group6()[3]),
                (-(self.group0()[2] * other.group7()[2]) - (self.group0()[0] * other.group7()[0]) - (self.group0()[1] * other.group7()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group8()[2]) - (self.group0()[0] * other.group8()[0]) - (self.group0()[1] * other.group8()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other.group9()[0]),
                (self.group0()[1] * other.group9()[0]),
                (self.group0()[2] * other.group9()[0]),
                (-(self.group0()[2] * other.group9()[3]) - (self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
            ]),
            // e15, e25, e35
            (self.group0() * Simd32x3::from(other[e45])),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                0.0,
            ]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for LineOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for LineOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (self.group0() * Simd32x3::from(other[e1234])));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for LineOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for LineOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<Plane> for LineOnOrigin {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<PlaneOnOrigin> for LineOnOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return FlatOrigin::from_groups(
            // e45
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for LineOnOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2        9        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other[e4315]),
                (self.group0()[1] * other[e4315]),
                (self.group0()[2] * other[e4315]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (self.group0() * Simd32x3::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for LineOnOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e15, e25, e35
            (self.group0() * Simd32x3::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for LineOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorEven> for LineOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for LineOnOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for LineOnOrigin {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for LineOnOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        2        3        0
    // no simd        4        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (-(Simd32x2::from(self.group0()[2]) * Simd32x2::from([other.group0()[2], other.group1()[2]]))
                - (Simd32x2::from(self.group0()[0]) * Simd32x2::from([other.group0()[0], other.group1()[0]]))
                - (Simd32x2::from(self.group0()[1]) * Simd32x2::from([other.group0()[1], other.group1()[1]]))),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for LineOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for LineOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for LineOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group3()[2]) - (self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group3()[3]),
                (self.group0()[1] * other.group3()[3]),
                (self.group0()[2] * other.group3()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for LineOnOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2        9        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e15, e25, e35
            (self.group0() * Simd32x3::from(other.group2()[3])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for LineOnOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for LineOnOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (self.group0() * Simd32x3::from(other.group1()[3])),
            // e15, e25, e35
            (self.group0() * Simd32x3::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for LineOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ Simd32x4::from([
            (self.group0()[0] * other.group1()[0]),
            (self.group0()[1] * other.group1()[0]),
            (self.group0()[2] * other.group1()[0]),
            (-(self.group0()[2] * other.group1()[3]) - (self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
        ]));
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for LineOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl InfixAntiWedge for Motor {}
impl AntiWedge<AntiCircleOnOrigin> for Motor {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for Motor {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDualNum> for Motor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e15, e25, e35, e3215
            (self.group0() * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for Motor {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other[e321])));
    }
}
impl AntiWedge<AntiFlatPoint> for Motor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e1, e2, e3, e5
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for Motor {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e1, e2, e3, e5
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for Motor {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            ((self.group0()[0] * other.group0()[0]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[1] * other.group0()[0]) + (self.group0()[3] * other.group0()[2])),
            ((self.group0()[2] * other.group0()[0]) + (self.group0()[3] * other.group0()[3])),
        ]));
    }
}
impl AntiWedge<AntiLine> for Motor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for Motor {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiMotor> for Motor {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for Motor {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for Motor {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for Motor {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiScalar> for Motor {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Motor::from_groups(
            // e415, e425, e435, e12345
            (self.group0() * Simd32x4::from(other[e12345])),
            // e235, e315, e125, e5
            (self.group1() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for Motor {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for Motor {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + (Simd32x4::from(self.group0()[3]) * other.group0())
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group1()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for Motor {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from([other.group1()[0], other.group0()[3], other.group0()[3], other.group0()[3]]))
                - (swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[2]) + (self.group0()[3] * other.group1()[1])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[2])),
                    ((self.group1()[0] * other.group0()[1]) + (self.group0()[3] * other.group1()[3])),
                ])),
        );
    }
}
impl AntiWedge<Circle> for Motor {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       24        0
    //    simd4        0        1        0
    // Totals...
    // yes simd       13       25        0
    //  no simd       13       28        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group0()[2])),
                ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group1()[3]) - (self.group1()[0] * other.group0()[2])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group0()[1])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for Motor {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       24        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for Motor {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       13        0
    //  no simd        5       16        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
            ]),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for Motor {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for Motor {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for Motor {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       14        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        7       16        0
    //  no simd       10       22        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<Dipole> for Motor {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       13        0
    //  no simd        5       16        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for Motor {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       10        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for Motor {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for Motor {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for Motor {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from(0.0),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for Motor {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DualNum> for Motor {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        1        6        0
    //  no simd        1        9        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[1] * other.group0()[1]),
                (self.group1()[2] * other.group0()[1]),
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[3] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<FlatOrigin> for Motor {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self.group0()[3] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for Motor {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<FlatPointAtInfinity> for Motor {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Flector> for Motor {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            (-(swizzle!(other.group1(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + (self.group0() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[3]]))
                + Simd32x4::from([
                    ((self.group1()[1] * other.group1()[2]) + (self.group0()[3] * other.group0()[0])),
                    ((self.group1()[2] * other.group1()[0]) + (self.group0()[3] * other.group0()[1])),
                    ((self.group1()[0] * other.group1()[1]) + (self.group0()[3] * other.group0()[2])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for Motor {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for Motor {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            ((swizzle!(other.group0(), 3, 1, 2, 0) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[0], self.group0()[3]]))
                - (swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2]))])),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (self.group0()[3] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Horizon> for Motor {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ (self.group0() * Simd32x4::from(other[e3215])));
    }
}
impl AntiWedge<Infinity> for Motor {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for Motor {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<LineAtInfinity> for Motor {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for Motor {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<Motor> for Motor {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group1())
                + (self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for Motor {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for Motor {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for Motor {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       30       49        0
    //    simd3        4        7        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       36       59        0
    //  no simd       50       82        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other.group9()[0]) - (self.group1()[2] * other.group3()[2]) - (self.group1()[1] * other.group3()[1]) - (self.group1()[0] * other.group3()[0])
                    + (self.group0()[3] * other.group0()[0])
                    - (self.group0()[2] * other.group5()[2])
                    - (self.group0()[0] * other.group5()[0])
                    - (self.group0()[1] * other.group5()[1])),
                (self.group0()[3] * other.group0()[1]),
            ]),
            // e1, e2, e3, e4
            ((self.group0() * Simd32x4::from([other.group6()[3], other.group6()[3], other.group6()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group7()[1]) + (self.group1()[1] * other.group7()[2]) + (self.group0()[3] * other.group1()[0])),
                    ((self.group1()[2] * other.group7()[0]) - (self.group1()[0] * other.group7()[2]) + (self.group0()[3] * other.group1()[1])),
                    (-(self.group1()[1] * other.group7()[0]) + (self.group1()[0] * other.group7()[1]) + (self.group0()[3] * other.group1()[2])),
                    (-(self.group0()[2] * other.group7()[2]) - (self.group0()[0] * other.group7()[0]) - (self.group0()[1] * other.group7()[1])),
                ])),
            // e5
            ((self.group1()[3] * other.group0()[1]) - (self.group1()[2] * other.group6()[2]) - (self.group1()[1] * other.group6()[1]) - (self.group1()[0] * other.group6()[0])
                + (self.group0()[3] * other[e1])
                - (self.group0()[2] * other.group8()[2])
                - (self.group0()[0] * other.group8()[0])
                - (self.group0()[1] * other.group8()[1])),
            // e41, e42, e43, e45
            ((self.group0() * Simd32x4::from([other.group9()[0], other.group9()[0], other.group9()[0], other.group3()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group3()[0]),
                    (self.group0()[3] * other.group3()[1]),
                    (self.group0()[3] * other.group3()[2]),
                    (-(self.group0()[2] * other.group9()[3]) - (self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * other.group4())
                + Simd32x3::from([
                    (-(self.group1()[2] * other.group9()[2]) + (self.group1()[1] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[1]) - (self.group1()[0] * other.group9()[3])),
                    (-(self.group1()[1] * other.group9()[1]) + (self.group1()[0] * other.group9()[2])),
                ])),
            // e23, e31, e12
            ((Simd32x3::from(self.group0()[3]) * other.group5()) + (Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[1]) + (self.group0()[3] * other.group6()[0])),
                ((self.group0()[1] * other.group0()[1]) + (self.group0()[3] * other.group6()[1])),
                ((self.group0()[2] * other.group0()[1]) + (self.group0()[3] * other.group6()[2])),
                (self.group0()[3] * other.group6()[3]),
            ]),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[3]) * other.group7()),
            // e235, e315, e125
            ((Simd32x3::from(self.group0()[3]) * other.group8()) + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[3]) * other.group9()),
            // e3215
            (self.group0()[3] * other[e45]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for Motor {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for Motor {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for Motor {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
            // e23, e31, e12, e1234
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for Motor {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e4, e1, e2, e3
            ((swizzle!(other.group0(), 3, 2, 0, 1) * Simd32x4::from([self.group0()[3], self.group1()[1], self.group1()[2], self.group1()[0]]))
                - (swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group0()[2], self.group1()[2], self.group1()[0], self.group1()[1]]))
                + Simd32x4::from([(-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for Motor {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<Origin> for Motor {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for Motor {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for Motor {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for Motor {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e5
            (self.group0()[3] * other[e2]),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for Motor {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (Simd32x2::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Scalar> for Motor {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Scalar) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[scalar]));
    }
}
impl AntiWedge<Sphere> for Motor {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       16        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        8       18        0
    //  no simd        8       24        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(other[e4315]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other[e4315]),
                (self.group1()[1] * other[e4315]),
                (self.group1()[2] * other[e4315]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3]) - (self.group1()[0] * other.group0()[2])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                (self.group0()[3] * other[e4315]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for Motor {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        4        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        6        0
    //  no simd        0       12        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[1] * other.group0()[1]),
                (self.group1()[2] * other.group0()[1]),
                (self.group0()[3] * other.group0()[0]),
            ]),
            // e15, e25, e35, e1234
            (self.group0() * Simd32x4::from([other.group0()[0], other.group0()[0], other.group0()[0], other.group0()[1]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for Motor {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       17        0
    //  no simd        5       20        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for Motor {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       21        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       16       26        0
    //  no simd       28       41        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group2())
                + (self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + (self.group0() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group3()[3]]))
                + Simd32x4::from([
                    ((self.group1()[1] * other.group0()[2]) + (self.group0()[3] * other.group3()[0])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group3()[1])),
                    ((self.group1()[0] * other.group0()[1]) + (self.group0()[3] * other.group3()[2])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Motor {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       18        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       22       34        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
                0.0,
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group2())
                + (self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    ((self.group0()[3] * other.group1()[3]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Motor {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group0()[2])),
                ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group0()[3])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[0]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group2())
                + (self.group1() * Simd32x4::from(other.group0()[0]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for Motor {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       14        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[0], self.group0()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1]))])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Motor {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       19        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        9       21        0
    //  no simd       12       27        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    ((self.group0()[3] * other.group1()[3]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for Motor {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       15        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        9       18        0
    //  no simd       15       27        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + (self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group2()[3]]))
                + Simd32x4::from([
                    ((self.group1()[1] * other.group0()[2]) + (self.group0()[3] * other.group2()[0])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group2()[1])),
                    ((self.group1()[0] * other.group0()[1]) + (self.group0()[3] * other.group2()[2])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOdd> for Motor {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       25        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       19       29        0
    //  no simd       28       41        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + (Simd32x4::from(self.group0()[3]) * other.group0())
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e45
            ((Simd32x4::from(self.group0()[3]) * other.group1())
                + Simd32x4::from([
                    (self.group1()[0] * other.group2()[3]),
                    (self.group1()[1] * other.group2()[3]),
                    (self.group1()[2] * other.group2()[3]),
                    (-(self.group0()[2] * other.group3()[2]) - (self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group3()[1])
                    + (self.group1()[1] * other.group3()[2])
                    + (self.group0()[0] * other.group3()[3])
                    + (self.group0()[3] * other.group2()[0])),
                ((self.group1()[2] * other.group3()[0]) - (self.group1()[0] * other.group3()[2]) + (self.group0()[1] * other.group3()[3]) + (self.group0()[3] * other.group2()[1])),
                (-(self.group1()[1] * other.group3()[0])
                    + (self.group1()[0] * other.group3()[1])
                    + (self.group0()[2] * other.group3()[3])
                    + (self.group0()[3] * other.group2()[2])),
                (self.group0()[3] * other.group2()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group3()),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for Motor {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       26        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       15       28        0
    //  no simd       18       34        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group2()[1])
                    + (self.group1()[1] * other.group2()[2])
                    + (self.group0()[0] * other.group2()[3])
                    + (self.group0()[3] * other.group1()[0])),
                ((self.group1()[2] * other.group2()[0]) - (self.group1()[0] * other.group2()[2]) + (self.group0()[1] * other.group2()[3]) + (self.group0()[3] * other.group1()[1])),
                (-(self.group1()[1] * other.group2()[0])
                    + (self.group1()[0] * other.group2()[1])
                    + (self.group0()[2] * other.group2()[3])
                    + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for Motor {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       19        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       12       21        0
    //  no simd       15       27        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from([other.group0()[0], other.group2()[3], other.group2()[3], other.group2()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    (-(self.group1()[2] * other.group2()[1]) + (self.group1()[1] * other.group2()[2]) + (self.group0()[3] * other.group0()[1])),
                    ((self.group1()[2] * other.group2()[0]) - (self.group1()[0] * other.group2()[2]) + (self.group0()[3] * other.group0()[2])),
                    (-(self.group1()[1] * other.group2()[0]) + (self.group1()[0] * other.group2()[1]) + (self.group0()[3] * other.group0()[3])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for Motor {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       18        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group1()[3]),
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for Motor {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       23        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       24        0
    //  no simd       12       27        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group1()[0]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[1] * other.group1()[0]),
                (self.group1()[2] * other.group1()[0]),
                ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[3]) - (self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[3]) - (self.group1()[2] * other.group1()[2])),
                (-(self.group1()[0] * other.group1()[3]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[1] * other.group1()[1])),
                (self.group0()[3] * other.group1()[0]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (self.group0()[3] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for Motor {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + (Simd32x4::from(self.group0()[3]) * other.group0())
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group2()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group2()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group2()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group2()[0])),
                ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group2()[1])),
                ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group2()[2])),
                (self.group0()[3] * other.group2()[3]),
            ]),
        );
    }
}
impl InfixAntiWedge for MotorAtInfinity {}
impl AntiWedge<AntiCircleOnOrigin> for MotorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for MotorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for MotorAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for MotorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for MotorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Circle> for MotorAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[1] * other.group0()[2]),
                    (self.group0()[2] * other.group0()[0]),
                    (self.group0()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for MotorAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[1] * other.group0()[2]),
                    (self.group0()[2] * other.group0()[0]),
                    (self.group0()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for MotorAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for MotorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for MotorAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[1] * other.group0()[2]),
                    (self.group0()[2] * other.group0()[0]),
                    (self.group0()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for MotorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Dipole> for MotorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for MotorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for MotorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for MotorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for MotorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DualNum> for MotorAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for MotorAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ Simd32x3::from([
            ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
            (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
            ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for MotorAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[3]) - (self.group0()[2] * other.group0()[2])),
            (-(self.group0()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Line> for MotorAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineOnOrigin> for MotorAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for MotorAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for MotorAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MultiVector> for MotorAtInfinity {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       20        0
    //    simd3        0        2        0
    // Totals...
    // yes simd       12       22        0
    //  no simd       12       26        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other.group9()[0]) - (self.group0()[2] * other.group3()[2]) - (self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[1] * other.group7()[2]) - (self.group0()[2] * other.group7()[1])),
                (-(self.group0()[0] * other.group7()[2]) + (self.group0()[2] * other.group7()[0])),
                ((self.group0()[0] * other.group7()[1]) - (self.group0()[1] * other.group7()[0])),
                0.0,
            ]),
            // e5
            ((self.group0()[3] * other.group0()[1]) - (self.group0()[2] * other.group6()[2]) - (self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from([
                ((self.group0()[1] * other.group9()[3]) - (self.group0()[2] * other.group9()[2])),
                (-(self.group0()[0] * other.group9()[3]) + (self.group0()[2] * other.group9()[1])),
                ((self.group0()[0] * other.group9()[2]) - (self.group0()[1] * other.group9()[1])),
            ]),
            // e23, e31, e12
            (Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for MotorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for MotorAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for MotorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group0() * Simd32x4::from(other[e1234])));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for MotorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for MotorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Plane> for MotorAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<PlaneOnOrigin> for MotorAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Sphere> for MotorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        3       10        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (self.group0() * Simd32x4::from(other[e4315])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for MotorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<SphereOnOrigin> for MotorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        3       10        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for MotorAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            ((swizzle!(self.group0(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1]))])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for MotorAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            ((swizzle!(self.group0(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1]))])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for MotorAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0]),
            (self.group0()[1] * other.group0()[0]),
            (self.group0()[2] * other.group0()[0]),
            ((self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenAtOrigin> for MotorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for MotorAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            ((swizzle!(self.group0(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1]))])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for MotorAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for MotorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       13        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                ((self.group0()[3] * other.group2()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group3()[2]) - (self.group0()[2] * other.group3()[1])),
                (-(self.group0()[0] * other.group3()[2]) + (self.group0()[2] * other.group3()[0])),
                ((self.group0()[0] * other.group3()[1]) - (self.group0()[1] * other.group3()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for MotorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       13        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group2()[2]) - (self.group0()[2] * other.group2()[1])),
                (-(self.group0()[0] * other.group2()[2]) + (self.group0()[2] * other.group2()[0])),
                ((self.group0()[0] * other.group2()[1]) - (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for MotorAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ Simd32x3::from([
            ((self.group0()[1] * other.group2()[2]) - (self.group0()[2] * other.group2()[1])),
            (-(self.group0()[0] * other.group2()[2]) + (self.group0()[2] * other.group2()[0])),
            ((self.group0()[0] * other.group2()[1]) - (self.group0()[1] * other.group2()[0])),
        ]));
    }
}
impl AntiWedge<VersorOddAtOrigin> for MotorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for MotorAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       13        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[2] * other.group1()[0]),
                ((self.group0()[3] * other.group1()[0]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[3]) - (self.group0()[2] * other.group1()[2])),
                (-(self.group0()[0] * other.group1()[3]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[1] * other.group1()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for MotorAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group2()[3]),
            (self.group0()[1] * other.group2()[3]),
            (self.group0()[2] * other.group2()[3]),
            ((self.group0()[3] * other.group2()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl InfixAntiWedge for MotorOnOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for MotorOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for MotorOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiDualNum> for MotorOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e15, e25, e35, e3215
            (self.group0() * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for MotorOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other[e321])));
    }
}
impl AntiWedge<AntiFlatPoint> for MotorOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e1, e2, e3, e5
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for MotorOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e1, e2, e3, e5
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for MotorOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            ((self.group0()[0] * other.group0()[0]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[1] * other.group0()[0]) + (self.group0()[3] * other.group0()[2])),
            ((self.group0()[2] * other.group0()[0]) + (self.group0()[3] * other.group0()[3])),
        ]));
    }
}
impl AntiWedge<AntiLine> for MotorOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for MotorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiMotor> for MotorOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for MotorOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for MotorOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for MotorOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiScalar> for MotorOnOrigin {
    type Output = MotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return MotorOnOrigin::from_groups(/* e415, e425, e435, e12345 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for MotorOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for MotorOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e1234
            (Simd32x4::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for MotorOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from([other.group1()[0], other.group0()[3], other.group0()[3], other.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (self.group0()[3] * other.group1()[3]),
                ])),
        );
    }
}
impl AntiWedge<Circle> for MotorOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       15        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        4       16        0
    //  no simd        4       19        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for MotorOnOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       15        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for MotorOnOrigin {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2       10        0
    //  no simd        2       13        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
            ]),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for MotorOnOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for MotorOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for MotorOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        4       13        0
    //  no simd        4       16        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<Dipole> for MotorOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2       10        0
    //  no simd        2       13        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for MotorOnOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        7        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for MotorOnOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for MotorOnOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[3]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for MotorOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for MotorOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       12        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DualNum> for MotorOnOrigin {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125, e5
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[0])]),
        );
    }
}
impl AntiWedge<FlatOrigin> for MotorOnOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self.group0()[3] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for MotorOnOrigin {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<FlatPointAtInfinity> for MotorOnOrigin {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Flector> for MotorOnOrigin {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            ((self.group0() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for MotorOnOrigin {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for MotorOnOrigin {
    type Output = FlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return FlectorOnOrigin::from_groups(/* e45, e4235, e4315, e4125 */ Simd32x4::from([
            ((self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<Horizon> for MotorOnOrigin {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ (self.group0() * Simd32x4::from(other[e3215])));
    }
}
impl AntiWedge<Infinity> for MotorOnOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for MotorOnOrigin {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<LineAtInfinity> for MotorOnOrigin {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for MotorOnOrigin {
    type Output = LineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return LineOnOrigin::from_groups(/* e415, e425, e435 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Motor> for MotorOnOrigin {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for MotorOnOrigin {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for MotorOnOrigin {
    type Output = MotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return MotorOnOrigin::from_groups(/* e415, e425, e435, e12345 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<MultiVector> for MotorOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       29        0
    //    simd3        1        5        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       16       37        0
    //  no simd       24       56        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group5()[2]) - (self.group0()[0] * other.group5()[0]) - (self.group0()[1] * other.group5()[1])),
                (self.group0()[3] * other.group0()[1]),
            ]),
            // e1, e2, e3, e4
            ((self.group0() * Simd32x4::from([other.group6()[3], other.group6()[3], other.group6()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group0()[2] * other.group7()[2]) - (self.group0()[0] * other.group7()[0]) - (self.group0()[1] * other.group7()[1])),
                ])),
            // e5
            ((self.group0()[3] * other[e1]) - (self.group0()[2] * other.group8()[2]) - (self.group0()[0] * other.group8()[0]) - (self.group0()[1] * other.group8()[1])),
            // e41, e42, e43, e45
            ((self.group0() * Simd32x4::from([other.group9()[0], other.group9()[0], other.group9()[0], other.group3()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group3()[0]),
                    (self.group0()[3] * other.group3()[1]),
                    (self.group0()[3] * other.group3()[2]),
                    (-(self.group0()[2] * other.group9()[3]) - (self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])) + (Simd32x3::from(self.group0()[3]) * other.group4())),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group5()),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[1]) + (self.group0()[3] * other.group6()[0])),
                ((self.group0()[1] * other.group0()[1]) + (self.group0()[3] * other.group6()[1])),
                ((self.group0()[2] * other.group0()[1]) + (self.group0()[3] * other.group6()[2])),
                (self.group0()[3] * other.group6()[3]),
            ]),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[3]) * other.group7()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[3]) * other.group8()),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[3]) * other.group9()),
            // e3215
            (self.group0()[3] * other[e45]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for MotorOnOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for MotorOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<NullSphereAtOrigin> for MotorOnOrigin {
    type Output = NullVersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullVersorOddAtOrigin::from_groups(/* e41, e42, e43, e1234 */ (self.group0() * Simd32x4::from(other[e1234])));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for MotorOnOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for MotorOnOrigin {
    type Output = NullVersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return NullVersorOddAtOrigin::from_groups(/* e41, e42, e43, e1234 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<Origin> for MotorOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for MotorOnOrigin {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for MotorOnOrigin {
    type Output = FlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return FlectorOnOrigin::from_groups(/* e45, e4235, e4315, e4125 */ Simd32x4::from([
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
        ]));
    }
}
impl AntiWedge<RoundPoint> for MotorOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e5
            (self.group0()[3] * other[e2]),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for MotorOnOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (Simd32x2::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Scalar> for MotorOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Scalar) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[scalar]));
    }
}
impl AntiWedge<Sphere> for MotorOnOrigin {
    type Output = VersorOddAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        2        8        0
    //  no simd        2       14        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorOddAligningOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other[e4315]),
                (self.group0()[1] * other[e4315]),
                (self.group0()[2] * other[e4315]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            (self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other[e4315]])),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for MotorOnOrigin {
    type Output = VersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorOddAtOrigin::from_groups(
            // e41, e42, e43, e3215
            (self.group0() * Simd32x4::from([other.group0()[1], other.group0()[1], other.group0()[1], other.group0()[0]])),
            // e15, e25, e35, e1234
            (self.group0() * Simd32x4::from([other.group0()[0], other.group0()[0], other.group0()[0], other.group0()[1]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for MotorOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[3]) * swizzle!(other.group0(), 3, 0, 1, 2)),
        );
    }
}
impl AntiWedge<VersorEven> for MotorOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       12       28        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                ((self.group0()[3] * other.group2()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            ((self.group0() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group3()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group3()[0]),
                    (self.group0()[3] * other.group3()[1]),
                    (self.group0()[3] * other.group3()[2]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for MotorOnOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e415, e425, e435, e4
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                ((self.group0()[3] * other.group2()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for MotorOnOrigin {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       21        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group0()[2])),
                ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group0()[3])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[0]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                ((self.group0()[3] * other.group2()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for MotorOnOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for MotorOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e415, e425, e435, e4
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for MotorOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group2()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group2()[0]),
                    (self.group0()[3] * other.group2()[1]),
                    (self.group0()[3] * other.group2()[2]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOdd> for MotorOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       12       28        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group3()[2]) - (self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group3()[3]) + (self.group0()[3] * other.group2()[0])),
                ((self.group0()[1] * other.group3()[3]) + (self.group0()[3] * other.group2()[1])),
                ((self.group0()[2] * other.group3()[3]) + (self.group0()[3] * other.group2()[2])),
                (self.group0()[3] * other.group2()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group3()),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for MotorOnOrigin {
    type Output = VersorOddAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOddAligningOrigin::from_groups(
            // e41, e42, e43, e45
            ((self.group0() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group2()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group2()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for MotorOnOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from([other.group0()[0], other.group2()[3], other.group2()[3], other.group2()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (self.group0()[3] * other.group0()[3]),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for MotorOnOrigin {
    type Output = VersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddAtOrigin::from_groups(
            // e41, e42, e43, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for MotorOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            ((self.group0() * Simd32x4::from([other.group1()[0], other.group1()[0], other.group1()[0], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group0()[2] * other.group1()[3]) - (self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
                ])),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for MotorOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group2()[0])),
                ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group2()[1])),
                ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group2()[2])),
                (self.group0()[3] * other.group2()[3]),
            ]),
        );
    }
}
impl InfixAntiWedge for MultiVector {}
impl AntiWedge<AntiCircleOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       17        0
    //    simd3        0        1        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       19        0
    //  no simd       13       24        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group6()[2] * other.group1()[2])
                    - (self.group6()[0] * other.group1()[0])
                    - (self.group6()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self[e45] * other.group0()[0]) - (self.group9()[3] * other.group1()[1])),
                    (-(self[e45] * other.group0()[1]) - (self.group9()[1] * other.group1()[2])),
                    (-(self[e45] * other.group0()[2]) - (self.group9()[2] * other.group1()[0])),
                    ((self.group9()[1] * other.group0()[0]) + (self.group9()[2] * other.group0()[1])),
                ])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       19        0
    //    simd3        1        3        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       12       23        0
    //  no simd       17       32        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    - (self.group3()[3] * other.group0()[3])
                    - (self.group4()[0] * other.group0()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group8()[2], self.group8()[0], self.group8()[1], self.group6()[2]]))
                + Simd32x4::from([
                    ((self.group6()[0] * other.group0()[3]) + (self.group8()[1] * other.group0()[2])),
                    ((self.group8()[2] * other.group0()[0]) + (self.group6()[1] * other.group0()[3])),
                    ((self.group6()[2] * other.group0()[3]) + (self.group8()[0] * other.group0()[1])),
                    (-(self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
                ])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group9()[2] * other.group0()[2]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[3] * other.group0()[0])),
                ((self.group9()[1] * other.group0()[1]) - (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (-(Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                + (Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiDualNum> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd3        0        3        0
    // Totals...
    // yes simd        1       11        0
    //  no simd        1       17        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([((self.group0()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[0])), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group3()[0] * other.group0()[0]),
                (self.group3()[1] * other.group0()[0]),
                (self.group3()[2] * other.group0()[0]),
                0.0,
            ]),
            // e5
            (self.group3()[3] * other.group0()[0]),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]])),
            // e23, e31, e12
            (self.group7() * Simd32x3::from(other.group0()[0])),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group9()[0] * other.group0()[0])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            (self.group0()[1] * other.group0()[0]),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        6        0
    //    simd3        0        2        0
    // Totals...
    // yes simd        0        8        0
    //  no simd        0       12        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self.group3()[3] * other[e321] * -1.0), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([(self.group6()[0] * other[e321]), (self.group6()[1] * other[e321]), (self.group6()[2] * other[e321]), 0.0]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(other[e321]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]) * Simd32x3::from(-1.0)),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other[e321])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiFlatPoint> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       23        0
    //    simd3        1        3        0
    // Totals...
    // yes simd       15       26        0
    //  no simd       17       32        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group3()[3] * other.group0()[3])
                    - (self.group3()[2] * other.group0()[2])
                    - (self.group3()[0] * other.group0()[0])
                    - (self.group3()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group7()[2] * other.group0()[1]) + (self.group6()[0] * other.group0()[3]) - (self.group7()[1] * other.group0()[2])),
                (-(self.group7()[2] * other.group0()[0]) + (self.group6()[1] * other.group0()[3]) + (self.group7()[0] * other.group0()[2])),
                ((self.group7()[1] * other.group0()[0]) + (self.group6()[2] * other.group0()[3]) - (self.group7()[0] * other.group0()[1])),
                0.0,
            ]),
            // e5
            (-(self.group6()[2] * other.group0()[2]) - (self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from([
                (-(self.group9()[2] * other.group0()[2]) + (self.group9()[3] * other.group0()[1])),
                ((self.group9()[1] * other.group0()[2]) - (self.group9()[3] * other.group0()[0])),
                (-(self.group9()[1] * other.group0()[1]) + (self.group9()[2] * other.group0()[0])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                - (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiFlector> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       22       31        0
    //    simd3        1        3        0
    // Totals...
    // yes simd       23       34        0
    //  no simd       25       40        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group9()[3] * other.group1()[2]) + (self.group9()[2] * other.group1()[1]) + (self.group9()[1] * other.group1()[0]) + (self.group9()[0] * other.group1()[3])
                    - (self.group3()[3] * other.group0()[3])
                    - (self.group3()[2] * other.group0()[2])
                    - (self.group3()[0] * other.group0()[0])
                    - (self.group3()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group7()[2] * other.group0()[1]) - (self.group7()[1] * other.group0()[2]) + (self.group0()[1] * other.group1()[0]) + (self.group6()[0] * other.group0()[3])),
                (-(self.group7()[2] * other.group0()[0])
                    + (self.group7()[0] * other.group0()[2])
                    + (self.group0()[1] * other.group1()[1])
                    + (self.group6()[1] * other.group0()[3])),
                ((self.group7()[1] * other.group0()[0]) - (self.group7()[0] * other.group0()[1]) + (self.group0()[1] * other.group1()[2]) + (self.group6()[2] * other.group0()[3])),
                0.0,
            ]),
            // e5
            (-(self.group6()[2] * other.group0()[2]) - (self.group6()[1] * other.group0()[1]) + (self.group0()[1] * other.group1()[3]) - (self.group6()[0] * other.group0()[0])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from([
                (-(self.group9()[2] * other.group0()[2]) + (self.group9()[3] * other.group0()[1])),
                ((self.group9()[1] * other.group0()[2]) - (self.group9()[3] * other.group0()[0])),
                (-(self.group9()[1] * other.group0()[1]) + (self.group9()[2] * other.group0()[0])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                - (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd3        0        2        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        6       17        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group9()[3] * other.group0()[3]) + (self.group9()[2] * other.group0()[2]) - (self.group3()[3] * other.group0()[0]) + (self.group9()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[1]) + (self.group6()[0] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[2]) + (self.group6()[1] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group6()[2] * other.group0()[0])),
                0.0,
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]) * Simd32x3::from(-1.0)),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[0])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiLine> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       18        0
    //    simd3        0        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       13       24        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group7()[2] * other.group1()[2])
                    - (self.group7()[1] * other.group1()[1])
                    - (self.group7()[0] * other.group1()[0])
                    - (self.group6()[2] * other.group0()[2])
                    - (self.group6()[0] * other.group0()[0])
                    - (self.group6()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group9()[3] * other.group0()[1]) + (self.group9()[0] * other.group1()[0]) + (self.group9()[2] * other.group0()[2])),
                ((self.group9()[3] * other.group0()[0]) + (self.group9()[0] * other.group1()[1]) - (self.group9()[1] * other.group0()[2])),
                (-(self.group9()[2] * other.group0()[0]) + (self.group9()[0] * other.group1()[2]) + (self.group9()[1] * other.group0()[1])),
                0.0,
            ]),
            // e5
            (-(self.group9()[3] * other.group1()[2]) - (self.group9()[1] * other.group1()[0]) - (self.group9()[2] * other.group1()[1])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        9        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group6()[2] * other.group0()[2]) - (self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group9()[2] * other.group0()[2]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[3] * other.group0()[0])),
                ((self.group9()[1] * other.group0()[1]) - (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiMotor> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       19       26        0
    //    simd3        2        5        0
    // Totals...
    // yes simd       21       31        0
    //  no simd       25       41        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group7()[2] * other.group1()[2])
                    - (self.group7()[1] * other.group1()[1])
                    - (self.group7()[0] * other.group1()[0])
                    - (self.group6()[2] * other.group0()[2])
                    - (self.group6()[1] * other.group0()[1])
                    - (self.group6()[0] * other.group0()[0])
                    + (self.group0()[1] * other.group0()[3])
                    + (self.group1()[3] * other.group1()[3])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group9()[3] * other.group0()[1])
                    + (self.group9()[2] * other.group0()[2])
                    + (self.group3()[0] * other.group1()[3])
                    + (self.group9()[0] * other.group1()[0])),
                ((self.group9()[3] * other.group0()[0]) - (self.group9()[1] * other.group0()[2]) + (self.group3()[1] * other.group1()[3]) + (self.group9()[0] * other.group1()[1])),
                (-(self.group9()[2] * other.group0()[0])
                    + (self.group9()[1] * other.group0()[1])
                    + (self.group3()[2] * other.group1()[3])
                    + (self.group9()[0] * other.group1()[2])),
                0.0,
            ]),
            // e5
            (-(self.group9()[3] * other.group1()[2]) - (self.group9()[2] * other.group1()[1]) + (self.group3()[3] * other.group1()[3]) - (self.group9()[1] * other.group1()[0])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])) + (self.group7() * Simd32x3::from(other.group1()[3]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group9()[0] * other.group1()[3])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            (self.group0()[1] * other.group1()[3]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       10        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        6       11        0
    //  no simd        6       13        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group6()[2] * other.group0()[2]) - (self.group6()[1] * other.group0()[1]) + (self.group0()[1] * other.group0()[3])
                    - (self.group6()[0] * other.group0()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group9()[2] * other.group0()[2]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[3] * other.group0()[0])),
                ((self.group9()[1] * other.group0()[1]) - (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiPlane> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group9()[3] * other.group0()[2]) + (self.group9()[2] * other.group0()[1]) + (self.group9()[0] * other.group0()[3]) + (self.group9()[1] * other.group0()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                0.0,
            ]),
            // e5
            (self.group0()[1] * other.group0()[3]),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group9()[3] * other.group0()[2]) + (self.group9()[1] * other.group0()[0]) + (self.group9()[2] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                0.0,
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiScalar> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd2        0        1        0
    //    simd3        0        4        0
    //    simd4        0        4        0
    // Totals...
    // yes simd        0       11        0
    //  no simd        0       32        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            (self.group0() * Simd32x2::from(other[e12345])),
            // e1, e2, e3, e4
            (self.group1() * Simd32x4::from(other[e12345])),
            // e5
            (self[e1] * other[e12345]),
            // e41, e42, e43, e45
            (self.group3() * Simd32x4::from(other[e12345])),
            // e15, e25, e35
            (self.group4() * Simd32x3::from(other[e12345])),
            // e23, e31, e12
            (self.group5() * Simd32x3::from(other[e12345])),
            // e415, e425, e435, e321
            (self.group6() * Simd32x4::from(other[e12345])),
            // e423, e431, e412
            (self.group7() * Simd32x3::from(other[e12345])),
            // e235, e315, e125
            (self.group8() * Simd32x3::from(other[e12345])),
            // e1234, e4235, e4315, e4125
            (self.group9() * Simd32x4::from(other[e12345])),
            // e3215
            (self[e45] * other[e12345]),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        4        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        5        0
    //  no simd        3        8        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e45] * other.group0()[3]) + (self.group9()[3] * other.group0()[2]) + (self.group9()[1] * other.group0()[0]) + (self.group9()[2] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       25        0
    //    simd3        1        4        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       17       31        0
    //  no simd       25       45        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group6()[2] * other.group1()[2])
                    - (self.group6()[1] * other.group1()[1])
                    - (self.group6()[0] * other.group1()[0])
                    + (self.group0()[1] * other.group0()[3])
                    + (self[e1] * other.group1()[3])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group1(), 1, 3, 0, 3) * Simd32x4::from([self.group9()[3], self.group4()[1], self.group9()[2], self.group3()[3]]))
                + (swizzle!(self.group9(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self[e45] * other.group0()[0]) - (self.group4()[0] * other.group1()[3])),
                    (-(self[e45] * other.group0()[1]) - (self.group9()[1] * other.group1()[2])),
                    (-(self[e45] * other.group0()[2]) - (self.group4()[2] * other.group1()[3])),
                    ((self.group9()[2] * other.group0()[1]) + (self.group9()[1] * other.group0()[0])),
                ])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[0]) + (self.group6()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group0()[1]) + (self.group6()[1] * other.group1()[3])),
                ((self.group0()[1] * other.group0()[2]) + (self.group6()[2] * other.group1()[3])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])) + (self.group8() * Simd32x3::from(other.group1()[3]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self[e45] * other.group1()[3] * -1.0)]),
            // e423, e431, e412
            (Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group0()[1] * other.group1()[3]), 0.0, 0.0, 0.0]),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       23        0
    //    simd3        1        3        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       17       28        0
    //  no simd       25       40        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e45] * other.group1()[0]) + (self.group9()[3] * other.group1()[3]) + (self.group9()[2] * other.group1()[2]) + (self.group9()[1] * other.group1()[1])
                    - (self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    - (self.group3()[3] * other.group0()[3])
                    - (self.group4()[0] * other.group0()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group8()[2], self.group8()[0], self.group8()[1], self.group6()[2]]))
                + (Simd32x4::from(self.group0()[1]) * swizzle!(other.group1(), 1, 2, 3, 0))
                + Simd32x4::from([
                    ((self.group8()[1] * other.group0()[2]) + (self.group6()[0] * other.group0()[3])),
                    ((self.group8()[2] * other.group0()[0]) + (self.group6()[1] * other.group0()[3])),
                    ((self.group8()[0] * other.group0()[1]) + (self.group6()[2] * other.group0()[3])),
                    (-(self.group6()[1] * other.group0()[1]) - (self.group6()[0] * other.group0()[0])),
                ])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group9()[2] * other.group0()[2]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[3] * other.group0()[0])),
                ((self.group9()[1] * other.group0()[1]) - (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (-(Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                + (Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Circle> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       38       48        0
    //    simd3        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       43       58        0
    //  no simd       54       80        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group5()[2] * other.group1()[2])
                    - (self.group5()[1] * other.group1()[1])
                    - (self.group5()[0] * other.group1()[0])
                    - (self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    - (self.group4()[0] * other.group0()[0])
                    - (self.group3()[3] * other.group1()[3])
                    - (self.group3()[2] * other.group2()[2])
                    - (self.group3()[0] * other.group2()[0])
                    - (self.group3()[1] * other.group2()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group8()[2] * other.group0()[1]) + (self.group8()[1] * other.group0()[2]) + (self.group7()[2] * other.group2()[1])
                    - (self.group7()[1] * other.group2()[2])
                    + (self.group6()[0] * other.group1()[3])
                    + (self.group6()[3] * other.group1()[0])),
                ((self.group8()[2] * other.group0()[0]) - (self.group8()[0] * other.group0()[2]) - (self.group7()[2] * other.group2()[0])
                    + (self.group7()[0] * other.group2()[2])
                    + (self.group6()[1] * other.group1()[3])
                    + (self.group6()[3] * other.group1()[1])),
                (-(self.group8()[1] * other.group0()[0]) + (self.group8()[0] * other.group0()[1]) + (self.group7()[1] * other.group2()[0])
                    - (self.group7()[0] * other.group2()[1])
                    + (self.group6()[2] * other.group1()[3])
                    + (self.group6()[3] * other.group1()[2])),
                (-(self.group7()[2] * other.group1()[2])
                    - (self.group7()[1] * other.group1()[1])
                    - (self.group7()[0] * other.group1()[0])
                    - (self.group6()[2] * other.group0()[2])
                    - (self.group6()[0] * other.group0()[0])
                    - (self.group6()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group8()[2] * other.group1()[2])
                - (self.group8()[1] * other.group1()[1])
                - (self.group8()[0] * other.group1()[0])
                - (self.group6()[2] * other.group2()[2])
                - (self.group6()[0] * other.group2()[0])
                - (self.group6()[1] * other.group2()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group9(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group9()[0] * other.group1()[0]) + (self.group9()[2] * other.group0()[2])),
                    ((self.group9()[3] * other.group0()[0]) + (self.group9()[0] * other.group1()[1])),
                    ((self.group9()[0] * other.group1()[2]) + (self.group9()[1] * other.group0()[1])),
                    (-(self.group9()[1] * other.group1()[0]) - (self.group9()[2] * other.group1()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                - (swizzle!(other.group2(), 2, 0, 1) * Simd32x3::from([self.group9()[2], self.group9()[3], self.group9()[1]]))
                + (swizzle!(other.group2(), 1, 2, 0) * Simd32x3::from([self.group9()[3], self.group9()[1], self.group9()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(self[e45]) * other.group0()) + (Simd32x3::from(self.group9()[0]) * other.group2())
                - (Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group2()),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       34       47        0
    //    simd3        3        7        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       38       55        0
    //  no simd       47       72        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group5()[2] * other.group1()[2])
                    - (self.group5()[1] * other.group1()[1])
                    - (self.group5()[0] * other.group1()[0])
                    - (self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    - (self.group4()[0] * other.group0()[0])
                    - (self.group3()[2] * other.group2()[2])
                    - (self.group3()[0] * other.group2()[0])
                    - (self.group3()[1] * other.group2()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group8()[2] * other.group0()[1])
                    + (self.group8()[1] * other.group0()[2])
                    + (self.group7()[2] * other.group2()[1])
                    + (self.group6()[3] * other.group1()[0])
                    - (self.group7()[1] * other.group2()[2])),
                ((self.group8()[2] * other.group0()[0]) - (self.group8()[0] * other.group0()[2]) - (self.group7()[2] * other.group2()[0])
                    + (self.group6()[3] * other.group1()[1])
                    + (self.group7()[0] * other.group2()[2])),
                (-(self.group8()[1] * other.group0()[0])
                    + (self.group8()[0] * other.group0()[1])
                    + (self.group7()[1] * other.group2()[0])
                    + (self.group6()[3] * other.group1()[2])
                    - (self.group7()[0] * other.group2()[1])),
                (-(self.group7()[2] * other.group1()[2])
                    - (self.group7()[1] * other.group1()[1])
                    - (self.group7()[0] * other.group1()[0])
                    - (self.group6()[2] * other.group0()[2])
                    - (self.group6()[0] * other.group0()[0])
                    - (self.group6()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group8()[2] * other.group1()[2])
                - (self.group8()[1] * other.group1()[1])
                - (self.group8()[0] * other.group1()[0])
                - (self.group6()[2] * other.group2()[2])
                - (self.group6()[0] * other.group2()[0])
                - (self.group6()[1] * other.group2()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group9(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group9()[0] * other.group1()[0]) + (self.group9()[2] * other.group0()[2])),
                    ((self.group9()[3] * other.group0()[0]) + (self.group9()[0] * other.group1()[1])),
                    ((self.group9()[0] * other.group1()[2]) + (self.group9()[1] * other.group0()[1])),
                    (-(self.group9()[1] * other.group1()[0]) - (self.group9()[2] * other.group1()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self[e45]) * other.group1()) - (swizzle!(other.group2(), 2, 0, 1) * Simd32x3::from([self.group9()[2], self.group9()[3], self.group9()[1]]))
                + (swizzle!(other.group2(), 1, 2, 0) * Simd32x3::from([self.group9()[3], self.group9()[1], self.group9()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group9()[0]) * other.group2()) + (Simd32x3::from(self[e45]) * other.group0())),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[1] * other.group1()[1]),
                (self.group0()[1] * other.group1()[2]),
                0.0,
            ]),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group2()),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<CircleAtInfinity> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       24       34        0
    //    simd3        3        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd       27       41        0
    //  no simd       33       56        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group5()[2] * other.group0()[2])
                    - (self.group5()[1] * other.group0()[1])
                    - (self.group5()[0] * other.group0()[0])
                    - (self.group3()[3] * other.group0()[3])
                    - (self.group3()[2] * other.group1()[2])
                    - (self.group3()[0] * other.group1()[0])
                    - (self.group3()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group7()[2] * other.group1()[1]) - (self.group7()[1] * other.group1()[2]) + (self.group6()[0] * other.group0()[3]) + (self.group6()[3] * other.group0()[0])),
                (-(self.group7()[2] * other.group1()[0])
                    + (self.group7()[0] * other.group1()[2])
                    + (self.group6()[1] * other.group0()[3])
                    + (self.group6()[3] * other.group0()[1])),
                ((self.group7()[1] * other.group1()[0]) - (self.group7()[0] * other.group1()[1]) + (self.group6()[2] * other.group0()[3]) + (self.group6()[3] * other.group0()[2])),
                (-(self.group7()[2] * other.group0()[2]) - (self.group7()[0] * other.group0()[0]) - (self.group7()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group8()[2] * other.group0()[2])
                - (self.group8()[1] * other.group0()[1])
                - (self.group8()[0] * other.group0()[0])
                - (self.group6()[2] * other.group1()[2])
                - (self.group6()[0] * other.group1()[0])
                - (self.group6()[1] * other.group1()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group9()[0] * other.group0()[0]),
                (self.group9()[0] * other.group0()[1]),
                (self.group9()[0] * other.group0()[2]),
                (-(self.group9()[3] * other.group0()[2]) - (self.group9()[1] * other.group0()[0]) - (self.group9()[2] * other.group0()[1])),
            ]),
            // e15, e25, e35
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                - (swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group9()[2], self.group9()[3], self.group9()[1]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group9()[3], self.group9()[1], self.group9()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group9()[0]) * other.group1()) - (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<CircleAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       21       30        0
    //    simd3        2        6        0
    // Totals...
    // yes simd       23       36        0
    //  no simd       27       48        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    - (self.group4()[0] * other.group0()[0])
                    - (self.group3()[2] * other.group1()[2])
                    - (self.group3()[0] * other.group1()[0])
                    - (self.group3()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group8()[2] * other.group0()[1]) + (self.group8()[1] * other.group0()[2]) - (self.group7()[1] * other.group1()[2])
                    + (self.group7()[2] * other.group1()[1])),
                ((self.group8()[2] * other.group0()[0]) - (self.group8()[0] * other.group0()[2]) + (self.group7()[0] * other.group1()[2]) - (self.group7()[2] * other.group1()[0])),
                (-(self.group8()[1] * other.group0()[0]) + (self.group8()[0] * other.group0()[1]) - (self.group7()[0] * other.group1()[1])
                    + (self.group7()[1] * other.group1()[0])),
                (-(self.group6()[2] * other.group0()[2]) - (self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group6()[2] * other.group1()[2]) - (self.group6()[0] * other.group1()[0]) - (self.group6()[1] * other.group1()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group9()[2] * other.group0()[2]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[3] * other.group0()[0])),
                ((self.group9()[1] * other.group0()[1]) - (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group9()[2], self.group9()[3], self.group9()[1]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group9()[3], self.group9()[1], self.group9()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group9()[0]) * other.group1()) + (Simd32x3::from(self[e45]) * other.group0())),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<CircleOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       22       35        0
    //    simd3        0        3        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       23       39        0
    //  no simd       26       48        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group5()[2] * other.group1()[2])
                    - (self.group5()[1] * other.group1()[1])
                    - (self.group5()[0] * other.group1()[0])
                    - (self.group4()[2] * other.group0()[2])
                    - (self.group4()[0] * other.group0()[0])
                    - (self.group4()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group8()[2] * other.group0()[1]) + (self.group6()[3] * other.group1()[0]) + (self.group8()[1] * other.group0()[2])),
                ((self.group8()[2] * other.group0()[0]) + (self.group6()[3] * other.group1()[1]) - (self.group8()[0] * other.group0()[2])),
                (-(self.group8()[1] * other.group0()[0]) + (self.group6()[3] * other.group1()[2]) + (self.group8()[0] * other.group0()[1])),
                (-(self.group7()[2] * other.group1()[2])
                    - (self.group7()[1] * other.group1()[1])
                    - (self.group7()[0] * other.group1()[0])
                    - (self.group6()[2] * other.group0()[2])
                    - (self.group6()[0] * other.group0()[0])
                    - (self.group6()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group8()[2] * other.group1()[2]) - (self.group8()[0] * other.group1()[0]) - (self.group8()[1] * other.group1()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group9(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group9()[0] * other.group1()[0]) + (self.group9()[2] * other.group0()[2])),
                    ((self.group9()[3] * other.group0()[0]) + (self.group9()[0] * other.group1()[1])),
                    ((self.group9()[0] * other.group1()[2]) + (self.group9()[1] * other.group0()[1])),
                    (-(self.group9()[1] * other.group1()[0]) - (self.group9()[2] * other.group1()[1])),
                ])),
            // e15, e25, e35
            (Simd32x3::from(self[e45]) * other.group1()),
            // e23, e31, e12
            (Simd32x3::from(self[e45]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[1] * other.group1()[1]),
                (self.group0()[1] * other.group1()[2]),
                0.0,
            ]),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       21       31        0
    //    simd3        3        7        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       25       39        0
    //  no simd       34       56        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    - (self.group4()[0] * other.group0()[0])
                    - (self.group3()[3] * other.group0()[3])
                    - (self.group3()[2] * other.group1()[2])
                    - (self.group3()[0] * other.group1()[0])
                    - (self.group3()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group8()[2], self.group8()[0], self.group8()[1], self.group6()[2]]))
                + Simd32x4::from([
                    ((self.group8()[1] * other.group0()[2]) + (self.group7()[2] * other.group1()[1]) + (self.group6()[0] * other.group0()[3])
                        - (self.group7()[1] * other.group1()[2])),
                    ((self.group8()[2] * other.group0()[0]) - (self.group7()[2] * other.group1()[0])
                        + (self.group6()[1] * other.group0()[3])
                        + (self.group7()[0] * other.group1()[2])),
                    ((self.group8()[0] * other.group0()[1]) + (self.group7()[1] * other.group1()[0]) + (self.group6()[2] * other.group0()[3])
                        - (self.group7()[0] * other.group1()[1])),
                    (-(self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group6()[2] * other.group1()[2]) - (self.group6()[0] * other.group1()[0]) - (self.group6()[1] * other.group1()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group9()[2] * other.group0()[2]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[3] * other.group0()[0])),
                ((self.group9()[1] * other.group0()[1]) - (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group9()[2], self.group9()[3], self.group9()[1]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group9()[3], self.group9()[1], self.group9()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])) + (Simd32x3::from(self.group9()[0]) * other.group1())
                - (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Dipole> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       22        0
    //    simd3        0        2        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       18       27        0
    //  no simd       24       40        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group7()[2] * other.group2()[2])
                    - (self.group7()[1] * other.group2()[1])
                    - (self.group7()[0] * other.group2()[0])
                    - (self.group6()[3] * other.group1()[3])
                    - (self.group6()[2] * other.group1()[2])
                    - (self.group6()[0] * other.group1()[0])
                    - (self.group6()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 0, 3, 0, 3) * Simd32x4::from([other.group2()[0], other.group1()[0], other.group2()[2], other.group0()[2]]))
                + (swizzle!(self.group9(), 2, 0, 1, 2) * Simd32x4::from([other.group1()[2], other.group2()[1], other.group1()[1], other.group0()[1]]))
                + Simd32x4::from([
                    (-(self[e45] * other.group0()[0]) - (self.group9()[3] * other.group1()[1])),
                    (-(self[e45] * other.group0()[1]) - (self.group9()[1] * other.group1()[2])),
                    (-(self[e45] * other.group0()[2]) - (self.group9()[2] * other.group1()[0])),
                    ((self.group9()[0] * other.group1()[3]) + (self.group9()[1] * other.group0()[0])),
                ])),
            // e5
            (-(self[e45] * other.group1()[3]) - (self.group9()[3] * other.group2()[2]) - (self.group9()[1] * other.group2()[0]) - (self.group9()[2] * other.group2()[1])),
            // e41, e42, e43, e45
            (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]])),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group2()),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd3        0        1        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       12       23        0
    //  no simd       15       31        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group7()[2] * other.group1()[2])
                    - (self.group7()[1] * other.group1()[1])
                    - (self.group6()[3] * other.group0()[3])
                    - (self.group7()[0] * other.group1()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 0, 0, 0, 3) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[2]]))
                + Simd32x4::from([
                    ((self[e45] * other.group0()[0]) * -1.0),
                    ((self[e45] * other.group0()[1]) * -1.0),
                    ((self[e45] * other.group0()[2]) * -1.0),
                    ((self.group9()[2] * other.group0()[1]) + (self.group9()[0] * other.group0()[3]) + (self.group9()[1] * other.group0()[0])),
                ])),
            // e5
            (-(self[e45] * other.group0()[3]) - (self.group9()[3] * other.group1()[2]) - (self.group9()[1] * other.group1()[0]) - (self.group9()[2] * other.group1()[1])),
            // e41, e42, e43, e45
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       22        0
    //    simd3        0        2        0
    // Totals...
    // yes simd       15       24        0
    //  no simd       15       28        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group7()[2] * other.group1()[2])
                    - (self.group7()[1] * other.group1()[1])
                    - (self.group7()[0] * other.group1()[0])
                    - (self.group6()[3] * other.group0()[3])
                    - (self.group6()[2] * other.group0()[2])
                    - (self.group6()[0] * other.group0()[0])
                    - (self.group6()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group9()[3] * other.group0()[1]) + (self.group9()[0] * other.group1()[0]) + (self.group9()[2] * other.group0()[2])),
                ((self.group9()[3] * other.group0()[0]) + (self.group9()[0] * other.group1()[1]) - (self.group9()[1] * other.group0()[2])),
                (-(self.group9()[2] * other.group0()[0]) + (self.group9()[0] * other.group1()[2]) + (self.group9()[1] * other.group0()[1])),
                (self.group9()[0] * other.group0()[3]),
            ]),
            // e5
            (-(self[e45] * other.group0()[3]) - (self.group9()[3] * other.group1()[2]) - (self.group9()[1] * other.group1()[0]) - (self.group9()[2] * other.group1()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       20        0
    //    simd3        0        1        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       12       27        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group7()[2] * other.group1()[2])
                    - (self.group7()[0] * other.group1()[0])
                    - (self.group7()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 0, 0, 0, 3) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[2]]))
                + Simd32x4::from([
                    ((self[e45] * other.group0()[0]) * -1.0),
                    ((self[e45] * other.group0()[1]) * -1.0),
                    ((self[e45] * other.group0()[2]) * -1.0),
                    ((self.group9()[1] * other.group0()[0]) + (self.group9()[2] * other.group0()[1])),
                ])),
            // e5
            (-(self.group9()[3] * other.group1()[2]) - (self.group9()[1] * other.group1()[0]) - (self.group9()[2] * other.group1()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        6       17        0
    //  no simd        6       20        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group6()[3] * other.group0()[3])
                    - (self.group8()[0] * other.group0()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self[e45] * other.group0()[0] * -1.0),
                (self[e45] * other.group0()[1] * -1.0),
                (self[e45] * other.group0()[2] * -1.0),
                ((self.group9()[3] * other.group0()[2]) + (self.group9()[2] * other.group0()[1]) + (self.group9()[0] * other.group0()[3]) + (self.group9()[1] * other.group0()[0])),
            ]),
            // e5
            (self[e45] * other.group0()[3] * -1.0),
            // e41, e42, e43, e45
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       22        0
    //    simd3        0        2        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       15       26        0
    //  no simd       21       36        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group7()[2] * other.group2()[2])
                    - (self.group7()[1] * other.group2()[1])
                    - (self.group7()[0] * other.group2()[0])
                    - (self.group6()[2] * other.group1()[2])
                    - (self.group6()[0] * other.group1()[0])
                    - (self.group6()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 0, 3, 0, 3) * Simd32x4::from([other.group2()[0], other.group1()[0], other.group2()[2], other.group0()[2]]))
                + (swizzle!(self.group9(), 2, 0, 1, 1) * Simd32x4::from([other.group1()[2], other.group2()[1], other.group1()[1], other.group0()[0]]))
                + Simd32x4::from([
                    (-(self[e45] * other.group0()[0]) - (self.group9()[3] * other.group1()[1])),
                    (-(self[e45] * other.group0()[1]) - (self.group9()[1] * other.group1()[2])),
                    (-(self[e45] * other.group0()[2]) - (self.group9()[2] * other.group1()[0])),
                    (self.group9()[2] * other.group0()[1]),
                ])),
            // e5
            (-(self.group9()[3] * other.group2()[2]) - (self.group9()[1] * other.group2()[0]) - (self.group9()[2] * other.group2()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group2()),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<DualNum> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        0        4        0
    //    simd4        0        4        0
    // Totals...
    // yes simd        2       14        0
    //  no simd        2       34        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[0] * other.group0()[1]) + (self.group9()[0] * other.group0()[0])),
                (self.group0()[1] * other.group0()[1]),
            ]),
            // e1, e2, e3, e4
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e5
            ((self.group0()[1] * other.group0()[0]) + (self[e1] * other.group0()[1])),
            // e41, e42, e43, e45
            (self.group3() * Simd32x4::from(other.group0()[1])),
            // e15, e25, e35
            (self.group4() * Simd32x3::from(other.group0()[1])),
            // e23, e31, e12
            (self.group5() * Simd32x3::from(other.group0()[1])),
            // e415, e425, e435, e321
            (self.group6() * Simd32x4::from(other.group0()[1])),
            // e423, e431, e412
            (self.group7() * Simd32x3::from(other.group0()[1])),
            // e235, e315, e125
            (self.group8() * Simd32x3::from(other.group0()[1])),
            // e1234, e4235, e4315, e4125
            (self.group9() * Simd32x4::from(other.group0()[1])),
            // e3215
            (self[e45] * other.group0()[1]),
        );
    }
}
impl AntiWedge<FlatOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        6        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self.group6()[3] * other[e45] * -1.0), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([0.0, 0.0, 0.0, (self.group9()[0] * other[e45])]),
            // e5
            (self[e45] * other[e45] * -1.0),
            // e41, e42, e43, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other[e45])]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<FlatPoint> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6        9        0
    //    simd3        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        6       11        0
    //  no simd        6       16        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group7()[2] * other.group0()[2])
                    - (self.group7()[1] * other.group0()[1])
                    - (self.group6()[3] * other.group0()[3])
                    - (self.group7()[0] * other.group0()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (Simd32x4::from(self.group9()[0]) * other.group0()),
            // e5
            (-(self[e45] * other.group0()[3]) - (self.group9()[3] * other.group0()[2]) - (self.group9()[1] * other.group0()[0]) - (self.group9()[2] * other.group0()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        9        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        4       10        0
    //  no simd        4       12        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group7()[2] * other.group0()[2]) - (self.group7()[0] * other.group0()[0]) - (self.group7()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group9()[0] * other.group0()[0]),
                (self.group9()[0] * other.group0()[1]),
                (self.group9()[0] * other.group0()[2]),
                0.0,
            ]),
            // e5
            (-(self.group9()[3] * other.group0()[2]) - (self.group9()[1] * other.group0()[0]) - (self.group9()[2] * other.group0()[1])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Flector> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       23       41        0
    //    simd3        5        9        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       31       53        0
    //  no simd       50       80        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group7()[2] * other.group0()[2])
                    - (self.group7()[1] * other.group0()[1])
                    - (self.group7()[0] * other.group0()[0])
                    - (self.group6()[3] * other.group0()[3])
                    + (self.group1()[3] * other.group1()[3])
                    + (self.group1()[2] * other.group1()[2])
                    + (self.group1()[0] * other.group1()[0])
                    + (self.group1()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group9()[0]) * other.group0())
                - (swizzle!(other.group1(), 1, 2, 0, 2) * Simd32x4::from([self.group5()[2], self.group5()[0], self.group5()[1], self.group3()[2]]))
                + Simd32x4::from([
                    ((self.group3()[0] * other.group1()[3]) + (self.group5()[1] * other.group1()[2])),
                    ((self.group5()[2] * other.group1()[0]) + (self.group3()[1] * other.group1()[3])),
                    ((self.group3()[2] * other.group1()[3]) + (self.group5()[0] * other.group1()[1])),
                    (-(self.group3()[0] * other.group1()[0]) - (self.group3()[1] * other.group1()[1])),
                ])),
            // e5
            (-(self[e45] * other.group0()[3]) - (self.group9()[3] * other.group0()[2]) - (self.group9()[2] * other.group0()[1]) - (self.group9()[1] * other.group0()[0])
                + (self.group4()[2] * other.group1()[2])
                + (self.group4()[1] * other.group1()[1])
                + (self.group3()[3] * other.group1()[3])
                + (self.group4()[0] * other.group1()[0])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group1(), 2, 0, 1, 2) * Simd32x4::from([self.group7()[1], self.group7()[2], self.group7()[0], self.group6()[2]]))
                + Simd32x4::from([
                    (self.group7()[2] * other.group1()[1]),
                    (self.group7()[0] * other.group1()[2]),
                    (self.group7()[1] * other.group1()[0]),
                    (-(self.group6()[1] * other.group1()[1]) + (self.group0()[1] * other.group0()[3]) - (self.group6()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group8(), 2, 0, 1) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[0]]))
                + (swizzle!(self.group8(), 1, 2, 0) * Simd32x3::from([other.group1()[2], other.group1()[0], other.group1()[1]]))
                + (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                + (Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))),
            // e23, e31, e12
            (-(Simd32x3::from(self.group6()[3]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])) + (self.group7() * Simd32x3::from(other.group1()[3]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group9()[2] * other.group1()[2]) + (self.group9()[3] * other.group1()[1])),
                ((self.group9()[1] * other.group1()[2]) - (self.group9()[3] * other.group1()[0])),
                (-(self.group9()[1] * other.group1()[1]) + (self.group9()[2] * other.group1()[0])),
                (self.group9()[0] * other.group1()[3]),
            ]),
            // e423, e431, e412
            (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
            // e235, e315, e125
            ((Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                - (Simd32x3::from(self[e45]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[1] * other.group1()[1]),
                (self.group0()[1] * other.group1()[2]),
            ]),
            // e3215
            (self.group0()[1] * other.group1()[3]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       16        0
    //    simd3        1        4        0
    // Totals...
    // yes simd       10       20        0
    //  no simd       12       28        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group7()[2] * other.group0()[2]) - (self.group7()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3])
                    - (self.group7()[0] * other.group0()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group3()[0] * other.group0()[3]) + (self.group9()[0] * other.group0()[0])),
                ((self.group3()[1] * other.group0()[3]) + (self.group9()[0] * other.group0()[1])),
                ((self.group3()[2] * other.group0()[3]) + (self.group9()[0] * other.group0()[2])),
                0.0,
            ]),
            // e5
            (-(self.group9()[3] * other.group0()[2]) - (self.group9()[2] * other.group0()[1]) + (self.group3()[3] * other.group0()[3]) - (self.group9()[1] * other.group0()[0])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                + (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))),
            // e23, e31, e12
            (self.group7() * Simd32x3::from(other.group0()[3])),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group9()[0] * other.group0()[3])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            (self.group0()[1] * other.group0()[3]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       21        0
    //    simd3        1        7        0
    //    simd4        4        4        0
    // Totals...
    // yes simd       16       32        0
    //  no simd       30       58        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group6()[3] * other.group0()[0])
                    + (self.group1()[2] * other.group0()[3])
                    + (self.group1()[0] * other.group0()[1])
                    + (self.group1()[1] * other.group0()[2])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 3, 1, 2, 0) * Simd32x4::from([self.group5()[1], self.group5()[2], self.group5()[0], self.group9()[0]]))
                - (swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group5()[2], self.group5()[0], self.group5()[1], self.group3()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group3()[0] * other.group0()[1]) - (self.group3()[1] * other.group0()[2]))])),
            // e5
            (-(self[e45] * other.group0()[0]) + (self.group4()[2] * other.group0()[3]) + (self.group4()[0] * other.group0()[1]) + (self.group4()[1] * other.group0()[2])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group0(), 3, 1, 2, 3) * Simd32x4::from([self.group7()[1], self.group7()[2], self.group7()[0], self.group6()[2]]))
                + (swizzle!(other.group0(), 2, 3, 1, 0) * Simd32x4::from([self.group7()[2], self.group7()[0], self.group7()[1], self.group0()[1]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group6()[1] * other.group0()[2]) - (self.group6()[0] * other.group0()[1]))])),
            // e15, e25, e35
            ((swizzle!(self.group8(), 1, 2, 0) * Simd32x3::from([other.group0()[3], other.group0()[1], other.group0()[2]]))
                - (swizzle!(self.group8(), 2, 0, 1) * Simd32x3::from([other.group0()[2], other.group0()[3], other.group0()[1]]))),
            // e23, e31, e12
            (Simd32x3::from(self.group6()[3]) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[3]]) * Simd32x3::from(-1.0)),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group9()[2] * other.group0()[3]) + (self.group9()[3] * other.group0()[2])),
                ((self.group9()[1] * other.group0()[3]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[2] * other.group0()[1])),
                0.0,
            ]),
            // e423, e431, e412
            (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[3]])),
            // e235, e315, e125
            (Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[3]]) * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                (self.group0()[1] * other.group0()[3]),
            ]),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Horizon> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        7        0
    //    simd3        0        3        0
    // Totals...
    // yes simd        0       10        0
    //  no simd        0       16        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self.group1()[3] * other[e3215]), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([(self.group3()[0] * other[e3215]), (self.group3()[1] * other[e3215]), (self.group3()[2] * other[e3215]), 0.0]),
            // e5
            (self.group3()[3] * other[e3215]),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            (Simd32x3::from(other[e3215]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]])),
            // e23, e31, e12
            (self.group7() * Simd32x3::from(other[e3215])),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group9()[0] * other[e3215])]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(other[e3215]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            (self.group0()[1] * other[e3215]),
        );
    }
}
impl AntiWedge<Infinity> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self.group9()[0] * other[e5]), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from(0.0),
            // e5
            (self.group0()[1] * other[e5]),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Line> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       20       33        0
    //    simd3        2        5        0
    // Totals...
    // yes simd       22       38        0
    //  no simd       26       48        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group5()[2] * other.group0()[2])
                    - (self.group5()[1] * other.group0()[1])
                    - (self.group5()[0] * other.group0()[0])
                    - (self.group3()[2] * other.group1()[2])
                    - (self.group3()[0] * other.group1()[0])
                    - (self.group3()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group7()[2] * other.group1()[1]) + (self.group6()[3] * other.group0()[0]) - (self.group7()[1] * other.group1()[2])),
                (-(self.group7()[2] * other.group1()[0]) + (self.group6()[3] * other.group0()[1]) + (self.group7()[0] * other.group1()[2])),
                ((self.group7()[1] * other.group1()[0]) + (self.group6()[3] * other.group0()[2]) - (self.group7()[0] * other.group1()[1])),
                (-(self.group7()[2] * other.group0()[2]) - (self.group7()[0] * other.group0()[0]) - (self.group7()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group8()[2] * other.group0()[2])
                - (self.group8()[1] * other.group0()[1])
                - (self.group8()[0] * other.group0()[0])
                - (self.group6()[2] * other.group1()[2])
                - (self.group6()[0] * other.group1()[0])
                - (self.group6()[1] * other.group1()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group9()[0] * other.group0()[0]),
                (self.group9()[0] * other.group0()[1]),
                (self.group9()[0] * other.group0()[2]),
                (-(self.group9()[3] * other.group0()[2]) - (self.group9()[1] * other.group0()[0]) - (self.group9()[2] * other.group0()[1])),
            ]),
            // e15, e25, e35
            ((Simd32x3::from(self[e45]) * other.group0()) - (swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group9()[2], self.group9()[3], self.group9()[1]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group9()[3], self.group9()[1], self.group9()[2]]))),
            // e23, e31, e12
            (Simd32x3::from(self.group9()[0]) * other.group1()),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                0.0,
            ]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<LineAtInfinity> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd3        1        4        0
    // Totals...
    // yes simd        8       16        0
    //  no simd       10       24        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group3()[2] * other.group0()[2]) - (self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group7()[1] * other.group0()[2]) + (self.group7()[2] * other.group0()[1])),
                ((self.group7()[0] * other.group0()[2]) - (self.group7()[2] * other.group0()[0])),
                (-(self.group7()[0] * other.group0()[1]) + (self.group7()[1] * other.group0()[0])),
                0.0,
            ]),
            // e5
            (-(self.group6()[2] * other.group0()[2]) - (self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group9()[2], self.group9()[3], self.group9()[1]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group9()[3], self.group9()[1], self.group9()[2]]))),
            // e23, e31, e12
            (Simd32x3::from(self.group9()[0]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<LineOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       21        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        8       22        0
    //  no simd        8       24        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group5()[2] * other.group0()[2]) - (self.group5()[0] * other.group0()[0]) - (self.group5()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group6()[3] * other.group0()[0]),
                (self.group6()[3] * other.group0()[1]),
                (self.group6()[3] * other.group0()[2]),
                (-(self.group7()[2] * other.group0()[2]) - (self.group7()[0] * other.group0()[0]) - (self.group7()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group8()[2] * other.group0()[2]) - (self.group8()[0] * other.group0()[0]) - (self.group8()[1] * other.group0()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group9()[0] * other.group0()[0]),
                (self.group9()[0] * other.group0()[1]),
                (self.group9()[0] * other.group0()[2]),
                (-(self.group9()[3] * other.group0()[2]) - (self.group9()[1] * other.group0()[0]) - (self.group9()[2] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (Simd32x3::from(self[e45]) * other.group0()),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                0.0,
            ]),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Motor> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       30       49        0
    //    simd3        4        7        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       36       59        0
    //  no simd       50       82        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group9()[0] * other.group1()[3])
                    - (self.group5()[2] * other.group0()[2])
                    - (self.group5()[1] * other.group0()[1])
                    - (self.group5()[0] * other.group0()[0])
                    - (self.group3()[2] * other.group1()[2])
                    - (self.group3()[1] * other.group1()[1])
                    + (self.group0()[0] * other.group0()[3])
                    - (self.group3()[0] * other.group1()[0])),
                (self.group0()[1] * other.group0()[3]),
            ]),
            // e1, e2, e3, e4
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    ((self.group7()[2] * other.group1()[1]) - (self.group7()[1] * other.group1()[2]) + (self.group6()[3] * other.group0()[0])),
                    (-(self.group7()[2] * other.group1()[0]) + (self.group7()[0] * other.group1()[2]) + (self.group6()[3] * other.group0()[1])),
                    ((self.group7()[1] * other.group1()[0]) - (self.group7()[0] * other.group1()[1]) + (self.group6()[3] * other.group0()[2])),
                    (-(self.group7()[2] * other.group0()[2]) - (self.group7()[1] * other.group0()[1]) - (self.group7()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group8()[2] * other.group0()[2])
                - (self.group8()[1] * other.group0()[1])
                - (self.group8()[0] * other.group0()[0])
                - (self.group6()[2] * other.group1()[2])
                - (self.group6()[1] * other.group1()[1])
                - (self.group6()[0] * other.group1()[0])
                + (self.group0()[1] * other.group1()[3])
                + (self[e1] * other.group0()[3])),
            // e41, e42, e43, e45
            ((self.group3() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group9()[0] * other.group0()[0]),
                    (self.group9()[0] * other.group0()[1]),
                    (self.group9()[0] * other.group0()[2]),
                    (-(self.group9()[3] * other.group0()[2]) - (self.group9()[2] * other.group0()[1]) - (self.group9()[1] * other.group0()[0])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                + (self.group4() * Simd32x3::from(other.group0()[3]))
                + Simd32x3::from([
                    ((self.group9()[3] * other.group1()[1]) - (self.group9()[2] * other.group1()[2])),
                    (-(self.group9()[3] * other.group1()[0]) + (self.group9()[1] * other.group1()[2])),
                    ((self.group9()[2] * other.group1()[0]) - (self.group9()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12
            ((self.group5() * Simd32x3::from(other.group0()[3])) + (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[0]) + (self.group6()[0] * other.group0()[3])),
                ((self.group0()[1] * other.group0()[1]) + (self.group6()[1] * other.group0()[3])),
                ((self.group0()[1] * other.group0()[2]) + (self.group6()[2] * other.group0()[3])),
                (self.group6()[3] * other.group0()[3]),
            ]),
            // e423, e431, e412
            (self.group7() * Simd32x3::from(other.group0()[3])),
            // e235, e315, e125
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])) + (self.group8() * Simd32x3::from(other.group0()[3]))),
            // e1234, e4235, e4315, e4125
            (self.group9() * Simd32x4::from(other.group0()[3])),
            // e3215
            (self[e45] * other.group0()[3]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       20        0
    //    simd3        0        2        0
    // Totals...
    // yes simd       12       22        0
    //  no simd       12       26        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group9()[0] * other.group0()[3]) - (self.group3()[2] * other.group0()[2]) - (self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group7()[1] * other.group0()[2]) + (self.group7()[2] * other.group0()[1])),
                ((self.group7()[0] * other.group0()[2]) - (self.group7()[2] * other.group0()[0])),
                (-(self.group7()[0] * other.group0()[1]) + (self.group7()[1] * other.group0()[0])),
                0.0,
            ]),
            // e5
            (-(self.group6()[2] * other.group0()[2]) - (self.group6()[1] * other.group0()[1]) + (self.group0()[1] * other.group0()[3]) - (self.group6()[0] * other.group0()[0])),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from([
                (-(self.group9()[2] * other.group0()[2]) + (self.group9()[3] * other.group0()[1])),
                ((self.group9()[1] * other.group0()[2]) - (self.group9()[3] * other.group0()[0])),
                (-(self.group9()[1] * other.group0()[1]) + (self.group9()[2] * other.group0()[0])),
            ]),
            // e23, e31, e12
            (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<MotorOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       29        0
    //    simd3        1        5        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       16       37        0
    //  no simd       24       56        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group5()[2] * other.group0()[2]) - (self.group5()[1] * other.group0()[1]) + (self.group0()[0] * other.group0()[3])
                    - (self.group5()[0] * other.group0()[0])),
                (self.group0()[1] * other.group0()[3]),
            ]),
            // e1, e2, e3, e4
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group6()[3] * other.group0()[0]),
                    (self.group6()[3] * other.group0()[1]),
                    (self.group6()[3] * other.group0()[2]),
                    (-(self.group7()[2] * other.group0()[2]) - (self.group7()[1] * other.group0()[1]) - (self.group7()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group8()[2] * other.group0()[2]) - (self.group8()[1] * other.group0()[1]) + (self[e1] * other.group0()[3]) - (self.group8()[0] * other.group0()[0])),
            // e41, e42, e43, e45
            ((self.group3() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group9()[0] * other.group0()[0]),
                    (self.group9()[0] * other.group0()[1]),
                    (self.group9()[0] * other.group0()[2]),
                    (-(self.group9()[3] * other.group0()[2]) - (self.group9()[2] * other.group0()[1]) - (self.group9()[1] * other.group0()[0])),
                ])),
            // e15, e25, e35
            ((self.group4() * Simd32x3::from(other.group0()[3])) + (Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e23, e31, e12
            (self.group5() * Simd32x3::from(other.group0()[3])),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[0]) + (self.group6()[0] * other.group0()[3])),
                ((self.group0()[1] * other.group0()[1]) + (self.group6()[1] * other.group0()[3])),
                ((self.group0()[1] * other.group0()[2]) + (self.group6()[2] * other.group0()[3])),
                (self.group6()[3] * other.group0()[3]),
            ]),
            // e423, e431, e412
            (self.group7() * Simd32x3::from(other.group0()[3])),
            // e235, e315, e125
            (self.group8() * Simd32x3::from(other.group0()[3])),
            // e1234, e4235, e4315, e4125
            (self.group9() * Simd32x4::from(other.group0()[3])),
            // e3215
            (self[e45] * other.group0()[3]),
        );
    }
}
impl AntiWedge<MultiVector> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       95      107        0
    //    simd3       20       24        0
    //    simd4       14       16        0
    // Totals...
    // yes simd      129      147        0
    //  no simd      211      243        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e45] * other.group1()[3])
                    + (self.group9()[3] * other.group1()[2])
                    + (self.group9()[2] * other.group1()[1])
                    + (self.group9()[1] * other.group1()[0])
                    + (self.group9()[0] * other[e1])
                    - (self.group8()[2] * other.group3()[2])
                    - (self.group8()[1] * other.group3()[1])
                    - (self.group8()[0] * other.group3()[0])
                    - (self.group7()[2] * other.group4()[2])
                    - (self.group7()[1] * other.group4()[1])
                    - (self.group7()[0] * other.group4()[0])
                    - (self.group6()[3] * other.group3()[3])
                    - (self.group6()[2] * other.group5()[2])
                    - (self.group6()[1] * other.group5()[1])
                    - (self.group6()[0] * other.group5()[0])
                    - (self.group5()[2] * other.group6()[2])
                    - (self.group5()[1] * other.group6()[1])
                    - (self.group5()[0] * other.group6()[0])
                    - (self.group4()[2] * other.group7()[2])
                    - (self.group4()[1] * other.group7()[1])
                    - (self.group4()[0] * other.group7()[0])
                    - (self.group3()[3] * other.group6()[3])
                    - (self.group3()[2] * other.group8()[2])
                    - (self.group3()[1] * other.group8()[1])
                    - (self.group3()[0] * other.group8()[0])
                    + (self[e1] * other.group9()[0])
                    + (self.group1()[3] * other[e45])
                    + (self.group1()[2] * other.group9()[3])
                    + (self.group1()[1] * other.group9()[2])
                    + (self.group1()[0] * other.group9()[1])
                    + (self.group0()[0] * other.group0()[1])
                    + (self.group0()[1] * other.group0()[0])),
                (self.group0()[1] * other.group0()[1]),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 2, 3, 1, 3) * Simd32x4::from([other.group5()[2], other.group5()[0], other.group5()[1], other.group3()[2]]))
                + (swizzle!(self.group9(), 0, 0, 0, 2) * Simd32x4::from([other.group4()[0], other.group4()[1], other.group4()[2], other.group3()[1]]))
                - (swizzle!(other.group9(), 2, 3, 1, 0) * Simd32x4::from([self.group5()[2], self.group5()[0], self.group5()[1], self.group3()[3]]))
                - (swizzle!(other.group9(), 0, 0, 0, 3) * Simd32x4::from([self.group4()[0], self.group4()[1], self.group4()[2], self.group3()[2]]))
                + (Simd32x4::from(self.group0()[1]) * other.group1())
                + (self.group1() * Simd32x4::from(other.group0()[1]))
                + Simd32x4::from([
                    (-(self[e45] * other.group3()[0]) - (self.group9()[3] * other.group5()[1]) - (self.group8()[2] * other.group7()[1])
                        + (self.group8()[1] * other.group7()[2])
                        + (self.group7()[2] * other.group8()[1])
                        - (self.group7()[1] * other.group8()[2])
                        + (self.group6()[3] * other.group6()[0])
                        + (self.group6()[0] * other.group6()[3])
                        + (self.group5()[1] * other.group9()[3])
                        + (self.group3()[0] * other[e45])),
                    (-(self[e45] * other.group3()[1]) - (self.group9()[1] * other.group5()[2]) + (self.group8()[2] * other.group7()[0])
                        - (self.group8()[0] * other.group7()[2])
                        - (self.group7()[2] * other.group8()[0])
                        + (self.group7()[0] * other.group8()[2])
                        + (self.group6()[3] * other.group6()[1])
                        + (self.group6()[1] * other.group6()[3])
                        + (self.group5()[2] * other.group9()[1])
                        + (self.group3()[1] * other[e45])),
                    (-(self[e45] * other.group3()[2]) - (self.group9()[2] * other.group5()[0]) - (self.group8()[1] * other.group7()[0])
                        + (self.group8()[0] * other.group7()[1])
                        + (self.group7()[1] * other.group8()[0])
                        - (self.group7()[0] * other.group8()[1])
                        + (self.group6()[3] * other.group6()[2])
                        + (self.group6()[2] * other.group6()[3])
                        + (self.group5()[0] * other.group9()[2])
                        + (self.group3()[2] * other[e45])),
                    ((self.group9()[1] * other.group3()[0]) + (self.group9()[0] * other.group3()[3])
                        - (self.group7()[2] * other.group6()[2])
                        - (self.group7()[1] * other.group6()[1])
                        - (self.group7()[0] * other.group6()[0])
                        - (self.group6()[2] * other.group7()[2])
                        - (self.group6()[1] * other.group7()[1])
                        - (self.group6()[0] * other.group7()[0])
                        - (self.group3()[1] * other.group9()[2])
                        - (self.group3()[0] * other.group9()[1])),
                ])),
            // e5
            (-(self[e45] * other.group3()[3])
                - (self.group9()[3] * other.group4()[2])
                - (self.group9()[2] * other.group4()[1])
                - (self.group9()[1] * other.group4()[0])
                - (self.group8()[2] * other.group6()[2])
                - (self.group8()[1] * other.group6()[1])
                - (self.group8()[0] * other.group6()[0])
                - (self.group6()[2] * other.group8()[2])
                - (self.group6()[1] * other.group8()[1])
                - (self.group6()[0] * other.group8()[0])
                + (self.group4()[2] * other.group9()[3])
                + (self.group4()[1] * other.group9()[2])
                + (self.group4()[0] * other.group9()[1])
                + (self.group3()[3] * other[e45])
                + (self.group0()[1] * other[e1])
                + (self[e1] * other.group0()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group9(), 3, 1, 2, 3) * Simd32x4::from([other.group7()[1], other.group7()[2], other.group7()[0], other.group6()[2]]))
                - (swizzle!(other.group9(), 3, 1, 2, 3) * Simd32x4::from([self.group7()[1], self.group7()[2], self.group7()[0], self.group6()[2]]))
                + (Simd32x4::from(self.group0()[1]) * other.group3())
                + (self.group3() * Simd32x4::from(other.group0()[1]))
                + Simd32x4::from([
                    ((self.group9()[2] * other.group7()[2])
                        + (self.group9()[0] * other.group6()[0])
                        + (self.group7()[2] * other.group9()[2])
                        + (self.group6()[0] * other.group9()[0])),
                    ((self.group9()[3] * other.group7()[0])
                        + (self.group9()[0] * other.group6()[1])
                        + (self.group7()[0] * other.group9()[3])
                        + (self.group6()[1] * other.group9()[0])),
                    ((self.group9()[1] * other.group7()[1])
                        + (self.group9()[0] * other.group6()[2])
                        + (self.group7()[1] * other.group9()[1])
                        + (self.group6()[2] * other.group9()[0])),
                    (-(self.group9()[2] * other.group6()[1])
                        - (self.group9()[1] * other.group6()[0])
                        - (self.group6()[1] * other.group9()[2])
                        - (self.group6()[0] * other.group9()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))
                + (swizzle!(other.group8(), 1, 2, 0) * Simd32x3::from([self.group9()[3], self.group9()[1], self.group9()[2]]))
                - (swizzle!(other.group8(), 2, 0, 1) * Simd32x3::from([self.group9()[2], self.group9()[3], self.group9()[1]]))
                - (swizzle!(self.group8(), 2, 0, 1) * Simd32x3::from([other.group9()[2], other.group9()[3], other.group9()[1]]))
                + (swizzle!(self.group8(), 1, 2, 0) * Simd32x3::from([other.group9()[3], other.group9()[1], other.group9()[2]]))
                + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))
                + (Simd32x3::from(self.group0()[1]) * other.group4())
                + (self.group4() * Simd32x3::from(other.group0()[1]))),
            // e23, e31, e12
            ((Simd32x3::from(self[e45]) * other.group7()) - (Simd32x3::from(other.group6()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                + (Simd32x3::from(self.group9()[0]) * other.group8())
                + (self.group8() * Simd32x3::from(other.group9()[0]))
                + (self.group7() * Simd32x3::from(other[e45]))
                - (Simd32x3::from(self.group6()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))
                + (Simd32x3::from(self.group0()[1]) * other.group5())
                + (self.group5() * Simd32x3::from(other.group0()[1]))),
            // e415, e425, e435, e321
            ((swizzle!(self.group9(), 3, 1, 2, 0) * Simd32x4::from([other.group9()[2], other.group9()[3], other.group9()[1], other[e45]]))
                - (swizzle!(other.group9(), 3, 1, 2, 0) * Simd32x4::from([self.group9()[2], self.group9()[3], self.group9()[1], self[e45]]))
                + (Simd32x4::from(self.group0()[1]) * other.group6())
                + (self.group6() * Simd32x4::from(other.group0()[1]))),
            // e423, e431, e412
            (-(Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                + (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))
                + (Simd32x3::from(self.group0()[1]) * other.group7())
                + (self.group7() * Simd32x3::from(other.group0()[1]))),
            // e235, e315, e125
            (-(Simd32x3::from(self[e45]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))
                + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                + (Simd32x3::from(self.group0()[1]) * other.group8())
                + (self.group8() * Simd32x3::from(other.group0()[1]))),
            // e1234, e4235, e4315, e4125
            ((Simd32x4::from(self.group0()[1]) * other.group9()) + (self.group9() * Simd32x4::from(other.group0()[1]))),
            // e3215
            ((self.group0()[1] * other[e45]) + (self[e45] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       18        0
    //    simd3        0        2        0
    // Totals...
    // yes simd       10       20        0
    //  no simd       10       24        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group4()[2] * other.group0()[2]) - (self.group4()[0] * other.group0()[0]) - (self.group4()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group8()[1] * other.group0()[2]) - (self.group8()[2] * other.group0()[1])),
                (-(self.group8()[0] * other.group0()[2]) + (self.group8()[2] * other.group0()[0])),
                ((self.group8()[0] * other.group0()[1]) - (self.group8()[1] * other.group0()[0])),
                (-(self.group6()[2] * other.group0()[2]) - (self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group9()[2] * other.group0()[2]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[3] * other.group0()[0])),
                ((self.group9()[1] * other.group0()[1]) - (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(self[e45]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       15        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2]) - (self.group8()[0] * other.group0()[0]) - (self.group8()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self[e45] * other.group0()[0] * -1.0),
                (self[e45] * other.group0()[1] * -1.0),
                (self[e45] * other.group0()[2] * -1.0),
                ((self.group9()[3] * other.group0()[2]) + (self.group9()[1] * other.group0()[0]) + (self.group9()[2] * other.group0()[1])),
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        7        0
    //    simd3        0        3        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0       12        0
    //  no simd        0       24        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self[e1] * other[e1234]), 0.0]),
            // e1, e2, e3, e4
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group4()[0], self.group4()[1], self.group4()[2], self.group3()[3]]) * Simd32x4::from(-1.0)),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([(self.group6()[0] * other[e1234]), (self.group6()[1] * other[e1234]), (self.group6()[2] * other[e1234]), 0.0]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (self.group8() * Simd32x3::from(other[e1234])),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self[e45] * other[e1234] * -1.0)]),
            // e423, e431, e412
            (Simd32x3::from(other[e1234]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group0()[1] * other[e1234]), 0.0, 0.0, 0.0]),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd3        0        2        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       16        0
    //  no simd       15       26        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e45] * other.group0()[3]) - (self.group4()[2] * other.group0()[2]) - (self.group4()[0] * other.group0()[0]) - (self.group4()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group8()[1], self.group8()[2], self.group8()[0], self.group0()[1]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group8()[2], self.group8()[0], self.group8()[1], self.group6()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group6()[1] * other.group0()[1]) - (self.group6()[0] * other.group0()[0]))])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group9()[2] * other.group0()[2]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[3] * other.group0()[0])),
                ((self.group9()[1] * other.group0()[1]) - (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       22        0
    //    simd3        0        3        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       26        0
    //  no simd       12       35        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2]) - (self.group8()[1] * other.group0()[1]) + (self[e1] * other.group0()[3]) - (self.group8()[0] * other.group0()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group4()[0], self.group4()[1], self.group4()[2], self.group3()[3]]))
                + Simd32x4::from([
                    ((self[e45] * other.group0()[0]) * -1.0),
                    ((self[e45] * other.group0()[1]) * -1.0),
                    ((self[e45] * other.group0()[2]) * -1.0),
                    ((self.group9()[3] * other.group0()[2]) + (self.group9()[2] * other.group0()[1]) + (self.group9()[1] * other.group0()[0])),
                ])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[0]) + (self.group6()[0] * other.group0()[3])),
                ((self.group0()[1] * other.group0()[1]) + (self.group6()[1] * other.group0()[3])),
                ((self.group0()[1] * other.group0()[2]) + (self.group6()[2] * other.group0()[3])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (self.group8() * Simd32x3::from(other.group0()[3])),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self[e45] * other.group0()[3] * -1.0)]),
            // e423, e431, e412
            (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group0()[1] * other.group0()[3]), 0.0, 0.0, 0.0]),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Origin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self[e45] * other[e4]), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other[e4])]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       32        0
    //    simd3        4        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       20       42        0
    //  no simd       34       64        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other.group0()[3]) + (self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group5()[2], self.group5()[0], self.group5()[1], self.group3()[2]]))
                + Simd32x4::from([
                    ((self.group3()[0] * other.group0()[3]) + (self.group5()[1] * other.group0()[2])),
                    ((self.group5()[2] * other.group0()[0]) + (self.group3()[1] * other.group0()[3])),
                    ((self.group3()[2] * other.group0()[3]) + (self.group5()[0] * other.group0()[1])),
                    (-(self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
                ])),
            // e5
            ((self.group4()[2] * other.group0()[2]) + (self.group4()[1] * other.group0()[1]) + (self.group3()[3] * other.group0()[3]) + (self.group4()[0] * other.group0()[0])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group7()[1], self.group7()[2], self.group7()[0], self.group6()[2]]))
                + Simd32x4::from([
                    (self.group7()[2] * other.group0()[1]),
                    (self.group7()[0] * other.group0()[2]),
                    (self.group7()[1] * other.group0()[0]),
                    (-(self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group8(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))
                + (swizzle!(self.group8(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))),
            // e23, e31, e12
            (-(Simd32x3::from(self.group6()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])) + (self.group7() * Simd32x3::from(other.group0()[3]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group9()[2] * other.group0()[2]) + (self.group9()[3] * other.group0()[1])),
                ((self.group9()[1] * other.group0()[2]) - (self.group9()[3] * other.group0()[0])),
                (-(self.group9()[1] * other.group0()[1]) + (self.group9()[2] * other.group0()[0])),
                (self.group9()[0] * other.group0()[3]),
            ]),
            // e423, e431, e412
            (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e235, e315, e125
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                - (Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
            ]),
            // e3215
            (self.group0()[1] * other.group0()[3]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       17       33        0
    //    simd3        1        7        0
    // Totals...
    // yes simd       18       40        0
    //  no simd       20       54        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group5()[1] * other.group0()[2]) - (self.group5()[2] * other.group0()[1])),
                (-(self.group5()[0] * other.group0()[2]) + (self.group5()[2] * other.group0()[0])),
                ((self.group5()[0] * other.group0()[1]) - (self.group5()[1] * other.group0()[0])),
                (-(self.group3()[2] * other.group0()[2]) - (self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
            ]),
            // e5
            ((self.group4()[2] * other.group0()[2]) + (self.group4()[0] * other.group0()[0]) + (self.group4()[1] * other.group0()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group7()[1] * other.group0()[2]) + (self.group7()[2] * other.group0()[1])),
                ((self.group7()[0] * other.group0()[2]) - (self.group7()[2] * other.group0()[0])),
                (-(self.group7()[0] * other.group0()[1]) + (self.group7()[1] * other.group0()[0])),
                (-(self.group6()[2] * other.group0()[2]) - (self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            ((swizzle!(self.group8(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group8(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
            // e23, e31, e12
            (Simd32x3::from(self.group6()[3]) * other.group0() * Simd32x3::from(-1.0)),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group9()[2] * other.group0()[2]) + (self.group9()[3] * other.group0()[1])),
                ((self.group9()[1] * other.group0()[2]) - (self.group9()[3] * other.group0()[0])),
                (-(self.group9()[1] * other.group0()[1]) + (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e423, e431, e412
            (Simd32x3::from(self.group9()[0]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self[e45]) * other.group0() * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
            ]),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<RoundPoint> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        4        7        0
    //  no simd        4       10        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e45] * other.group0()[3])
                    + (self.group9()[3] * other.group0()[2])
                    + (self.group9()[2] * other.group0()[1])
                    + (self.group9()[0] * other[e2])
                    + (self.group9()[1] * other.group0()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e5
            (self.group0()[1] * other[e2]),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        4        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([((self.group9()[0] * other.group0()[1]) + (self[e45] * other.group0()[0])), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[0])]),
            // e5
            (self.group0()[1] * other.group0()[1]),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Scalar> for MultiVector {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Scalar) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[1] * other[scalar]));
    }
}
impl AntiWedge<Sphere> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       30        0
    //    simd3        6       10        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       25       45        0
    //  no simd       49       80        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e1] * other[e4315])
                    + (self.group1()[3] * other.group0()[3])
                    + (self.group1()[2] * other.group0()[2])
                    + (self.group1()[0] * other.group0()[0])
                    + (self.group1()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group5()[2], self.group5()[0], self.group5()[1], self.group3()[2]]))
                - (Simd32x4::from(other[e4315]) * Simd32x4::from([self.group4()[0], self.group4()[1], self.group4()[2], self.group3()[3]]))
                + Simd32x4::from([
                    ((self.group5()[1] * other.group0()[2]) + (self.group3()[0] * other.group0()[3])),
                    ((self.group5()[2] * other.group0()[0]) + (self.group3()[1] * other.group0()[3])),
                    ((self.group5()[0] * other.group0()[1]) + (self.group3()[2] * other.group0()[3])),
                    (-(self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
                ])),
            // e5
            ((self.group4()[2] * other.group0()[2]) + (self.group4()[1] * other.group0()[1]) + (self.group3()[3] * other.group0()[3]) + (self.group4()[0] * other.group0()[0])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group7()[1], self.group7()[2], self.group7()[0], self.group6()[2]]))
                + Simd32x4::from([
                    ((self.group7()[2] * other.group0()[1]) + (self.group6()[0] * other[e4315])),
                    ((self.group6()[1] * other[e4315]) + (self.group7()[0] * other.group0()[2])),
                    ((self.group7()[1] * other.group0()[0]) + (self.group6()[2] * other[e4315])),
                    (-(self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group8(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))
                + (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))
                + (swizzle!(self.group8(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))),
            // e23, e31, e12
            ((self.group8() * Simd32x3::from(other[e4315])) - (Simd32x3::from(self.group6()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                + (self.group7() * Simd32x3::from(other.group0()[3]))),
            // e415, e425, e435, e321
            ((swizzle!(self.group9(), 3, 1, 2, 0) * swizzle!(other.group0(), 1, 2, 0, 3))
                - Simd32x4::from([
                    (self.group9()[2] * other.group0()[2]),
                    (self.group9()[3] * other.group0()[0]),
                    (self.group9()[1] * other.group0()[1]),
                    (self[e45] * other[e4315]),
                ])),
            // e423, e431, e412
            ((Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                - (Simd32x3::from(other[e4315]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e235, e315, e125
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                - (Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other[e4315], other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e3215
            (self.group0()[1] * other.group0()[3]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       18        0
    //    simd3        1        6        0
    // Totals...
    // yes simd        6       24        0
    //  no simd        8       36        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([((self.group1()[3] * other.group0()[0]) + (self[e1] * other.group0()[1])), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group3()[0] * other.group0()[0]) - (self.group4()[0] * other.group0()[1])),
                ((self.group3()[1] * other.group0()[0]) - (self.group4()[1] * other.group0()[1])),
                ((self.group3()[2] * other.group0()[0]) - (self.group4()[2] * other.group0()[1])),
                (self.group3()[3] * other.group0()[1] * -1.0),
            ]),
            // e5
            (self.group3()[3] * other.group0()[0]),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group6()[0] * other.group0()[1]),
                (self.group6()[1] * other.group0()[1]),
                (self.group6()[2] * other.group0()[1]),
                0.0,
            ]),
            // e15, e25, e35
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]])),
            // e23, e31, e12
            ((self.group7() * Simd32x3::from(other.group0()[0])) + (self.group8() * Simd32x3::from(other.group0()[1]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, ((self.group9()[0] * other.group0()[0]) - (self[e45] * other.group0()[1]))]),
            // e423, e431, e412
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group0()[1] * other.group0()[1]), 0.0, 0.0, 0.0]),
            // e3215
            (self.group0()[1] * other.group0()[0]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       28        0
    //    simd3        3        8        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       19       40        0
    //  no simd       34       68        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e1] * other.group0()[3]) + (self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 3, 0, 3) * Simd32x4::from([self.group5()[2], self.group4()[1], self.group5()[1], self.group3()[3]]))
                - (swizzle!(other.group0(), 3, 2, 3, 2) * Simd32x4::from([self.group4()[0], self.group5()[0], self.group4()[2], self.group3()[2]]))
                + Simd32x4::from([
                    (self.group5()[1] * other.group0()[2]),
                    (self.group5()[2] * other.group0()[0]),
                    (self.group5()[0] * other.group0()[1]),
                    (-(self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
                ])),
            // e5
            ((self.group4()[2] * other.group0()[2]) + (self.group4()[0] * other.group0()[0]) + (self.group4()[1] * other.group0()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group7()[1], self.group7()[2], self.group7()[0], self.group6()[2]]))
                + Simd32x4::from([
                    ((self.group7()[2] * other.group0()[1]) + (self.group6()[0] * other.group0()[3])),
                    ((self.group6()[1] * other.group0()[3]) + (self.group7()[0] * other.group0()[2])),
                    ((self.group7()[1] * other.group0()[0]) + (self.group6()[2] * other.group0()[3])),
                    (-(self.group6()[0] * other.group0()[0]) - (self.group6()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35
            ((swizzle!(self.group8(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group8(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12
            (-(Simd32x3::from(self.group6()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])) + (self.group8() * Simd32x3::from(other.group0()[3]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group9()[2] * other.group0()[2]) + (self.group9()[3] * other.group0()[1])),
                ((self.group9()[1] * other.group0()[2]) - (self.group9()[3] * other.group0()[0])),
                (-(self.group9()[1] * other.group0()[1]) + (self.group9()[2] * other.group0()[0])),
                (self[e45] * other.group0()[3] * -1.0),
            ]),
            // e423, e431, e412
            ((Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                - (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e235, e315, e125
            (Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[1]) * swizzle!(other.group0(), 3, 0, 1, 2)),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<VersorEven> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       45       60        0
    //    simd3        7       10        0
    //    simd4        6        8        0
    // Totals...
    // yes simd       58       78        0
    //  no simd       90      122        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e45] * other.group3()[3])
                    + (self.group9()[3] * other.group3()[2])
                    + (self.group9()[2] * other.group3()[1])
                    + (self.group9()[1] * other.group3()[0])
                    + (self.group9()[0] * other.group2()[3])
                    - (self.group5()[2] * other.group1()[2])
                    - (self.group5()[1] * other.group1()[1])
                    - (self.group5()[0] * other.group1()[0])
                    - (self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    - (self.group4()[0] * other.group0()[0])
                    - (self.group3()[3] * other.group1()[3])
                    - (self.group3()[2] * other.group2()[2])
                    - (self.group3()[1] * other.group2()[1])
                    + (self.group0()[0] * other.group0()[3])
                    - (self.group3()[0] * other.group2()[0])),
                (self.group0()[1] * other.group0()[3]),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group8()[2], self.group8()[0], self.group8()[1], self.group6()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group8()[1], self.group8()[2], self.group8()[0], self.group1()[3]]))
                + (Simd32x4::from(self.group0()[1]) * other.group3())
                + Simd32x4::from([
                    ((self.group7()[2] * other.group2()[1]) - (self.group7()[1] * other.group2()[2])
                        + (self.group6()[3] * other.group1()[0])
                        + (self.group6()[0] * other.group1()[3])
                        + (self.group1()[0] * other.group0()[3])),
                    (-(self.group7()[2] * other.group2()[0])
                        + (self.group7()[0] * other.group2()[2])
                        + (self.group6()[3] * other.group1()[1])
                        + (self.group6()[1] * other.group1()[3])
                        + (self.group1()[1] * other.group0()[3])),
                    ((self.group7()[1] * other.group2()[0]) - (self.group7()[0] * other.group2()[1])
                        + (self.group6()[3] * other.group1()[2])
                        + (self.group6()[2] * other.group1()[3])
                        + (self.group1()[2] * other.group0()[3])),
                    (-(self.group7()[2] * other.group1()[2])
                        - (self.group7()[1] * other.group1()[1])
                        - (self.group7()[0] * other.group1()[0])
                        - (self.group6()[1] * other.group0()[1])
                        - (self.group6()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group8()[2] * other.group1()[2])
                - (self.group8()[1] * other.group1()[1])
                - (self.group8()[0] * other.group1()[0])
                - (self.group6()[2] * other.group2()[2])
                - (self.group6()[1] * other.group2()[1])
                - (self.group6()[0] * other.group2()[0])
                + (self.group0()[1] * other.group2()[3])
                + (self[e1] * other.group0()[3])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group9(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group9()[2], self.group9()[3], self.group9()[1], self.group3()[3]]))
                + Simd32x4::from([
                    ((self.group3()[0] * other.group0()[3]) + (self.group9()[0] * other.group1()[0])),
                    ((self.group3()[1] * other.group0()[3]) + (self.group9()[0] * other.group1()[1])),
                    ((self.group3()[2] * other.group0()[3]) + (self.group9()[0] * other.group1()[2])),
                    (-(self.group9()[2] * other.group1()[1]) - (self.group9()[1] * other.group1()[0])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (self.group4() * Simd32x3::from(other.group0()[3]))
                + Simd32x3::from([
                    ((self.group9()[3] * other.group2()[1]) - (self.group9()[2] * other.group2()[2])),
                    (-(self.group9()[3] * other.group2()[0]) + (self.group9()[1] * other.group2()[2])),
                    ((self.group9()[2] * other.group2()[0]) - (self.group9()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                - (Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                + (self.group5() * Simd32x3::from(other.group0()[3]))
                + (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]]))),
            // e415, e425, e435, e321
            ((Simd32x4::from(self.group0()[1]) * other.group1()) + (self.group6() * Simd32x4::from(other.group0()[3]))),
            // e423, e431, e412
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])) + (self.group7() * Simd32x3::from(other.group0()[3]))),
            // e235, e315, e125
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]])) + (self.group8() * Simd32x3::from(other.group0()[3]))),
            // e1234, e4235, e4315, e4125
            (self.group9() * Simd32x4::from(other.group0()[3])),
            // e3215
            (self[e45] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       38       57        0
    //    simd3        6        9        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       49       72        0
    //  no simd       76      108        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e45] * other.group1()[3]) + (self.group9()[0] * other.group2()[3])
                    - (self.group5()[2] * other.group1()[2])
                    - (self.group5()[1] * other.group1()[1])
                    - (self.group5()[0] * other.group1()[0])
                    - (self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    - (self.group4()[0] * other.group0()[0])
                    - (self.group3()[2] * other.group2()[2])
                    - (self.group3()[1] * other.group2()[1])
                    + (self.group0()[0] * other.group0()[3])
                    - (self.group3()[0] * other.group2()[0])),
                (self.group0()[1] * other.group0()[3]),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group8()[2], self.group8()[0], self.group8()[1], self.group6()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group8()[1], self.group8()[2], self.group8()[0], self.group1()[3]]))
                + (other.group1() * Simd32x4::from([self.group6()[3], self.group6()[3], self.group6()[3], self.group0()[1]]))
                + Simd32x4::from([
                    ((self.group7()[2] * other.group2()[1]) - (self.group7()[1] * other.group2()[2]) + (self.group1()[0] * other.group0()[3])),
                    (-(self.group7()[2] * other.group2()[0]) + (self.group7()[0] * other.group2()[2]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group7()[1] * other.group2()[0]) - (self.group7()[0] * other.group2()[1]) + (self.group1()[2] * other.group0()[3])),
                    (-(self.group7()[2] * other.group1()[2])
                        - (self.group7()[1] * other.group1()[1])
                        - (self.group7()[0] * other.group1()[0])
                        - (self.group6()[1] * other.group0()[1])
                        - (self.group6()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group8()[2] * other.group1()[2])
                - (self.group8()[1] * other.group1()[1])
                - (self.group8()[0] * other.group1()[0])
                - (self.group6()[2] * other.group2()[2])
                - (self.group6()[1] * other.group2()[1])
                - (self.group6()[0] * other.group2()[0])
                + (self.group0()[1] * other.group2()[3])
                + (self[e1] * other.group0()[3])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group9(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group9()[2], self.group9()[3], self.group9()[1], self.group3()[3]]))
                + Simd32x4::from([
                    ((self.group3()[0] * other.group0()[3]) + (self.group9()[0] * other.group1()[0])),
                    ((self.group3()[1] * other.group0()[3]) + (self.group9()[0] * other.group1()[1])),
                    ((self.group3()[2] * other.group0()[3]) + (self.group9()[0] * other.group1()[2])),
                    (-(self.group9()[2] * other.group1()[1]) - (self.group9()[1] * other.group1()[0])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (self.group4() * Simd32x3::from(other.group0()[3]))
                + Simd32x3::from([
                    ((self.group9()[3] * other.group2()[1]) - (self.group9()[2] * other.group2()[2])),
                    (-(self.group9()[3] * other.group2()[0]) + (self.group9()[1] * other.group2()[2])),
                    ((self.group9()[2] * other.group2()[0]) - (self.group9()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                + (self.group5() * Simd32x3::from(other.group0()[3]))
                + (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[0]) + (self.group6()[0] * other.group0()[3])),
                ((self.group0()[1] * other.group1()[1]) + (self.group6()[1] * other.group0()[3])),
                ((self.group0()[1] * other.group1()[2]) + (self.group6()[2] * other.group0()[3])),
                (self.group6()[3] * other.group0()[3]),
            ]),
            // e423, e431, e412
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])) + (self.group7() * Simd32x3::from(other.group0()[3]))),
            // e235, e315, e125
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]])) + (self.group8() * Simd32x3::from(other.group0()[3]))),
            // e1234, e4235, e4315, e4125
            (self.group9() * Simd32x4::from(other.group0()[3])),
            // e3215
            (self[e45] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       37       52        0
    //    simd3        5        8        0
    //    simd4        3        5        0
    // Totals...
    // yes simd       45       65        0
    //  no simd       64       96        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group9()[3] * other.group0()[3]) + (self.group9()[2] * other.group0()[2]) + (self.group9()[1] * other.group0()[1]) + (self.group9()[0] * other.group2()[3])
                    - (self.group5()[2] * other.group1()[2])
                    - (self.group5()[1] * other.group1()[1])
                    - (self.group5()[0] * other.group1()[0])
                    - (self.group3()[3] * other.group1()[3])
                    - (self.group3()[2] * other.group2()[2])
                    - (self.group3()[1] * other.group2()[1])
                    + (self.group0()[0] * other.group0()[0])
                    - (self.group3()[0] * other.group2()[0])),
                (self.group0()[1] * other.group0()[0]),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 1, 2, 3, 0) * Simd32x4::from([self.group0()[1], self.group0()[1], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group7()[2] * other.group2()[1]) - (self.group7()[1] * other.group2()[2])
                        + (self.group6()[3] * other.group1()[0])
                        + (self.group6()[0] * other.group1()[3])
                        + (self.group1()[0] * other.group0()[0])),
                    (-(self.group7()[2] * other.group2()[0])
                        + (self.group7()[0] * other.group2()[2])
                        + (self.group6()[3] * other.group1()[1])
                        + (self.group6()[1] * other.group1()[3])
                        + (self.group1()[1] * other.group0()[0])),
                    ((self.group7()[1] * other.group2()[0]) - (self.group7()[0] * other.group2()[1])
                        + (self.group6()[3] * other.group1()[2])
                        + (self.group6()[2] * other.group1()[3])
                        + (self.group1()[2] * other.group0()[0])),
                    (-(self.group7()[2] * other.group1()[2]) - (self.group7()[1] * other.group1()[1]) - (self.group7()[0] * other.group1()[0])),
                ])),
            // e5
            (-(self.group8()[2] * other.group1()[2])
                - (self.group8()[1] * other.group1()[1])
                - (self.group8()[0] * other.group1()[0])
                - (self.group6()[2] * other.group2()[2])
                - (self.group6()[1] * other.group2()[1])
                - (self.group6()[0] * other.group2()[0])
                + (self.group0()[1] * other.group2()[3])
                + (self[e1] * other.group0()[0])),
            // e41, e42, e43, e45
            ((self.group3() * Simd32x4::from(other.group0()[0]))
                + Simd32x4::from([
                    (self.group9()[0] * other.group1()[0]),
                    (self.group9()[0] * other.group1()[1]),
                    (self.group9()[0] * other.group1()[2]),
                    (-(self.group9()[3] * other.group1()[2]) - (self.group9()[2] * other.group1()[1]) - (self.group9()[1] * other.group1()[0])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (self.group4() * Simd32x3::from(other.group0()[0]))
                + Simd32x3::from([
                    ((self.group9()[3] * other.group2()[1]) - (self.group9()[2] * other.group2()[2])),
                    (-(self.group9()[3] * other.group2()[0]) + (self.group9()[1] * other.group2()[2])),
                    ((self.group9()[2] * other.group2()[0]) - (self.group9()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12
            (-(Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                + (self.group5() * Simd32x3::from(other.group0()[0]))
                + (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]]))),
            // e415, e425, e435, e321
            ((Simd32x4::from(self.group0()[1]) * other.group1()) + (self.group6() * Simd32x4::from(other.group0()[0]))),
            // e423, e431, e412
            (self.group7() * Simd32x3::from(other.group0()[0])),
            // e235, e315, e125
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]])) + (self.group8() * Simd32x3::from(other.group0()[0]))),
            // e1234, e4235, e4315, e4125
            (self.group9() * Simd32x4::from(other.group0()[0])),
            // e3215
            (self[e45] * other.group0()[0]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       20       32        0
    //    simd3        1        4        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       23       38        0
    //  no simd       31       52        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e45] * other.group0()[3]) + (self.group9()[0] * other.group1()[3])
                    - (self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    - (self.group4()[0] * other.group0()[0])
                    - (self.group3()[2] * other.group1()[2])
                    - (self.group3()[0] * other.group1()[0])
                    - (self.group3()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group8()[2], self.group8()[0], self.group8()[1], self.group6()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group8()[1], self.group8()[2], self.group8()[0], self.group0()[1]]))
                + Simd32x4::from([
                    (-(self.group7()[1] * other.group1()[2]) + (self.group7()[2] * other.group1()[1])),
                    ((self.group7()[0] * other.group1()[2]) - (self.group7()[2] * other.group1()[0])),
                    (-(self.group7()[0] * other.group1()[1]) + (self.group7()[1] * other.group1()[0])),
                    (-(self.group6()[1] * other.group0()[1]) - (self.group6()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group6()[2] * other.group1()[2]) - (self.group6()[1] * other.group1()[1]) + (self.group0()[1] * other.group1()[3]) - (self.group6()[0] * other.group1()[0])),
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group9()[2] * other.group0()[2]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[3] * other.group0()[0])),
                ((self.group9()[1] * other.group0()[1]) - (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from([
                (-(self.group9()[2] * other.group1()[2]) + (self.group9()[3] * other.group1()[1])),
                ((self.group9()[1] * other.group1()[2]) - (self.group9()[3] * other.group1()[0])),
                (-(self.group9()[1] * other.group1()[1]) + (self.group9()[2] * other.group1()[0])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       21       37        0
    //    simd3        3        7        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       29       50        0
    //  no simd       50       82        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e45] * other.group1()[3])
                    - (self.group5()[2] * other.group1()[2])
                    - (self.group5()[1] * other.group1()[1])
                    - (self.group5()[0] * other.group1()[0])
                    - (self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    + (self.group0()[0] * other.group0()[3])
                    - (self.group4()[0] * other.group0()[0])),
                (self.group0()[1] * other.group0()[3]),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group8()[2], self.group8()[0], self.group8()[1], self.group6()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group8()[1], self.group8()[2], self.group8()[0], self.group1()[3]]))
                + (other.group1() * Simd32x4::from([self.group6()[3], self.group6()[3], self.group6()[3], self.group0()[1]]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group7()[2] * other.group1()[2])
                        - (self.group7()[1] * other.group1()[1])
                        - (self.group7()[0] * other.group1()[0])
                        - (self.group6()[1] * other.group0()[1])
                        - (self.group6()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group8()[2] * other.group1()[2]) - (self.group8()[1] * other.group1()[1]) + (self[e1] * other.group0()[3]) - (self.group8()[0] * other.group1()[0])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group9(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group9()[2], self.group9()[3], self.group9()[1], self.group3()[3]]))
                + Simd32x4::from([
                    ((self.group3()[0] * other.group0()[3]) + (self.group9()[0] * other.group1()[0])),
                    ((self.group3()[1] * other.group0()[3]) + (self.group9()[0] * other.group1()[1])),
                    ((self.group3()[2] * other.group0()[3]) + (self.group9()[0] * other.group1()[2])),
                    (-(self.group9()[2] * other.group1()[1]) - (self.group9()[1] * other.group1()[0])),
                ])),
            // e15, e25, e35
            ((self.group4() * Simd32x3::from(other.group0()[3])) + (Simd32x3::from(self[e45]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))),
            // e23, e31, e12
            ((self.group5() * Simd32x3::from(other.group0()[3])) + (Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[0]) + (self.group6()[0] * other.group0()[3])),
                ((self.group0()[1] * other.group1()[1]) + (self.group6()[1] * other.group0()[3])),
                ((self.group0()[1] * other.group1()[2]) + (self.group6()[2] * other.group0()[3])),
                (self.group6()[3] * other.group0()[3]),
            ]),
            // e423, e431, e412
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])) + (self.group7() * Simd32x3::from(other.group0()[3]))),
            // e235, e315, e125
            (self.group8() * Simd32x3::from(other.group0()[3])),
            // e1234, e4235, e4315, e4125
            (self.group9() * Simd32x4::from(other.group0()[3])),
            // e3215
            (self[e45] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       30       43        0
    //    simd3        2        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       34       50        0
    //  no simd       44       66        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e45] * other.group2()[3])
                    + (self.group9()[3] * other.group2()[2])
                    + (self.group9()[2] * other.group2()[1])
                    + (self.group9()[1] * other.group2()[0])
                    + (self.group9()[0] * other.group1()[3])
                    - (self.group4()[2] * other.group0()[2])
                    - (self.group4()[1] * other.group0()[1])
                    - (self.group4()[0] * other.group0()[0])
                    - (self.group3()[3] * other.group0()[3])
                    - (self.group3()[2] * other.group1()[2])
                    - (self.group3()[0] * other.group1()[0])
                    - (self.group3()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group8()[2], self.group8()[0], self.group8()[1], self.group6()[2]]))
                + (Simd32x4::from(self.group0()[1]) * other.group2())
                + Simd32x4::from([
                    ((self.group8()[1] * other.group0()[2]) + (self.group7()[2] * other.group1()[1]) - (self.group7()[1] * other.group1()[2])
                        + (self.group6()[0] * other.group0()[3])),
                    ((self.group8()[2] * other.group0()[0]) - (self.group7()[2] * other.group1()[0])
                        + (self.group7()[0] * other.group1()[2])
                        + (self.group6()[1] * other.group0()[3])),
                    ((self.group8()[0] * other.group0()[1]) + (self.group7()[1] * other.group1()[0]) - (self.group7()[0] * other.group1()[1])
                        + (self.group6()[2] * other.group0()[3])),
                    (-(self.group6()[1] * other.group0()[1]) - (self.group6()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group6()[2] * other.group1()[2]) - (self.group6()[1] * other.group1()[1]) + (self.group0()[1] * other.group1()[3]) - (self.group6()[0] * other.group1()[0])),
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group9()[2] * other.group0()[2]) - (self.group9()[3] * other.group0()[1])),
                (-(self.group9()[1] * other.group0()[2]) + (self.group9()[3] * other.group0()[0])),
                ((self.group9()[1] * other.group0()[1]) - (self.group9()[2] * other.group0()[0])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from([
                (-(self.group9()[2] * other.group1()[2]) + (self.group9()[3] * other.group1()[1])),
                ((self.group9()[1] * other.group1()[2]) - (self.group9()[3] * other.group1()[0])),
                (-(self.group9()[1] * other.group1()[1]) + (self.group9()[2] * other.group1()[0])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(self[e45]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                + (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                - (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[3])]),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<VersorOdd> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       38       53        0
    //    simd3        8       12        0
    //    simd4        7        8        0
    // Totals...
    // yes simd       53       73        0
    //  no simd       90      121        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group7()[2] * other.group2()[2])
                    - (self.group7()[1] * other.group2()[1])
                    - (self.group7()[0] * other.group2()[0])
                    - (self.group6()[3] * other.group1()[3])
                    - (self.group6()[2] * other.group1()[2])
                    - (self.group6()[1] * other.group1()[1])
                    - (self.group6()[0] * other.group1()[0])
                    + (self[e1] * other.group2()[3])
                    + (self.group1()[3] * other.group3()[3])
                    + (self.group1()[2] * other.group3()[2])
                    + (self.group1()[1] * other.group3()[1])
                    + (self.group0()[1] * other.group0()[3])
                    + (self.group1()[0] * other.group3()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(self.group9(), 0, 0, 0, 2) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[1]]))
                - (swizzle!(other.group3(), 1, 2, 0, 2) * Simd32x4::from([self.group5()[2], self.group5()[0], self.group5()[1], self.group3()[2]]))
                - (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group4()[0], self.group4()[1], self.group4()[2], self.group3()[3]]))
                + Simd32x4::from([
                    (-(self[e45] * other.group0()[0]) - (self.group9()[3] * other.group1()[1]) + (self.group5()[1] * other.group3()[2]) + (self.group3()[0] * other.group3()[3])),
                    (-(self[e45] * other.group0()[1]) - (self.group9()[1] * other.group1()[2]) + (self.group5()[2] * other.group3()[0]) + (self.group3()[1] * other.group3()[3])),
                    (-(self[e45] * other.group0()[2]) - (self.group9()[2] * other.group1()[0]) + (self.group5()[0] * other.group3()[1]) + (self.group3()[2] * other.group3()[3])),
                    ((self.group9()[1] * other.group0()[0]) + (self.group9()[0] * other.group1()[3])
                        - (self.group3()[0] * other.group3()[0])
                        - (self.group3()[1] * other.group3()[1])),
                ])),
            // e5
            (-(self[e45] * other.group1()[3]) - (self.group9()[3] * other.group2()[2]) - (self.group9()[2] * other.group2()[1]) - (self.group9()[1] * other.group2()[0])
                + (self.group4()[2] * other.group3()[2])
                + (self.group4()[1] * other.group3()[1])
                + (self.group3()[3] * other.group3()[3])
                + (self.group4()[0] * other.group3()[0])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group3(), 2, 0, 1, 2) * Simd32x4::from([self.group7()[1], self.group7()[2], self.group7()[0], self.group6()[2]]))
                + (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group7()[2] * other.group3()[1]) + (self.group6()[0] * other.group2()[3])),
                    ((self.group7()[0] * other.group3()[2]) + (self.group6()[1] * other.group2()[3])),
                    ((self.group7()[1] * other.group3()[0]) + (self.group6()[2] * other.group2()[3])),
                    (-(self.group6()[1] * other.group3()[1]) - (self.group6()[0] * other.group3()[0])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group8(), 2, 0, 1) * Simd32x3::from([other.group3()[1], other.group3()[2], other.group3()[0]]))
                + (swizzle!(self.group8(), 1, 2, 0) * Simd32x3::from([other.group3()[2], other.group3()[0], other.group3()[1]]))
                + (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]]))
                + (Simd32x3::from(other.group3()[3]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))),
            // e23, e31, e12
            ((self.group8() * Simd32x3::from(other.group2()[3]))
                + (self.group7() * Simd32x3::from(other.group3()[3]))
                + (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                - (Simd32x3::from(self.group6()[3]) * Simd32x3::from([other.group3()[0], other.group3()[1], other.group3()[2]]))),
            // e415, e425, e435, e321
            ((swizzle!(self.group9(), 3, 1, 2, 0) * swizzle!(other.group3(), 1, 2, 0, 3))
                - Simd32x4::from([
                    (self.group9()[2] * other.group3()[2]),
                    (self.group9()[3] * other.group3()[0]),
                    (self.group9()[1] * other.group3()[1]),
                    (self[e45] * other.group2()[3]),
                ])),
            // e423, e431, e412
            ((Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group3()[0], other.group3()[1], other.group3()[2]]))
                - (Simd32x3::from(other.group2()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e235, e315, e125
            ((Simd32x3::from(other.group3()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                - (Simd32x3::from(self[e45]) * Simd32x3::from([other.group3()[0], other.group3()[1], other.group3()[2]]))),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group2()[3], other.group3()[0], other.group3()[1], other.group3()[2]])),
            // e3215
            (self.group0()[1] * other.group3()[3]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       32       47        0
    //    simd3        7       11        0
    //    simd4        6        7        0
    // Totals...
    // yes simd       45       65        0
    //  no simd       77      108        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group7()[2] * other.group1()[2])
                    - (self.group7()[1] * other.group1()[1])
                    - (self.group7()[0] * other.group1()[0])
                    - (self.group6()[3] * other.group0()[3])
                    + (self[e1] * other.group1()[3])
                    + (self.group1()[3] * other.group2()[3])
                    + (self.group1()[2] * other.group2()[2])
                    + (self.group1()[0] * other.group2()[0])
                    + (self.group1()[1] * other.group2()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 0, 0, 0, 3) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[2]]))
                - (swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group5()[2], self.group5()[0], self.group5()[1], self.group3()[2]]))
                - (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group4()[0], self.group4()[1], self.group4()[2], self.group3()[3]]))
                + Simd32x4::from([
                    (-(self[e45] * other.group0()[0]) + (self.group5()[1] * other.group2()[2]) + (self.group3()[0] * other.group2()[3])),
                    (-(self[e45] * other.group0()[1]) + (self.group5()[2] * other.group2()[0]) + (self.group3()[1] * other.group2()[3])),
                    (-(self[e45] * other.group0()[2]) + (self.group5()[0] * other.group2()[1]) + (self.group3()[2] * other.group2()[3])),
                    ((self.group9()[2] * other.group0()[1]) + (self.group9()[1] * other.group0()[0]) + (self.group9()[0] * other.group0()[3])
                        - (self.group3()[0] * other.group2()[0])
                        - (self.group3()[1] * other.group2()[1])),
                ])),
            // e5
            (-(self[e45] * other.group0()[3]) - (self.group9()[3] * other.group1()[2]) - (self.group9()[2] * other.group1()[1]) - (self.group9()[1] * other.group1()[0])
                + (self.group4()[2] * other.group2()[2])
                + (self.group4()[1] * other.group2()[1])
                + (self.group3()[3] * other.group2()[3])
                + (self.group4()[0] * other.group2()[0])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group2(), 2, 0, 1, 2) * Simd32x4::from([self.group7()[1], self.group7()[2], self.group7()[0], self.group6()[2]]))
                + (Simd32x4::from(self.group0()[1]) * other.group0())
                + Simd32x4::from([
                    ((self.group7()[2] * other.group2()[1]) + (self.group6()[0] * other.group1()[3])),
                    ((self.group7()[0] * other.group2()[2]) + (self.group6()[1] * other.group1()[3])),
                    ((self.group7()[1] * other.group2()[0]) + (self.group6()[2] * other.group1()[3])),
                    (-(self.group6()[1] * other.group2()[1]) - (self.group6()[0] * other.group2()[0])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group8(), 2, 0, 1) * Simd32x3::from([other.group2()[1], other.group2()[2], other.group2()[0]]))
                + (swizzle!(self.group8(), 1, 2, 0) * Simd32x3::from([other.group2()[2], other.group2()[0], other.group2()[1]]))
                + (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (Simd32x3::from(other.group2()[3]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))),
            // e23, e31, e12
            ((self.group8() * Simd32x3::from(other.group1()[3])) - (Simd32x3::from(self.group6()[3]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]]))
                + (self.group7() * Simd32x3::from(other.group2()[3]))),
            // e415, e425, e435, e321
            ((swizzle!(self.group9(), 3, 1, 2, 0) * swizzle!(other.group2(), 1, 2, 0, 3))
                - Simd32x4::from([
                    (self.group9()[2] * other.group2()[2]),
                    (self.group9()[3] * other.group2()[0]),
                    (self.group9()[1] * other.group2()[1]),
                    (self[e45] * other.group1()[3]),
                ])),
            // e423, e431, e412
            ((Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]]))
                - (Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e235, e315, e125
            ((Simd32x3::from(other.group2()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                - (Simd32x3::from(self[e45]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]]))),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group1()[3], other.group2()[0], other.group2()[1], other.group2()[2]])),
            // e3215
            (self.group0()[1] * other.group2()[3]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       33       51        0
    //    simd3        6       10        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       42       64        0
    //  no simd       63       93        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group7()[2] * other.group0()[3])
                    - (self.group7()[1] * other.group0()[2])
                    - (self.group7()[0] * other.group0()[1])
                    - (self.group6()[3] * other.group1()[3])
                    - (self.group6()[2] * other.group1()[2])
                    - (self.group6()[1] * other.group1()[1])
                    - (self.group6()[0] * other.group1()[0])
                    + (self.group1()[3] * other.group2()[3])
                    + (self.group1()[2] * other.group2()[2])
                    + (self.group1()[1] * other.group2()[1])
                    + (self.group0()[1] * other.group0()[0])
                    + (self.group1()[0] * other.group2()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 2, 3, 1, 0) * swizzle!(other.group1(), 2, 0, 1, 3))
                - (swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group5()[2], self.group5()[0], self.group5()[1], self.group3()[2]]))
                + Simd32x4::from([
                    (-(self.group9()[3] * other.group1()[1])
                        + (self.group9()[0] * other.group0()[1])
                        + (self.group3()[0] * other.group2()[3])
                        + (self.group5()[1] * other.group2()[2])),
                    (-(self.group9()[1] * other.group1()[2])
                        + (self.group9()[0] * other.group0()[2])
                        + (self.group5()[2] * other.group2()[0])
                        + (self.group3()[1] * other.group2()[3])),
                    (-(self.group9()[2] * other.group1()[0])
                        + (self.group9()[0] * other.group0()[3])
                        + (self.group3()[2] * other.group2()[3])
                        + (self.group5()[0] * other.group2()[1])),
                    (-(self.group3()[0] * other.group2()[0]) - (self.group3()[1] * other.group2()[1])),
                ])),
            // e5
            (-(self[e45] * other.group1()[3]) - (self.group9()[3] * other.group0()[3]) - (self.group9()[2] * other.group0()[2]) - (self.group9()[1] * other.group0()[1])
                + (self.group4()[2] * other.group2()[2])
                + (self.group4()[1] * other.group2()[1])
                + (self.group3()[3] * other.group2()[3])
                + (self.group4()[0] * other.group2()[0])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group2(), 2, 0, 1, 2) * Simd32x4::from([self.group7()[1], self.group7()[2], self.group7()[0], self.group6()[2]]))
                + Simd32x4::from([
                    (self.group7()[2] * other.group2()[1]),
                    (self.group7()[0] * other.group2()[2]),
                    (self.group7()[1] * other.group2()[0]),
                    (-(self.group6()[1] * other.group2()[1]) + (self.group0()[1] * other.group1()[3]) - (self.group6()[0] * other.group2()[0])),
                ])),
            // e15, e25, e35
            (-(swizzle!(self.group8(), 2, 0, 1) * Simd32x3::from([other.group2()[1], other.group2()[2], other.group2()[0]]))
                + (swizzle!(self.group8(), 1, 2, 0) * Simd32x3::from([other.group2()[2], other.group2()[0], other.group2()[1]]))
                + (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[3]]))
                + (Simd32x3::from(other.group2()[3]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))),
            // e23, e31, e12
            ((self.group7() * Simd32x3::from(other.group2()[3])) + (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                - (Simd32x3::from(self.group6()[3]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group9()[2] * other.group2()[2]) + (self.group9()[3] * other.group2()[1])),
                ((self.group9()[1] * other.group2()[2]) - (self.group9()[3] * other.group2()[0])),
                (-(self.group9()[1] * other.group2()[1]) + (self.group9()[2] * other.group2()[0])),
                (self.group9()[0] * other.group2()[3]),
            ]),
            // e423, e431, e412
            (Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]])),
            // e235, e315, e125
            ((Simd32x3::from(other.group2()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))
                - (Simd32x3::from(self[e45]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]]))),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[1] * other.group2()[0]),
                (self.group0()[1] * other.group2()[1]),
                (self.group0()[1] * other.group2()[2]),
            ]),
            // e3215
            (self.group0()[1] * other.group2()[3]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       22        0
    //    simd3        2        7        0
    //    simd4        3        5        0
    // Totals...
    // yes simd       19       34        0
    //  no simd       32       63        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group7()[2] * other.group1()[2])
                    - (self.group7()[1] * other.group1()[1])
                    - (self.group7()[0] * other.group1()[0])
                    + (self.group1()[3] * other.group0()[3])
                    + (self[e1] * other.group1()[3])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 0, 0, 0, 3) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[2]]))
                + (swizzle!(other.group0(), 3, 3, 3, 1) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group9()[2]]))
                - (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group4()[0], self.group4()[1], self.group4()[2], self.group3()[3]]))
                + (Simd32x4::from([-1.0, -1.0, -1.0, 1.0]) * swizzle!(other.group0(), 0, 1, 2, 0) * Simd32x4::from([self[e45], self[e45], self[e45], self.group9()[1]]))),
            // e5
            (-(self.group9()[3] * other.group1()[2]) - (self.group9()[2] * other.group1()[1]) + (self.group3()[3] * other.group0()[3]) - (self.group9()[1] * other.group1()[0])),
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[0]) + (self.group6()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group0()[1]) + (self.group6()[1] * other.group1()[3])),
                ((self.group0()[1] * other.group0()[2]) + (self.group6()[2] * other.group1()[3])),
                0.0,
            ]),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))),
            // e23, e31, e12
            ((self.group7() * Simd32x3::from(other.group0()[3])) + (self.group8() * Simd32x3::from(other.group1()[3]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, ((self.group9()[0] * other.group0()[3]) - (self[e45] * other.group1()[3]))]),
            // e423, e431, e412
            (Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group0()[1] * other.group1()[3]), 0.0, 0.0, 0.0]),
            // e3215
            (self.group0()[1] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       25       40        0
    //    simd3        3        8        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       32       53        0
    //  no simd       50       84        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group6()[3] * other.group0()[3])
                    + (self[e1] * other.group1()[0])
                    + (self.group1()[2] * other.group1()[3])
                    + (self.group1()[0] * other.group1()[1])
                    + (self.group1()[1] * other.group1()[2])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group1(), 2, 0, 1, 0) * Simd32x4::from([self.group5()[2], self.group4()[1], self.group5()[1], self.group3()[3]]))
                - (swizzle!(other.group1(), 0, 3, 0, 3) * Simd32x4::from([self.group4()[0], self.group5()[0], self.group4()[2], self.group3()[2]]))
                + Simd32x4::from([
                    (-(self[e45] * other.group0()[0]) + (self.group5()[1] * other.group1()[3])),
                    (-(self[e45] * other.group0()[1]) + (self.group5()[2] * other.group1()[1])),
                    (-(self[e45] * other.group0()[2]) + (self.group5()[0] * other.group1()[2])),
                    ((self.group9()[3] * other.group0()[2])
                        + (self.group9()[2] * other.group0()[1])
                        + (self.group9()[1] * other.group0()[0])
                        + (self.group9()[0] * other.group0()[3])
                        - (self.group3()[0] * other.group1()[1])
                        - (self.group3()[1] * other.group1()[2])),
                ])),
            // e5
            (-(self[e45] * other.group0()[3]) + (self.group4()[2] * other.group1()[3]) + (self.group4()[0] * other.group1()[1]) + (self.group4()[1] * other.group1()[2])),
            // e41, e42, e43, e45
            (-(swizzle!(other.group1(), 3, 1, 2, 3) * Simd32x4::from([self.group7()[1], self.group7()[2], self.group7()[0], self.group6()[2]]))
                + (Simd32x4::from(self.group0()[1]) * other.group0())
                + Simd32x4::from([
                    ((self.group7()[2] * other.group1()[2]) + (self.group6()[0] * other.group1()[0])),
                    ((self.group7()[0] * other.group1()[3]) + (self.group6()[1] * other.group1()[0])),
                    ((self.group7()[1] * other.group1()[1]) + (self.group6()[2] * other.group1()[0])),
                    (-(self.group6()[1] * other.group1()[2]) - (self.group6()[0] * other.group1()[1])),
                ])),
            // e15, e25, e35
            ((swizzle!(self.group8(), 1, 2, 0) * Simd32x3::from([other.group1()[3], other.group1()[1], other.group1()[2]]))
                - (swizzle!(self.group8(), 2, 0, 1) * Simd32x3::from([other.group1()[2], other.group1()[3], other.group1()[1]]))),
            // e23, e31, e12
            (-(Simd32x3::from(self.group6()[3]) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[3]])) + (self.group8() * Simd32x3::from(other.group1()[0]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group9()[2] * other.group1()[3]) + (self.group9()[3] * other.group1()[2])),
                ((self.group9()[1] * other.group1()[3]) - (self.group9()[3] * other.group1()[1])),
                (-(self.group9()[1] * other.group1()[2]) + (self.group9()[2] * other.group1()[1])),
                (self[e45] * other.group1()[0] * -1.0),
            ]),
            // e423, e431, e412
            ((Simd32x3::from(self.group9()[0]) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[3]]))
                - (Simd32x3::from(other.group1()[0]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]))),
            // e235, e315, e125
            (Simd32x3::from(self[e45]) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[3]]) * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for MultiVector {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       24       36        0
    //    simd3        3        8        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       30       47        0
    //  no simd       45       72        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group8()[2] * other.group0()[2])
                    - (self.group8()[1] * other.group0()[1])
                    - (self.group8()[0] * other.group0()[0])
                    - (self.group7()[2] * other.group2()[2])
                    - (self.group7()[1] * other.group2()[1])
                    - (self.group7()[0] * other.group2()[0])
                    - (self.group6()[2] * other.group1()[2])
                    - (self.group6()[1] * other.group1()[1])
                    - (self.group6()[0] * other.group1()[0])
                    + (self[e1] * other.group2()[3])
                    + (self.group0()[1] * other.group0()[3])
                    + (self.group1()[3] * other.group1()[3])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group9(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(self.group9(), 0, 0, 0, 2) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[1]]))
                - (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group4()[0], self.group4()[1], self.group4()[2], self.group3()[3]]))
                + Simd32x4::from([
                    (-(self[e45] * other.group0()[0]) - (self.group9()[3] * other.group1()[1]) + (self.group3()[0] * other.group1()[3])),
                    (-(self[e45] * other.group0()[1]) - (self.group9()[1] * other.group1()[2]) + (self.group3()[1] * other.group1()[3])),
                    (-(self[e45] * other.group0()[2]) - (self.group9()[2] * other.group1()[0]) + (self.group3()[2] * other.group1()[3])),
                    (self.group9()[1] * other.group0()[0]),
                ])),
            // e5
            (-(self.group9()[3] * other.group2()[2]) - (self.group9()[2] * other.group2()[1]) + (self.group3()[3] * other.group1()[3]) - (self.group9()[1] * other.group2()[0])),
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[0]) + (self.group6()[0] * other.group2()[3])),
                ((self.group0()[1] * other.group0()[1]) + (self.group6()[1] * other.group2()[3])),
                ((self.group0()[1] * other.group0()[2]) + (self.group6()[2] * other.group2()[3])),
                0.0,
            ]),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group2()[0], other.group2()[1], other.group2()[2]]))
                + (Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group6()[0], self.group6()[1], self.group6()[2]]))),
            // e23, e31, e12
            ((self.group8() * Simd32x3::from(other.group2()[3]))
                + (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (self.group7() * Simd32x3::from(other.group1()[3]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, ((self.group9()[0] * other.group1()[3]) - (self[e45] * other.group2()[3]))]),
            // e423, e431, e412
            (Simd32x3::from(other.group2()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            (Simd32x3::from(other.group1()[3]) * Simd32x3::from([self.group9()[1], self.group9()[2], self.group9()[3]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group0()[1] * other.group2()[3]), 0.0, 0.0, 0.0]),
            // e3215
            (self.group0()[1] * other.group1()[3]),
        );
    }
}
impl InfixAntiWedge for NullCircleAtOrigin {}
impl AntiWedge<AntiDualNum> for NullCircleAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatPoint> for NullCircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<AntiFlector> for NullCircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<AntiLine> for NullCircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for NullCircleAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for NullCircleAtOrigin {
    type Output = NullCircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return NullCircleAtOrigin::from_groups(/* e423, e431, e412 */ (self.group0() * Simd32x3::from(other[e12345])));
    }
}
impl AntiWedge<Circle> for NullCircleAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
            ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
            (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleAligningOrigin> for NullCircleAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
            ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
            (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleAtInfinity> for NullCircleAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<CircleAtOrigin> for NullCircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group1(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group1(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for NullCircleAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for NullCircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group1(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group1(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Dipole> for NullCircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for NullCircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for NullCircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for NullCircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for NullCircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for NullCircleAtOrigin {
    type Output = NullCircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return NullCircleAtOrigin::from_groups(/* e423, e431, e412 */ (self.group0() * Simd32x3::from(other.group0()[1])));
    }
}
impl AntiWedge<FlatPoint> for NullCircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for NullCircleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for NullCircleAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for NullCircleAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for NullCircleAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[3], other.group0()[1], other.group0()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[2], other.group0()[3], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<Horizon> for NullCircleAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group0() * Simd32x3::from(other[e3215])));
    }
}
impl AntiWedge<Line> for NullCircleAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineAtInfinity> for NullCircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for NullCircleAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for NullCircleAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for NullCircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for NullCircleAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MultiVector> for NullCircleAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       18        0
    //    simd3        0        2        0
    // Totals...
    // yes simd       10       20        0
    //  no simd       10       24        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group0()[2] * other.group4()[2]) - (self.group0()[0] * other.group4()[0]) - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group8()[2]) + (self.group0()[2] * other.group8()[1])),
                ((self.group0()[0] * other.group8()[2]) - (self.group0()[2] * other.group8()[0])),
                (-(self.group0()[0] * other.group8()[1]) + (self.group0()[1] * other.group8()[0])),
                (-(self.group0()[2] * other.group6()[2]) - (self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other[e45])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[1])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for NullCircleAtOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        3        0
    // no simd        3        9        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for NullCircleAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Sphere> for NullCircleAtOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        3        0
    // no simd        3        9        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for NullCircleAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<SphereOnOrigin> for NullCircleAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEven> for NullCircleAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for NullCircleAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for NullCircleAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for NullCircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group1()[2], other.group1()[0], other.group1()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[0]]))),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for NullCircleAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for NullCircleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group1()[2], other.group1()[0], other.group1()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[0]]))),
        );
    }
}
impl AntiWedge<VersorOdd> for NullCircleAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group3()[2]) + (self.group0()[2] * other.group3()[1])),
                ((self.group0()[0] * other.group3()[2]) - (self.group0()[2] * other.group3()[0])),
                (-(self.group0()[0] * other.group3()[1]) + (self.group0()[1] * other.group3()[0])),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group3()[3]),
                (self.group0()[1] * other.group3()[3]),
                (self.group0()[2] * other.group3()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for NullCircleAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for NullCircleAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for NullCircleAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for NullCircleAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group1()[3], other.group1()[1], other.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group1()[2], other.group1()[3], other.group1()[1]]))),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for NullCircleAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        ]));
    }
}
impl InfixAntiWedge for NullDipoleAtOrigin {}
impl AntiWedge<AntiDualNum> for NullDipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatPoint> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiFlector> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for NullDipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group1()[3])));
    }
}
impl AntiWedge<AntiScalar> for NullDipoleAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (self.group0() * Simd32x3::from(other[e12345])));
    }
}
impl AntiWedge<Circle> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DualNum> for NullDipoleAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (self.group0() * Simd32x3::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for NullDipoleAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<FlectorAtInfinity> for NullDipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<FlectorOnOrigin> for NullDipoleAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Horizon> for NullDipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other[e3215])));
    }
}
impl AntiWedge<Line> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for NullDipoleAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for NullDipoleAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<MultiVector> for NullDipoleAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                (-(self.group0()[2] * other.group8()[2]) - (self.group0()[0] * other.group8()[0]) - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other[e45]),
                (self.group0()[1] * other[e45]),
                (self.group0()[2] * other[e45]),
                (-(self.group0()[2] * other.group9()[3]) - (self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
            ]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for NullDipoleAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<PlaneOnOrigin> for NullDipoleAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for NullDipoleAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<SphereAtOrigin> for NullDipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<SphereOnOrigin> for NullDipoleAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for NullDipoleAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for NullDipoleAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for NullDipoleAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for NullDipoleAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for NullDipoleAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<VersorOdd> for NullDipoleAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group3()[3]),
            (self.group0()[1] * other.group3()[3]),
            (self.group0()[2] * other.group3()[3]),
            (-(self.group0()[2] * other.group3()[2]) - (self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddAligningOrigin> for NullDipoleAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group2()[3]),
            (self.group0()[1] * other.group2()[3]),
            (self.group0()[2] * other.group2()[3]),
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddAtInfinity> for NullDipoleAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group2()[3]),
            (self.group0()[1] * other.group2()[3]),
            (self.group0()[2] * other.group2()[3]),
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddAtOrigin> for NullDipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorOddOnOrigin> for NullDipoleAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group1()[3]) - (self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for NullDipoleAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (self.group0() * Simd32x3::from(other.group1()[3])));
    }
}
impl InfixAntiWedge for NullSphereAtOrigin {}
impl AntiWedge<AntiDualNum> for NullSphereAtOrigin {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self[e1234] * other.group0()[0]));
    }
}
impl AntiWedge<AntiFlatPoint> for NullSphereAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e1234]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiFlector> for NullSphereAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]])),
        );
    }
}
impl AntiWedge<AntiLine> for NullSphereAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e1234]) * other.group1()));
    }
}
impl AntiWedge<AntiMotor> for NullSphereAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (Simd32x4::from(self[e1234]) * swizzle!(other.group1(), 3, 0, 1, 2)));
    }
}
impl AntiWedge<AntiPlane> for NullSphereAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e1234] * other.group0()[3]));
    }
}
impl AntiWedge<AntiScalar> for NullSphereAtOrigin {
    type Output = NullSphereAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return NullSphereAtOrigin::from_groups(/* e1234 */ (self[e1234] * other[e12345]));
    }
}
impl AntiWedge<Circle> for NullSphereAtOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        use crate::elements::*;
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self[e1234]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
            // e23, e31, e12
            (Simd32x3::from(self[e1234]) * other.group2()),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for NullSphereAtOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self[e1234]) * other.group1()),
            // e23, e31, e12
            (Simd32x3::from(self[e1234]) * other.group2()),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for NullSphereAtOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self[e1234]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e23, e31, e12
            (Simd32x3::from(self[e1234]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for NullSphereAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self[e1234]) * other.group1()));
    }
}
impl AntiWedge<CircleOnOrigin> for NullSphereAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (Simd32x3::from(self[e1234]) * other.group1()));
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for NullSphereAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self[e1234]) * other.group1()));
    }
}
impl AntiWedge<Dipole> for NullSphereAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        use crate::elements::*;
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for NullSphereAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for NullSphereAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for NullSphereAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e1234]) * other.group1()));
    }
}
impl AntiWedge<DipoleOnOrigin> for NullSphereAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e1234] * other.group0()[3]));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for NullSphereAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e1234]) * other.group2()));
    }
}
impl AntiWedge<DualNum> for NullSphereAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self[e1234] * other.group0()[0])]),
            // e23, e31, e12, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self[e1234] * other.group0()[1])]),
        );
    }
}
impl AntiWedge<FlatOrigin> for NullSphereAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e1234] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for NullSphereAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        use crate::elements::*;
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self[e1234]) * other.group0()));
    }
}
impl AntiWedge<FlatPointAtInfinity> for NullSphereAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self[e1234]) * other.group0()));
    }
}
impl AntiWedge<Flector> for NullSphereAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        use crate::elements::*;
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self[e1234]) * other.group1()),
            // e4, e1, e2, e3
            (Simd32x4::from(self[e1234]) * swizzle!(other.group0(), 3, 0, 1, 2)),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for NullSphereAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (Simd32x4::from(self[e1234]) * swizzle!(other.group0(), 3, 0, 1, 2)));
    }
}
impl AntiWedge<FlectorOnOrigin> for NullSphereAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ (Simd32x4::from(self[e1234]) * swizzle!(other.group0(), 1, 2, 3, 0)));
    }
}
impl AntiWedge<Horizon> for NullSphereAtOrigin {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self[e1234] * other[e3215]));
    }
}
impl AntiWedge<Infinity> for NullSphereAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e1234] * other[e5]));
    }
}
impl AntiWedge<Line> for NullSphereAtOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        use crate::elements::*;
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self[e1234]) * other.group0()),
            // e23, e31, e12
            (Simd32x3::from(self[e1234]) * other.group1()),
        );
    }
}
impl AntiWedge<LineAtInfinity> for NullSphereAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self[e1234]) * other.group0()));
    }
}
impl AntiWedge<LineOnOrigin> for NullSphereAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        use crate::elements::*;
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (Simd32x3::from(self[e1234]) * other.group0()));
    }
}
impl AntiWedge<Motor> for NullSphereAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]])),
            // e23, e31, e12, e1234
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for NullSphereAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self[e1234]) * other.group0()));
    }
}
impl AntiWedge<MotorOnOrigin> for NullSphereAtOrigin {
    type Output = NullVersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return NullVersorOddAtOrigin::from_groups(/* e41, e42, e43, e1234 */ (Simd32x4::from(self[e1234]) * other.group0()));
    }
}
impl AntiWedge<MultiVector> for NullSphereAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        6        0
    //    simd3        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        9        0
    //  no simd        0       16        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self[e1234] * other[e1]), 0.0]),
            // e1, e2, e3, e4
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group4()[0], other.group4()[1], other.group4()[2], other.group3()[3]])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([(self[e1234] * other.group6()[0]), (self[e1234] * other.group6()[1]), (self[e1234] * other.group6()[2]), 0.0]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(self[e1234]) * other.group8()),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self[e1234] * other[e45])]),
            // e423, e431, e412
            (Simd32x3::from(self[e1234]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self[e1234] * other.group0()[1]), 0.0, 0.0, 0.0]),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for NullSphereAtOrigin {
    type Output = AntiDipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        use crate::elements::*;
        return AntiDipoleOnOrigin::from_groups(/* e423, e431, e412, e321 */ (Simd32x4::from(self[e1234]) * other.group0()));
    }
}
impl AntiWedge<PlaneOnOrigin> for NullSphereAtOrigin {
    type Output = NullCircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        use crate::elements::*;
        return NullCircleAtOrigin::from_groups(/* e423, e431, e412 */ (Simd32x3::from(self[e1234]) * other.group0()));
    }
}
impl AntiWedge<RoundPoint> for NullSphereAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e1234] * other[e2]));
    }
}
impl AntiWedge<RoundPointAtOrigin> for NullSphereAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e1234] * other.group0()[1]));
    }
}
impl AntiWedge<Sphere> for NullSphereAtOrigin {
    type Output = AntiDipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return AntiDipoleOnOrigin::from_groups(/* e423, e431, e412, e321 */ (Simd32x4::from(self[e1234]) * other.group0()));
    }
}
impl AntiWedge<SphereAtOrigin> for NullSphereAtOrigin {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self[e1234] * other.group0()[0]));
    }
}
impl AntiWedge<SphereOnOrigin> for NullSphereAtOrigin {
    type Output = NullCircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return NullCircleAtOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self[e1234]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<VersorEven> for NullSphereAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]])),
            // e23, e31, e12, e1234
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for NullSphereAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]])),
            // e23, e31, e12, e1234
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for NullSphereAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]])),
            // e23, e31, e12, e1234
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[0]])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for NullSphereAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self[e1234]) * other.group1()));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for NullSphereAtOrigin {
    type Output = NullVersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return NullVersorOddAtOrigin::from_groups(
            // e41, e42, e43, e1234
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for NullSphereAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self[e1234]) * other.group1()));
    }
}
impl AntiWedge<VersorOdd> for NullSphereAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        use crate::elements::*;
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self[e1234]) * other.group3()),
            // e4, e1, e2, e3
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[3], other.group2()[0], other.group2()[1], other.group2()[2]])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for NullSphereAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self[e1234]) * other.group2()),
            // e4, e1, e2, e3
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for NullSphereAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self[e1234]) * other.group2()),
            // e4, e1, e2, e3
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[3], other.group0()[1], other.group0()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for NullSphereAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for NullSphereAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return NullVersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[3], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for NullSphereAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self[e1234]) * Simd32x4::from([other.group1()[3], other.group2()[0], other.group2()[1], other.group2()[2]])),
        );
    }
}
impl InfixAntiWedge for NullVersorEvenAtOrigin {}
impl AntiWedge<AntiDualNum> for NullVersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group0() * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatPoint> for NullVersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for NullVersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiLine> for NullVersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for NullVersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for NullVersorEvenAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<Circle> for NullVersorEvenAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group2()[1]),
                    (self.group0()[0] * other.group2()[2]),
                    (self.group0()[1] * other.group2()[0]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for NullVersorEvenAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group2()[1]),
                    (self.group0()[0] * other.group2()[2]),
                    (self.group0()[1] * other.group2()[0]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for NullVersorEvenAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[1]),
                    (self.group0()[0] * other.group1()[2]),
                    (self.group0()[1] * other.group1()[0]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for NullVersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for NullVersorEvenAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for NullVersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Dipole> for NullVersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for NullVersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for NullVersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for NullVersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for NullVersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for NullVersorEvenAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<FlatPoint> for NullVersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for NullVersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for NullVersorEvenAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group1(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for NullVersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for NullVersorEvenAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[3]) + (self.group0()[2] * other.group0()[2])),
            ((self.group0()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Horizon> for NullVersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group0() * Simd32x4::from(other[e3215])));
    }
}
impl AntiWedge<Line> for NullVersorEvenAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[1]),
                    (self.group0()[0] * other.group1()[2]),
                    (self.group0()[1] * other.group1()[0]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for NullVersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for NullVersorEvenAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for NullVersorEvenAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 2, 0, 1) * Simd32x4::from([other.group0()[3], other.group1()[1], other.group1()[2], other.group1()[0]]))
                - (swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                + Simd32x4::from([(-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for NullVersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for NullVersorEvenAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MultiVector> for NullVersorEvenAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd3        0        2        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       16        0
    //  no simd       15       26        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other[e45]) - (self.group0()[2] * other.group4()[2]) - (self.group0()[0] * other.group4()[0]) - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group8()[2], other.group8()[0], other.group8()[1], other.group6()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * Simd32x4::from([other.group8()[1], other.group8()[2], other.group8()[0], other.group0()[1]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1]))])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for NullVersorEvenAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for NullVersorEvenAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Sphere> for NullVersorEvenAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for NullVersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group0() * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<SphereOnOrigin> for NullVersorEvenAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<VersorEven> for NullVersorEvenAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 2, 0, 1) * Simd32x4::from([other.group0()[3], other.group2()[1], other.group2()[2], other.group2()[0]]))
                - (swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group1()[2], other.group2()[2], other.group2()[0], other.group2()[1]]))
                + Simd32x4::from([(-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for NullVersorEvenAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 2, 0, 1) * Simd32x4::from([other.group0()[3], other.group2()[1], other.group2()[2], other.group2()[0]]))
                - (swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group1()[2], other.group2()[2], other.group2()[0], other.group2()[1]]))
                + Simd32x4::from([(-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for NullVersorEvenAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 2, 0, 1) * Simd32x4::from([other.group0()[0], other.group2()[1], other.group2()[2], other.group2()[0]]))
                - (swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group1()[2], other.group2()[2], other.group2()[0], other.group2()[1]]))
                + Simd32x4::from([(-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for NullVersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for NullVersorEvenAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for NullVersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for NullVersorEvenAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group2()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group3(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group3()[3]),
                (self.group0()[1] * other.group3()[3]),
                (self.group0()[2] * other.group3()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for NullVersorEvenAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group2(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for NullVersorEvenAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group0()[3]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group2(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for NullVersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for NullVersorEvenAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ Simd32x3::from([
            (-(self.group0()[1] * other.group1()[3]) + (self.group0()[2] * other.group1()[2])),
            ((self.group0()[0] * other.group1()[3]) - (self.group0()[2] * other.group1()[1])),
            (-(self.group0()[0] * other.group1()[2]) + (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for NullVersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        ]));
    }
}
impl InfixAntiWedge for NullVersorOddAtOrigin {}
impl AntiWedge<AntiDualNum> for NullVersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatPoint> for NullVersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for NullVersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiLine> for NullVersorOddAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group1()));
    }
}
impl AntiWedge<AntiMotor> for NullVersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group1()[3]),
            ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group1()[0])),
            ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group1()[1])),
            ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group1()[2])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for NullVersorOddAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<AntiScalar> for NullVersorOddAtOrigin {
    type Output = NullVersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return NullVersorOddAtOrigin::from_groups(/* e41, e42, e43, e1234 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<Circle> for NullVersorOddAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for NullVersorOddAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for NullVersorOddAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for NullVersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group1()[0]),
            (self.group0()[3] * other.group1()[1]),
            (self.group0()[3] * other.group1()[2]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleOnOrigin> for NullVersorOddAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (Simd32x3::from(self.group0()[3]) * other.group1()));
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for NullVersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group1()[0]),
            (self.group0()[3] * other.group1()[1]),
            (self.group0()[3] * other.group1()[2]),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<Dipole> for NullVersorOddAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for NullVersorOddAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for NullVersorOddAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for NullVersorOddAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group1()));
    }
}
impl AntiWedge<DipoleOnOrigin> for NullVersorOddAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for NullVersorOddAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group2()));
    }
}
impl AntiWedge<DualNum> for NullVersorOddAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (self.group0() * Simd32x4::from([other.group0()[1], other.group0()[1], other.group0()[1], other.group0()[0]])),
            // e23, e31, e12, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
        );
    }
}
impl AntiWedge<FlatOrigin> for NullVersorOddAtOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for NullVersorOddAtOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<FlatPointAtInfinity> for NullVersorOddAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Flector> for NullVersorOddAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from([other.group0()[3], other.group1()[3], other.group1()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for NullVersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[3]),
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for NullVersorOddAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (self.group0()[3] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<Horizon> for NullVersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other[e3215])));
    }
}
impl AntiWedge<Infinity> for NullVersorOddAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for NullVersorOddAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<LineAtInfinity> for NullVersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for NullVersorOddAtOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Motor> for NullVersorOddAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e1234
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for NullVersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for NullVersorOddAtOrigin {
    type Output = NullVersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return NullVersorOddAtOrigin::from_groups(/* e41, e42, e43, e1234 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<MultiVector> for NullVersorOddAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       18        0
    //    simd3        0        2        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       21        0
    //  no simd       12       28        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other[e1]) - (self.group0()[2] * other.group8()[2]) - (self.group0()[0] * other.group8()[0]) - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((self.group0() * Simd32x4::from([other[e45], other[e45], other[e45], other.group3()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group4()[0]),
                    (self.group0()[3] * other.group4()[1]),
                    (self.group0()[3] * other.group4()[2]),
                    (-(self.group0()[2] * other.group9()[3]) - (self.group0()[0] * other.group9()[1]) - (self.group0()[1] * other.group9()[2])),
                ])),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[1]) + (self.group0()[3] * other.group6()[0])),
                ((self.group0()[1] * other.group0()[1]) + (self.group0()[3] * other.group6()[1])),
                ((self.group0()[2] * other.group0()[1]) + (self.group0()[3] * other.group6()[2])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group8()),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other[e45])]),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]])),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group0()[3] * other.group0()[1]), 0.0, 0.0, 0.0]),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for NullVersorOddAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for NullVersorOddAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<RoundPoint> for NullVersorOddAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e2]));
    }
}
impl AntiWedge<RoundPointAtOrigin> for NullVersorOddAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[1]));
    }
}
impl AntiWedge<Sphere> for NullVersorOddAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for NullVersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<SphereOnOrigin> for NullVersorOddAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<VersorEven> for NullVersorOddAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group2()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e1234
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for NullVersorOddAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group2()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e1234
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for NullVersorOddAtOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from([other.group0()[0], other.group0()[0], other.group0()[0], other.group2()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e1234
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[0]])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for NullVersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group1()[0]),
            (self.group0()[3] * other.group1()[1]),
            (self.group0()[3] * other.group1()[2]),
            ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for NullVersorOddAtOrigin {
    type Output = NullVersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return NullVersorOddAtOrigin::from_groups(/* e41, e42, e43, e1234 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for NullVersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group1()[0]),
            (self.group0()[3] * other.group1()[1]),
            (self.group0()[3] * other.group1()[2]),
            ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for NullVersorOddAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group3()),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from([other.group1()[3], other.group3()[3], other.group3()[3], other.group3()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[2] * other.group3()[2]) - (self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                    (self.group0()[3] * other.group2()[0]),
                    (self.group0()[3] * other.group2()[1]),
                    (self.group0()[3] * other.group2()[2]),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for NullVersorOddAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group2()),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from([other.group0()[3], other.group2()[3], other.group2()[3], other.group2()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for NullVersorOddAtOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group2()),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from([other.group1()[3], other.group2()[3], other.group2()[3], other.group2()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (self.group0()[3] * other.group0()[3]),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for NullVersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[3]),
            ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
        ]));
    }
}
impl AntiWedge<VersorOddOnOrigin> for NullVersorOddAtOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group1()[1]),
            (self.group0()[3] * other.group1()[2]),
            (self.group0()[3] * other.group1()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[3]) - (self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[2])),
        ]));
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for NullVersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group1()[3]),
            ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group2()[0])),
            ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group2()[1])),
            ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group2()[2])),
        ]));
    }
}
impl InfixAntiWedge for Origin {}
impl AntiWedge<AntiDualNum> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group0()[0]));
    }
}
impl AntiWedge<AntiMotor> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group1()[3]));
    }
}
impl AntiWedge<AntiScalar> for Origin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e4] * other[e12345]));
    }
}
impl AntiWedge<DualNum> for Origin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e4] * other.group0()[1]));
    }
}
impl AntiWedge<Flector> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group1()[3]));
    }
}
impl AntiWedge<FlectorAtInfinity> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group0()[3]));
    }
}
impl AntiWedge<Horizon> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other[e3215]));
    }
}
impl AntiWedge<Motor> for Origin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e4] * other.group0()[3]));
    }
}
impl AntiWedge<MotorOnOrigin> for Origin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e4] * other.group0()[3]));
    }
}
impl AntiWedge<MultiVector> for Origin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([(self[e4] * other[e45]), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([0.0, 0.0, 0.0, (self[e4] * other.group0()[1])]),
            // e5
            0.0,
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<Plane> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group0()[3]));
    }
}
impl AntiWedge<Sphere> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group0()[3]));
    }
}
impl AntiWedge<SphereAtOrigin> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group0()[0]));
    }
}
impl AntiWedge<VersorEven> for Origin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e4] * other.group0()[3]));
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Origin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e4] * other.group0()[3]));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Origin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e4] * other.group0()[0]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Origin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self[e4] * other.group0()[3]));
    }
}
impl AntiWedge<VersorOdd> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group3()[3]));
    }
}
impl AntiWedge<VersorOddAligningOrigin> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group2()[3]));
    }
}
impl AntiWedge<VersorOddAtInfinity> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group2()[3]));
    }
}
impl AntiWedge<VersorOddAtOrigin> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group0()[3]));
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for Origin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4] * other.group1()[3]));
    }
}
impl InfixAntiWedge for Plane {}
impl AntiWedge<AntiCircleOnOrigin> for Plane {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    (-(self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for Plane {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        4        8        0
    //  no simd        6       12        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            (-(Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
        );
    }
}
impl AntiWedge<AntiDualNum> for Plane {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return LineAtInfinity::from_groups(
            // e235, e315, e125
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for Plane {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other[e321]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for Plane {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       12        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]) * Simd32x3::from(-1.0)),
            // e15, e25, e35
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for Plane {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3] * -1.0),
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                ((self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for Plane {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0] * -1.0),
            (self.group0()[1] * other.group0()[0] * -1.0),
            (self.group0()[2] * other.group0()[0] * -1.0),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<AntiLine> for Plane {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[1] * other.group0()[2]),
                    (self.group0()[2] * other.group0()[0]),
                    (self.group0()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for Plane {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<AntiMotor> for Plane {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        9        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[1] * other.group0()[2]),
                    (self.group0()[2] * other.group0()[0]),
                    (self.group0()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for Plane {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for Plane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for Plane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for Plane {
    type Output = Plane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Plane::from_groups(/* e4235, e4315, e4125, e3215 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for Plane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for Plane {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        5       11        0
    //  no simd        8       20        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                    (-(self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    (-(self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for Plane {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       16        0
    //  no simd        9       19        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[0]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group0()[2] * other.group1()[3]) + (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (-(self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                (-(self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Circle> for Plane {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        3        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5       11        0
    //  no simd       14       24        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                - (swizzle!(other.group2(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group2(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for Plane {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        3        5        0
    // Totals...
    // yes simd        5       11        0
    //  no simd       11       21        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * other.group1()) - (swizzle!(other.group2(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group2(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for Plane {
    type Output = DipoleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd3        2        3        0
    // Totals...
    // yes simd        4       12        0
    //  no simd        8       18        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return DipoleAtInfinity::from_groups(
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3] * -1.0),
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                - (swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for Plane {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        5        0
    // no simd        6       15        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group0()),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for Plane {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        1        3        0
    // Totals...
    // yes simd        3        9        0
    //  no simd        5       15        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for Plane {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        2        4        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        9       18        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            (-(Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Dipole> for Plane {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       13        0
    //  no simd       11       16        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    (-(self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for Plane {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       13        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for Plane {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        6       10        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group0(), 1, 2, 0, 3))
                + Simd32x4::from([
                    (self.group0()[1] * other.group0()[2]),
                    (self.group0()[2] * other.group0()[0]),
                    (self.group0()[0] * other.group0()[1]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for Plane {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for Plane {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       11        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (self.group0()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for Plane {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    (-(self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for Plane {
    type Output = Plane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return Plane::from_groups(/* e4235, e4315, e4125, e3215 */ (self.group0() * Simd32x4::from(other.group0()[1])));
    }
}
impl AntiWedge<FlatOrigin> for Plane {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for Plane {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for Plane {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for Plane {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group1()[3]),
                    (self.group0()[1] * other.group1()[3]),
                    (self.group0()[2] * other.group1()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for Plane {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for Plane {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       14        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[2] * other.group0()[2])),
                ((self.group0()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group0()[1])),
                0.0,
            ]),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[3]) * swizzle!(other.group0(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Horizon> for Plane {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return LineAtInfinity::from_groups(
            // e235, e315, e125
            (Simd32x3::from(other[e3215]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<Line> for Plane {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return FlatPoint::from_groups(
            // e15, e25, e35, e45
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for Plane {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for Plane {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Motor> for Plane {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for Plane {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for Plane {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for Plane {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       33        0
    //    simd3        4        9        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       20       44        0
    //  no simd       34       68        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other.group1()[3]) + (self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group5()[2], other.group5()[0], other.group5()[1], other.group3()[2]]))
                + Simd32x4::from([
                    (-(self.group0()[3] * other.group3()[0]) - (self.group0()[2] * other.group5()[1])),
                    (-(self.group0()[3] * other.group3()[1]) - (self.group0()[0] * other.group5()[2])),
                    (-(self.group0()[3] * other.group3()[2]) - (self.group0()[1] * other.group5()[0])),
                    ((self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
                ])),
            // e5
            (-(self.group0()[3] * other.group3()[3]) - (self.group0()[2] * other.group4()[2]) - (self.group0()[0] * other.group4()[0]) - (self.group0()[1] * other.group4()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group7()[1], other.group7()[2], other.group7()[0], other.group6()[2]]))
                + Simd32x4::from([
                    (self.group0()[1] * other.group7()[2]),
                    (self.group0()[2] * other.group7()[0]),
                    (self.group0()[0] * other.group7()[1]),
                    (-(self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))
                - (swizzle!(other.group8(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group8(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12
            (-(Simd32x3::from(other.group6()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])) + (Simd32x3::from(self.group0()[3]) * other.group7())),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                (self.group0()[3] * other.group9()[0] * -1.0),
            ]),
            // e423, e431, e412
            (Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
            ]),
            // e3215
            (self.group0()[3] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for Plane {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        3        0
    // no simd        3        9        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for Plane {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0] * -1.0),
            (self.group0()[3] * other.group0()[1] * -1.0),
            (self.group0()[3] * other.group0()[2] * -1.0),
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullSphereAtOrigin> for Plane {
    type Output = AntiDipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiDipoleOnOrigin::from_groups(/* e423, e431, e412, e321 */ (self.group0() * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for Plane {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for Plane {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        2       11        0
    //  no simd        2       17        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            Simd32x4::from([
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
            ]),
        );
    }
}
impl AntiWedge<Origin> for Plane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for Plane {
    type Output = Line;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        4        8        0
    //  no simd        6       12        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Line::from_groups(
            // e415, e425, e435
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
            // e235, e315, e125
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for Plane {
    type Output = Line;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        4        0
    // no simd        3       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Line::from_groups(
            // e415, e425, e435
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<RoundPoint> for Plane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for Plane {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[0]));
    }
}
impl AntiWedge<Sphere> for Plane {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd3        1        4        0
    // Totals...
    // yes simd        4       12        0
    //  no simd        6       20        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Circle::from_groups(
            // e423, e431, e412
            (Simd32x3::from(other[e4315]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]) * Simd32x3::from(-1.0)),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other[e4315] * -1.0),
            ]),
            // e235, e315, e125
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for Plane {
    type Output = CircleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       11        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return CircleOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[1]) * Simd32x4::from(-1.0)),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for Plane {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd3        0        4        0
    // Totals...
    // yes simd        3       12        0
    //  no simd        3       20        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Circle::from_groups(
            // e423, e431, e412
            (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]) * Simd32x3::from(-1.0)),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3] * -1.0),
            ]),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorEven> for Plane {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       23        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       11       26        0
    //  no simd       17       35        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group3()[3]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group0()[2] * other.group3()[2]) + (self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group0()[3] * other.group1()[2]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Plane {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       22        0
    //    simd4        0        1        0
    // Totals...
    // yes simd       11       23        0
    //  no simd       11       26        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group0()[3] * other.group1()[2]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Plane {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        2        5        0
    // Totals...
    // yes simd        4       14        0
    //  no simd       10       29        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((swizzle!(self.group0(), 2, 3, 3, 3) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (swizzle!(self.group0(), 0, 2, 0, 1) * Simd32x4::from([other.group0()[1], other.group2()[1], other.group2()[2], other.group2()[0]]))
                + (Simd32x4::from([1.0, -1.0, -1.0, -1.0])
                    * swizzle!(self.group0(), 1, 1, 2, 0)
                    * Simd32x4::from([other.group0()[2], other.group2()[2], other.group2()[0], other.group2()[1]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3] * -1.0),
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for Plane {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Plane {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       17        0
    //  no simd        5       20        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for Plane {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       21        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       12       25        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group2()[3]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (-(self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                (-(self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOdd> for Plane {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       28        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       11       30        0
    //  no simd       17       36        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3] * -1.0),
                (self.group0()[1] * other.group2()[3] * -1.0),
                (self.group0()[2] * other.group2()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group3()[2]) + (self.group0()[2] * other.group3()[1])),
                ((self.group0()[0] * other.group3()[2]) - (self.group0()[2] * other.group3()[0])),
                (-(self.group0()[0] * other.group3()[1]) + (self.group0()[1] * other.group3()[0])),
                (self.group0()[3] * other.group2()[3] * -1.0),
            ]),
            // e235, e315, e125, e5
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group3()[3]),
                    (self.group0()[1] * other.group3()[3]),
                    (self.group0()[2] * other.group3()[3]),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    (-(self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for Plane {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       29        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       30        0
    //  no simd       11       33        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3] * -1.0),
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (self.group0()[3] * other.group1()[3] * -1.0),
            ]),
            // e235, e315, e125, e5
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group2()[3]),
                    (self.group0()[1] * other.group2()[3]),
                    (self.group0()[2] * other.group2()[3]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for Plane {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       12       22        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group2()[3]),
                    (self.group0()[1] * other.group2()[3]),
                    (self.group0()[2] * other.group2()[3]),
                    (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for Plane {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       15        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        4       17        0
    //  no simd        4       23        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for Plane {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       23        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        5       25        0
    //  no simd        5       31        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0] * -1.0),
                (self.group0()[1] * other.group1()[0] * -1.0),
                (self.group0()[2] * other.group1()[0] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[3]) + (self.group0()[2] * other.group1()[2])),
                ((self.group0()[0] * other.group1()[3]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[1] * other.group1()[1])),
                (self.group0()[3] * other.group1()[0] * -1.0),
            ]),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[3], other.group0()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for Plane {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       14        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        7       17        0
    //  no simd       10       26        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group2()[3]) * Simd32x4::from(-1.0)),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    (-(self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl InfixAntiWedge for PlaneOnOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for PlaneOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
            (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
            ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for PlaneOnOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        4        0
    // no simd        3       12        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[3]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiDualNum> for PlaneOnOrigin {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return LineAtInfinity::from_groups(/* e235, e315, e125 */ (self.group0() * Simd32x3::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatOrigin> for PlaneOnOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (self.group0() * Simd32x3::from(other[e321]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiFlatPoint> for PlaneOnOrigin {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        4        0
    // no simd        3       12        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[3]) * Simd32x3::from(-1.0)),
            // e15, e25, e35
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<AntiFlector> for PlaneOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3] * -1.0),
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                ((self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for PlaneOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0] * -1.0),
            (self.group0()[1] * other.group0()[0] * -1.0),
            (self.group0()[2] * other.group0()[0] * -1.0),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<AntiLine> for PlaneOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<AntiLineOnOrigin> for PlaneOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<AntiMotor> for PlaneOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for PlaneOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<AntiPlane> for PlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for PlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for PlaneOnOrigin {
    type Output = PlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return PlaneOnOrigin::from_groups(/* e4235, e4315, e4125 */ (self.group0() * Simd32x3::from(other[e12345])));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for PlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for PlaneOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3] * -1.0),
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                0.0,
            ]),
            // e4, e1, e2, e3
            Simd32x4::from([
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
            ]),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for PlaneOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                ((self.group0()[2] * other.group1()[3]) + (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3] * -1.0),
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Circle> for PlaneOnOrigin {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd3        2        4        0
    // Totals...
    // yes simd        4       13        0
    //  no simd        8       21        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            ((swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3] * -1.0),
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group2(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group2(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for PlaneOnOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        9        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        6       11        0
    //  no simd        8       15        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group2(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group2(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for PlaneOnOrigin {
    type Output = DipoleAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        3       11        0
    //  no simd        5       15        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return DipoleAtInfinity::from_groups(
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3] * -1.0),
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group1(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group1(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for PlaneOnOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        4        0
    // no simd        6       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
            // e15, e25, e35
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group1(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group1(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for PlaneOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ Simd32x4::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for PlaneOnOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        6        0
    // no simd        6       18        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group0()[3]) * Simd32x3::from(-1.0)),
            // e15, e25, e35
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group1(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group1(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<Dipole> for PlaneOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for PlaneOnOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        6        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ Simd32x2::from([
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<DipoleAtInfinity> for PlaneOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<DipoleAtOrigin> for PlaneOnOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ Simd32x2::from([
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<DipoleOnOrigin> for PlaneOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for PlaneOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for PlaneOnOrigin {
    type Output = PlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return PlaneOnOrigin::from_groups(/* e4235, e4315, e4125 */ (self.group0() * Simd32x3::from(other.group0()[1])));
    }
}
impl AntiWedge<FlatPoint> for PlaneOnOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for PlaneOnOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for PlaneOnOrigin {
    type Output = Motor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Motor::from_groups(
            // e415, e425, e435, e12345
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for PlaneOnOrigin {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for PlaneOnOrigin {
    type Output = LineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return LineOnOrigin::from_groups(
            // e415, e425, e435
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[3], other.group0()[1], other.group0()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[2], other.group0()[3], other.group0()[1]]))),
        );
    }
}
impl AntiWedge<Horizon> for PlaneOnOrigin {
    type Output = LineAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return LineAtInfinity::from_groups(/* e235, e315, e125 */ (self.group0() * Simd32x3::from(other[e3215])));
    }
}
impl AntiWedge<Line> for PlaneOnOrigin {
    type Output = FlatPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        9        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return FlatPoint::from_groups(/* e15, e25, e35, e45 */ Simd32x4::from([
            (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineAtInfinity> for PlaneOnOrigin {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for PlaneOnOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return FlatOrigin::from_groups(
            // e45
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for PlaneOnOrigin {
    type Output = Flector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return Flector::from_groups(
            // e15, e25, e35, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for PlaneOnOrigin {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for PlaneOnOrigin {
    type Output = FlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return FlectorOnOrigin::from_groups(/* e45, e4235, e4315, e4125 */ Simd32x4::from([
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<MultiVector> for PlaneOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       17       33        0
    //    simd3        1        7        0
    // Totals...
    // yes simd       18       40        0
    //  no simd       20       54        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[1] * other.group5()[2]) - (self.group0()[2] * other.group5()[1])),
                (-(self.group0()[0] * other.group5()[2]) + (self.group0()[2] * other.group5()[0])),
                ((self.group0()[0] * other.group5()[1]) - (self.group0()[1] * other.group5()[0])),
                ((self.group0()[2] * other.group3()[2]) + (self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group4()[2]) - (self.group0()[0] * other.group4()[0]) - (self.group0()[1] * other.group4()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[1] * other.group7()[2]) - (self.group0()[2] * other.group7()[1])),
                (-(self.group0()[0] * other.group7()[2]) + (self.group0()[2] * other.group7()[0])),
                ((self.group0()[0] * other.group7()[1]) - (self.group0()[1] * other.group7()[0])),
                (-(self.group0()[2] * other.group6()[2]) - (self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
            ]),
            // e15, e25, e35
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group8(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group8(), 1, 2, 0))),
            // e23, e31, e12
            (self.group0() * Simd32x3::from(other.group6()[3]) * Simd32x3::from(-1.0)),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group9()[0]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            (self.group0() * Simd32x3::from(other[e45])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
            ]),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for PlaneOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) - (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for PlaneOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for PlaneOnOrigin {
    type Output = NullCircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullCircleAtOrigin::from_groups(/* e423, e431, e412 */ (self.group0() * Simd32x3::from(other[e1234]) * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for PlaneOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for PlaneOnOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3] * -1.0),
            (self.group0()[1] * other.group0()[3] * -1.0),
            (self.group0()[2] * other.group0()[3] * -1.0),
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Plane> for PlaneOnOrigin {
    type Output = Line;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        3        0
    // no simd        3        9        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Line::from_groups(
            // e415, e425, e435
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e235, e315, e125
            (self.group0() * Simd32x3::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for PlaneOnOrigin {
    type Output = LineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return LineOnOrigin::from_groups(
            // e415, e425, e435
            (-(swizzle!(self.group0(), 1, 2, 0) * swizzle!(other.group0(), 2, 0, 1)) + (swizzle!(self.group0(), 2, 0, 1) * swizzle!(other.group0(), 1, 2, 0))),
        );
    }
}
impl AntiWedge<RoundPoint> for PlaneOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for PlaneOnOrigin {
    type Output = CircleAligningOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        5        0
    // no simd        3       15        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return CircleAligningOrigin::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other[e4315]) * Simd32x3::from(-1.0)),
            // e415, e425, e435
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e235, e315, e125
            (self.group0() * Simd32x3::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for PlaneOnOrigin {
    type Output = CircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return CircleAtOrigin::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[1]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            (self.group0() * Simd32x3::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for PlaneOnOrigin {
    type Output = CircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        4        0
    // no simd        3       12        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return CircleOnOrigin::from_groups(
            // e423, e431, e412
            (self.group0() * Simd32x3::from(other.group0()[3]) * Simd32x3::from(-1.0)),
            // e415, e425, e435
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
        );
    }
}
impl AntiWedge<VersorEven> for PlaneOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       27        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                ((self.group0()[2] * other.group3()[2]) + (self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3] * -1.0),
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for PlaneOnOrigin {
    type Output = VersorOddAligningOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOddAligningOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for PlaneOnOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       21        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3] * -1.0),
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for PlaneOnOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        4        0
    // no simd        6       12        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group0()[2], other.group0()[0], other.group0()[1]]))
                - (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group0()[1], other.group0()[2], other.group0()[0]]))),
            // e15, e25, e35
            (-(swizzle!(self.group0(), 1, 2, 0) * Simd32x3::from([other.group1()[2], other.group1()[0], other.group1()[1]]))
                + (swizzle!(self.group0(), 2, 0, 1) * Simd32x3::from([other.group1()[1], other.group1()[2], other.group1()[0]]))),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for PlaneOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for PlaneOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       21        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                ((self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3] * -1.0),
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOdd> for PlaneOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       27        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3] * -1.0),
                (self.group0()[1] * other.group2()[3] * -1.0),
                (self.group0()[2] * other.group2()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group3()[2]) + (self.group0()[2] * other.group3()[1])),
                ((self.group0()[0] * other.group3()[2]) - (self.group0()[2] * other.group3()[0])),
                (-(self.group0()[0] * other.group3()[1]) + (self.group0()[1] * other.group3()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group3()[3]),
                (self.group0()[1] * other.group3()[3]),
                (self.group0()[2] * other.group3()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for PlaneOnOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       21        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3] * -1.0),
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for PlaneOnOrigin {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for PlaneOnOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       15        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3] * -1.0),
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for PlaneOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0] * -1.0),
                (self.group0()[1] * other.group1()[0] * -1.0),
                (self.group0()[2] * other.group1()[0] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[3]) + (self.group0()[2] * other.group1()[2])),
                ((self.group0()[0] * other.group1()[3]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[1] * other.group1()[1])),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for PlaneOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       21        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3] * -1.0),
                (self.group0()[1] * other.group2()[3] * -1.0),
                (self.group0()[2] * other.group2()[3] * -1.0),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl InfixAntiWedge for RoundPoint {}
impl AntiWedge<AntiDualNum> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[0]));
    }
}
impl AntiWedge<AntiMotor> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group1()[3]));
    }
}
impl AntiWedge<AntiScalar> for RoundPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(/* e1, e2, e3, e4 */ (self.group0() * Simd32x4::from(other[e12345])), /* e5 */ (self[e2] * other[e12345]));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e2] * other.group1()[3]));
    }
}
impl AntiWedge<DualNum> for RoundPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e5
            (self[e2] * other.group0()[1]),
        );
    }
}
impl AntiWedge<Flector> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group1()[3]) + (self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<FlectorOnOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        );
    }
}
impl AntiWedge<Horizon> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e3215]));
    }
}
impl AntiWedge<Motor> for RoundPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e5
            (self[e2] * other.group0()[3]),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for RoundPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e5
            (self[e2] * other.group0()[3]),
        );
    }
}
impl AntiWedge<MultiVector> for RoundPoint {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        4        7        0
    //  no simd        4       10        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e2] * other.group9()[0])
                    + (self.group0()[3] * other[e45])
                    + (self.group0()[2] * other.group9()[3])
                    + (self.group0()[0] * other.group9()[1])
                    + (self.group0()[1] * other.group9()[2])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e5
            (self[e2] * other.group0()[1]),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e2] * other[e1234]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e2] * other.group0()[3]));
    }
}
impl AntiWedge<Plane> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Sphere> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        5        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self[e2] * other[e4315])
                + (self.group0()[3] * other.group0()[3])
                + (self.group0()[2] * other.group0()[2])
                + (self.group0()[0] * other.group0()[0])
                + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ ((self.group0()[3] * other.group0()[0]) + (self[e2] * other.group0()[1])));
    }
}
impl AntiWedge<SphereOnOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self[e2] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<VersorEven> for RoundPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e5
            (self[e2] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for RoundPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e5
            (self[e2] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for RoundPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e5
            (self[e2] * other.group0()[0]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for RoundPoint {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e5
            (self[e2] * other.group0()[3]),
        );
    }
}
impl AntiWedge<VersorOdd> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        5        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self[e2] * other.group2()[3])
                + (self.group0()[3] * other.group3()[3])
                + (self.group0()[2] * other.group3()[2])
                + (self.group0()[0] * other.group3()[0])
                + (self.group0()[1] * other.group3()[1])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        5        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self[e2] * other.group1()[3])
                + (self.group0()[3] * other.group2()[3])
                + (self.group0()[2] * other.group2()[2])
                + (self.group0()[0] * other.group2()[0])
                + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group2()[3]) + (self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ ((self.group0()[3] * other.group0()[3]) + (self[e2] * other.group1()[3])));
    }
}
impl AntiWedge<VersorOddOnOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self[e2] * other.group1()[0]) + (self.group0()[2] * other.group1()[3]) + (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for RoundPoint {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ ((self.group0()[3] * other.group1()[3]) + (self[e2] * other.group2()[3])));
    }
}
impl InfixAntiWedge for RoundPointAtOrigin {}
impl AntiWedge<AntiDualNum> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[0]));
    }
}
impl AntiWedge<AntiMotor> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group1()[3]));
    }
}
impl AntiWedge<AntiScalar> for RoundPointAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (self.group0() * Simd32x2::from(other[e12345])));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[1] * other.group1()[3]));
    }
}
impl AntiWedge<DualNum> for RoundPointAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (self.group0() * Simd32x2::from(other.group0()[1])));
    }
}
impl AntiWedge<Flector> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group1()[3]));
    }
}
impl AntiWedge<FlectorAtInfinity> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[3]));
    }
}
impl AntiWedge<Horizon> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other[e3215]));
    }
}
impl AntiWedge<Motor> for RoundPointAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (self.group0() * Simd32x2::from(other.group0()[3])));
    }
}
impl AntiWedge<MotorOnOrigin> for RoundPointAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (self.group0() * Simd32x2::from(other.group0()[3])));
    }
}
impl AntiWedge<MultiVector> for RoundPointAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        4        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([((self.group0()[0] * other[e45]) + (self.group0()[1] * other.group9()[0])), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[1])]),
            // e5
            (self.group0()[1] * other.group0()[1]),
            // e41, e42, e43, e45
            Simd32x4::from(0.0),
            // e15, e25, e35
            Simd32x3::from(0.0),
            // e23, e31, e12
            Simd32x3::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            Simd32x3::from(0.0),
            // e235, e315, e125
            Simd32x3::from(0.0),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[1] * other[e1234]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[1] * other.group0()[3]));
    }
}
impl AntiWedge<Plane> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[3]));
    }
}
impl AntiWedge<Sphere> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ ((self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other[e4315])));
    }
}
impl AntiWedge<SphereAtOrigin> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])));
    }
}
impl AntiWedge<SphereOnOrigin> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[1] * other.group0()[3]));
    }
}
impl AntiWedge<VersorEven> for RoundPointAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (self.group0() * Simd32x2::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for RoundPointAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (self.group0() * Simd32x2::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for RoundPointAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (self.group0() * Simd32x2::from(other.group0()[0])));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for RoundPointAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (self.group0() * Simd32x2::from(other.group0()[3])));
    }
}
impl AntiWedge<VersorOdd> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return Scalar::from_groups(/* scalar */ ((self.group0()[0] * other.group3()[3]) + (self.group0()[1] * other.group2()[3])));
    }
}
impl AntiWedge<VersorOddAligningOrigin> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ ((self.group0()[0] * other.group2()[3]) + (self.group0()[1] * other.group1()[3])));
    }
}
impl AntiWedge<VersorOddAtInfinity> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group2()[3]));
    }
}
impl AntiWedge<VersorOddAtOrigin> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ ((self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other.group1()[3])));
    }
}
impl AntiWedge<VersorOddOnOrigin> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[1] * other.group1()[0]));
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for RoundPointAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ ((self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other.group2()[3])));
    }
}
impl InfixAntiWedge for Scalar {}
impl AntiWedge<AntiScalar> for Scalar {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[scalar] * other[e12345]));
    }
}
impl AntiWedge<DualNum> for Scalar {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[scalar] * other.group0()[1]));
    }
}
impl AntiWedge<Motor> for Scalar {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[scalar] * other.group0()[3]));
    }
}
impl AntiWedge<MotorOnOrigin> for Scalar {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[scalar] * other.group0()[3]));
    }
}
impl AntiWedge<MultiVector> for Scalar {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[scalar] * other.group0()[1]));
    }
}
impl AntiWedge<VersorEven> for Scalar {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[scalar] * other.group0()[3]));
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Scalar {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[scalar] * other.group0()[3]));
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Scalar {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[scalar] * other.group0()[0]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Scalar {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[scalar] * other.group0()[3]));
    }
}
impl InfixAntiWedge for Sphere {}
impl AntiWedge<AntiCircleOnOrigin> for Sphere {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    (-(self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for Sphere {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        4        8        0
    //  no simd        6       12        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            (-(Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
        );
    }
}
impl AntiWedge<AntiDualNum> for Sphere {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        use crate::elements::*;
        return AntiFlatPoint::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self[e4315]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for Sphere {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other[e321]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for Sphere {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        4        8        0
    //  no simd        6       12        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (-(Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self[e4315]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e15, e25, e35
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for Sphere {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       16        0
    //  no simd        9       19        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(self[e4315]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) * -1.0),
                    ((self.group0()[1] * other.group0()[3]) * -1.0),
                    ((self.group0()[2] * other.group0()[3]) * -1.0),
                    ((self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for Sphere {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0] * -1.0),
            (self.group0()[1] * other.group0()[0] * -1.0),
            (self.group0()[2] * other.group0()[0] * -1.0),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<AntiLine> for Sphere {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self[e4315] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self[e4315] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self[e4315] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for Sphere {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<AntiMotor> for Sphere {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self[e4315]])),
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self[e4315] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self[e4315] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self[e4315] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for Sphere {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for Sphere {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self[e4315] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for Sphere {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for Sphere {
    type Output = Sphere;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return Sphere::from_groups(
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other[e12345])),
            // e1234
            (self[e4315] * other[e12345]),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for Sphere {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for Sphere {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        5       11        0
    //  no simd        8       20        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                    (-(self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    (-(self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for Sphere {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       16        0
    //  no simd        9       19        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[0]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group0()[2] * other.group1()[3]) + (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (-(self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                (-(self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Circle> for Sphere {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd3        4        6        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       15        0
    //  no simd       20       30        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            ((Simd32x3::from(self[e4315]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    ((self[e4315] * other.group2()[0]) + (self.group0()[3] * other.group0()[0])),
                    ((self[e4315] * other.group2()[1]) + (self.group0()[3] * other.group0()[1])),
                    ((self[e4315] * other.group2()[2]) + (self.group0()[3] * other.group0()[2])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                - (swizzle!(other.group2(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group2(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for Sphere {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        9        0
    //    simd3        4        6        0
    // Totals...
    // yes simd        9       15        0
    //  no simd       17       27        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            ((Simd32x3::from(self[e4315]) * other.group1()) + (swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self[e4315] * other.group2()[0])),
                ((self.group0()[3] * other.group0()[1]) + (self[e4315] * other.group2()[1])),
                ((self.group0()[3] * other.group0()[2]) + (self[e4315] * other.group2()[2])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * other.group1()) - (swizzle!(other.group2(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group2(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for Sphere {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        2        4        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4       10        0
    //  no simd       11       21        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self[e4315]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 0, 1, 2, 2) * swizzle!(other.group0(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self[e4315] * other.group1()[0]),
                    (self[e4315] * other.group1()[1]),
                    (self[e4315] * other.group1()[2]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))
                - (swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for Sphere {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        3        6        0
    // no simd        9       18        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group0()[3]) * other.group0()) + (Simd32x3::from(self[e4315]) * other.group1())),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for Sphere {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        2        4        0
    // Totals...
    // yes simd        4       10        0
    //  no simd        8       18        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            ((Simd32x3::from(self[e4315]) * other.group1()) + (swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for Sphere {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        3        5        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       21        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(self[e4315]) * other.group1()) - (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Dipole> for Sphere {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       14        0
    //  no simd       15       20        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((Simd32x4::from(self[e4315]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    (-(self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for Sphere {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       14        0
    //  no simd        9       17        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((Simd32x4::from(self[e4315]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) * -1.0),
                    ((self.group0()[3] * other.group0()[1]) * -1.0),
                    ((self.group0()[3] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for Sphere {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       14        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self[e4315] * other.group1()[0]) + (self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                ((self[e4315] * other.group1()[1]) - (self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self[e4315] * other.group1()[2]) + (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self[e4315] * other.group0()[3]),
            ]),
            // e5
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for Sphere {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[3] * other.group0()[0]) + (self[e4315] * other.group1()[0])),
                (-(self.group0()[3] * other.group0()[1]) + (self[e4315] * other.group1()[1])),
                (-(self.group0()[3] * other.group0()[2]) + (self[e4315] * other.group1()[2])),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for Sphere {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       12        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self[e4315] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (self.group0()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for Sphere {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       13       18        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self[e4315] * other.group2()[0]) - (self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    ((self[e4315] * other.group2()[1]) - (self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    ((self[e4315] * other.group2()[2]) - (self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for Sphere {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        6        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self[e4315] * other.group0()[0])]),
            // e23, e31, e12, e45
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self[e4315] * other.group0()[1])]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for Sphere {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (Simd32x2::from(other[e45]) * Simd32x2::from([self[e4315], self.group0()[3]]) * Simd32x2::from([1.0, -1.0])),
        );
    }
}
impl AntiWedge<FlatPoint> for Sphere {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        4        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        5        0
    //  no simd        3        8        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self[e4315]) * other.group0()),
            // e5
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for Sphere {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self[e4315] * other.group0()[0]),
            (self[e4315] * other.group0()[1]),
            (self[e4315] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Flector> for Sphere {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       18        0
    //  no simd        9       24        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([(self[e4315] * other.group1()[0]), (self[e4315] * other.group1()[1]), (self[e4315] * other.group1()[2]), 0.0]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (self[e4315] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group1()[3]),
                    (self.group0()[1] * other.group1()[3]),
                    (self.group0()[2] * other.group1()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e1, e2, e3, e4
            (Simd32x4::from(self[e4315]) * other.group0()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for Sphere {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self[e4315]])),
            // e1, e2, e3, e5
            Simd32x4::from([
                (self[e4315] * other.group0()[0]),
                (self[e4315] * other.group0()[1]),
                (self[e4315] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for Sphere {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3       10        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3       12        0
    //  no simd        3       18        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([(self[e4315] * other.group0()[1]), (self[e4315] * other.group0()[2]), (self[e4315] * other.group0()[3]), 0.0]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[2] * other.group0()[2])),
                ((self.group0()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group0()[1])),
                (self[e4315] * other.group0()[0]),
            ]),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[3]) * swizzle!(other.group0(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Horizon> for Sphere {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlatPoint::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self[e4315]])),
        );
    }
}
impl AntiWedge<Infinity> for Sphere {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self[e4315] * other[e5]));
    }
}
impl AntiWedge<Line> for Sphere {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        2        4        0
    // Totals...
    // yes simd        4       10        0
    //  no simd        8       18        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        use crate::elements::*;
        return Dipole::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self[e4315]) * other.group0()),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self[e4315] * other.group1()[0]),
                (self[e4315] * other.group1()[1]),
                (self[e4315] * other.group1()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * other.group0()) - (swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineAtInfinity> for Sphere {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        3        0
    // no simd        3        9        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self[e4315]) * other.group0()),
            // e15, e25, e35
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for Sphere {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2        9        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        use crate::elements::*;
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (self[e4315] * other.group0()[0]),
                (self[e4315] * other.group0()[1]),
                (self[e4315] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<Motor> for Sphere {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       16        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        8       18        0
    //  no simd        8       24        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self[e4315]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self[e4315] * other.group1()[0]),
                (self[e4315] * other.group1()[1]),
                (self[e4315] * other.group1()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                ((self.group0()[3] * other.group0()[2]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (self[e4315] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for Sphere {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        3       10        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self[e4315]) * other.group0()),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for Sphere {
    type Output = VersorOddAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        2        8        0
    //  no simd        2       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOddAligningOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (self[e4315] * other.group0()[0]),
                (self[e4315] * other.group0()[1]),
                (self[e4315] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            (other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self[e4315]])),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for Sphere {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       30        0
    //    simd3        6       10        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       25       45        0
    //  no simd       49       80        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self[e4315] * other[e1])
                    + (self.group0()[3] * other.group1()[3])
                    + (self.group0()[2] * other.group1()[2])
                    + (self.group0()[0] * other.group1()[0])
                    + (self.group0()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self[e4315]) * Simd32x4::from([other.group4()[0], other.group4()[1], other.group4()[2], other.group3()[3]]))
                + (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group5()[2], other.group5()[0], other.group5()[1], other.group3()[2]]))
                + Simd32x4::from([
                    (-(self.group0()[3] * other.group3()[0]) - (self.group0()[2] * other.group5()[1])),
                    (-(self.group0()[3] * other.group3()[1]) - (self.group0()[0] * other.group5()[2])),
                    (-(self.group0()[3] * other.group3()[2]) - (self.group0()[1] * other.group5()[0])),
                    ((self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
                ])),
            // e5
            (-(self.group0()[3] * other.group3()[3]) - (self.group0()[2] * other.group4()[2]) - (self.group0()[0] * other.group4()[0]) - (self.group0()[1] * other.group4()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group7()[1], other.group7()[2], other.group7()[0], other.group6()[2]]))
                + Simd32x4::from([
                    ((self[e4315] * other.group6()[0]) + (self.group0()[1] * other.group7()[2])),
                    ((self[e4315] * other.group6()[1]) + (self.group0()[2] * other.group7()[0])),
                    ((self[e4315] * other.group6()[2]) + (self.group0()[0] * other.group7()[1])),
                    (-(self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))
                - (swizzle!(other.group8(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group8(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12
            ((Simd32x3::from(self[e4315]) * other.group8()) - (Simd32x3::from(other.group6()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * other.group7())),
            // e415, e425, e435, e321
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * swizzle!(other.group9(), 3, 1, 2, 0))
                + Simd32x4::from([
                    (self.group0()[2] * other.group9()[2]),
                    (self.group0()[0] * other.group9()[3]),
                    (self.group0()[1] * other.group9()[1]),
                    (self[e4315] * other[e45]),
                ])),
            // e423, e431, e412
            (-(Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self[e4315]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e235, e315, e125
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(other.group0()[1]) * Simd32x4::from([self[e4315], self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e3215
            (self.group0()[3] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for Sphere {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        3        0
    // no simd        3        9        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for Sphere {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0] * -1.0),
            (self.group0()[3] * other.group0()[1] * -1.0),
            (self.group0()[3] * other.group0()[2] * -1.0),
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullSphereAtOrigin> for Sphere {
    type Output = AntiDipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiDipoleOnOrigin::from_groups(/* e423, e431, e412, e321 */ (self.group0() * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for Sphere {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for Sphere {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        2       11        0
    //  no simd        2       17        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            Simd32x4::from([
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
            ]),
        );
    }
}
impl AntiWedge<Origin> for Sphere {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for Sphere {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        7        0
    //    simd3        1        3        0
    // Totals...
    // yes simd        4       10        0
    //  no simd        6       16        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        use crate::elements::*;
        return Circle::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self[e4315]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self[e4315] * other.group0()[3]),
            ]),
            // e235, e315, e125
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for Sphere {
    type Output = CircleAligningOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        5        0
    // no simd        3       15        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        use crate::elements::*;
        return CircleAligningOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self[e4315]) * other.group0()),
            // e415, e425, e435
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<RoundPoint> for Sphere {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self[e4315] * other[e2])
                + (self.group0()[3] * other.group0()[3])
                + (self.group0()[2] * other.group0()[2])
                + (self.group0()[0] * other.group0()[0])
                + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for Sphere {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ ((self.group0()[3] * other.group0()[0]) + (self[e4315] * other.group0()[1])));
    }
}
impl AntiWedge<Sphere> for Sphere {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        2        4        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        6        0
    //  no simd       10       20        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Circle::from_groups(
            // e423, e431, e412
            (-(Simd32x3::from(other[e4315]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self[e4315]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e415, e425, e435, e321
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other[e4315]]))
                + (swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self[e4315]]))),
            // e235, e315, e125
            ((Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for Sphere {
    type Output = CircleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        1        9        0
    //  no simd        1       11        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return CircleOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1] * -1.0),
                (self.group0()[1] * other.group0()[1] * -1.0),
                (self.group0()[2] * other.group0()[1] * -1.0),
                (-(self.group0()[3] * other.group0()[1]) + (self[e4315] * other.group0()[0])),
            ]),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for Sphere {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd3        1        4        0
    // Totals...
    // yes simd        4       12        0
    //  no simd        6       20        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        use crate::elements::*;
        return Circle::from_groups(
            // e423, e431, e412
            (-(Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self[e4315]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3] * -1.0),
            ]),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorEven> for Sphere {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       27        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       15       31        0
    //  no simd       24       43        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self[e4315]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + (swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group3()[3]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group0()[2] * other.group3()[2]) + (self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    ((self[e4315] * other.group2()[0]) + (self.group0()[3] * other.group0()[0])),
                    ((self[e4315] * other.group2()[1]) + (self.group0()[3] * other.group0()[1])),
                    ((self[e4315] * other.group2()[2]) + (self.group0()[3] * other.group0()[2])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group0()[3] * other.group1()[2]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (self[e4315] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for Sphere {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       26        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       15       28        0
    //  no simd       18       34        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((other.group1() * Simd32x4::from([self[e4315], self[e4315], self[e4315], self.group0()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                    (self[e4315] * other.group2()[3]),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self[e4315] * other.group2()[0])),
                ((self.group0()[3] * other.group0()[1]) + (self[e4315] * other.group2()[1])),
                ((self.group0()[3] * other.group0()[2]) + (self[e4315] * other.group2()[2])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group0()[3] * other.group1()[2]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (self[e4315] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for Sphere {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       22        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       11       24        0
    //  no simd       14       30        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self[e4315] * other.group1()[0]),
                (self[e4315] * other.group1()[1]),
                (self[e4315] * other.group1()[2]),
                ((self[e4315] * other.group2()[3]) + (self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self[e4315] * other.group2()[0]),
                    (self[e4315] * other.group2()[1]),
                    (self[e4315] * other.group2()[2]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) - (self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                ((self.group0()[3] * other.group1()[2]) - (self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (self[e4315] * other.group0()[0]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for Sphere {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       16        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        7       18        0
    //  no simd       10       24        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                + (Simd32x4::from([
                    (self.group0()[2] * other.group0()[1]),
                    (self.group0()[0] * other.group0()[2]),
                    (self.group0()[1] * other.group0()[0]),
                    (self[e4315] * other.group1()[3]),
                ]) * Simd32x4::from([-1.0, -1.0, -1.0, 1.0]))),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self[e4315] * other.group1()[0])),
                ((self.group0()[3] * other.group0()[1]) + (self[e4315] * other.group1()[1])),
                ((self.group0()[3] * other.group0()[2]) + (self[e4315] * other.group1()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for Sphere {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       20        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        8       21        0
    //  no simd        8       24        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self[e4315] * other.group1()[0]) + (self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                ((self[e4315] * other.group1()[1]) - (self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self[e4315] * other.group1()[2]) + (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (self[e4315] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for Sphere {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       25        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       13       26        0
    //  no simd       16       29        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group2()[3]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self[e4315] * other.group1()[3]) + (self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self[e4315] * other.group1()[0]) - (self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self[e4315] * other.group1()[1]) - (self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self[e4315] * other.group1()[2]) - (self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOdd> for Sphere {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       20        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       13       25        0
    //  no simd       25       40        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (-(self.group0()[0] * other.group2()[3]) + (self[e4315] * other.group3()[0])),
                (-(self.group0()[1] * other.group2()[3]) + (self[e4315] * other.group3()[1])),
                (-(self.group0()[2] * other.group2()[3]) + (self[e4315] * other.group3()[2])),
                0.0,
            ]),
            // e415, e425, e435, e321
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group2()[3]]))
                + (swizzle!(other.group3(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self[e4315]]))),
            // e235, e315, e125, e5
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group3()[3]),
                    (self.group0()[1] * other.group3()[3]),
                    (self.group0()[2] * other.group3()[3]),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e1, e2, e3, e4
            ((Simd32x4::from(self[e4315]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    (-(self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for Sphere {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       21        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       10       25        0
    //  no simd       19       37        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (-(self.group0()[0] * other.group1()[3]) + (self[e4315] * other.group2()[0])),
                (-(self.group0()[1] * other.group1()[3]) + (self[e4315] * other.group2()[1])),
                (-(self.group0()[2] * other.group1()[3]) + (self[e4315] * other.group2()[2])),
                0.0,
            ]),
            // e415, e425, e435, e321
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[3]]))
                + (swizzle!(other.group2(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self[e4315]]))),
            // e235, e315, e125, e5
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group2()[3]),
                    (self.group0()[1] * other.group2()[3]),
                    (self.group0()[2] * other.group2()[3]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e1, e2, e3, e4
            ((Simd32x4::from(self[e4315]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) * -1.0),
                    ((self.group0()[3] * other.group0()[1]) * -1.0),
                    ((self.group0()[3] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for Sphere {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       26        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       12       27        0
    //  no simd       15       30        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([(self[e4315] * other.group2()[0]), (self[e4315] * other.group2()[1]), (self[e4315] * other.group2()[2]), 0.0]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (self[e4315] * other.group2()[3]),
            ]),
            // e235, e315, e125, e5
            (-(Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group2()[3]),
                    (self.group0()[1] * other.group2()[3]),
                    (self.group0()[2] * other.group2()[3]),
                    (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
                ])),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self[e4315] * other.group0()[1]) + (self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                ((self[e4315] * other.group0()[2]) - (self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self[e4315] * other.group0()[3]) + (self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
                (self[e4315] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for Sphere {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       23        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3] * -1.0),
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                (-(self.group0()[3] * other.group1()[3]) + (self[e4315] * other.group0()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[3] * other.group0()[0]) + (self[e4315] * other.group1()[0])),
                (-(self.group0()[3] * other.group0()[1]) + (self[e4315] * other.group1()[1])),
                (-(self.group0()[3] * other.group0()[2]) + (self[e4315] * other.group1()[2])),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for Sphere {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       24        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        9       26        0
    //  no simd        9       32        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (-(self.group0()[0] * other.group1()[0]) + (self[e4315] * other.group1()[1])),
                (-(self.group0()[1] * other.group1()[0]) + (self[e4315] * other.group1()[2])),
                (-(self.group0()[2] * other.group1()[0]) + (self[e4315] * other.group1()[3])),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[3]) + (self.group0()[2] * other.group1()[2])),
                ((self.group0()[0] * other.group1()[3]) - (self.group0()[2] * other.group1()[1])),
                (-(self.group0()[0] * other.group1()[2]) + (self.group0()[1] * other.group1()[1])),
                (self.group0()[3] * other.group1()[0] * -1.0),
            ]),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[3], other.group0()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self[e4315] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for Sphere {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       25        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       11       26        0
    //  no simd       14       29        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3] * -1.0),
                (self.group0()[1] * other.group2()[3] * -1.0),
                (self.group0()[2] * other.group2()[3] * -1.0),
                (-(self.group0()[3] * other.group2()[3]) + (self[e4315] * other.group1()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self[e4315] * other.group2()[0]) - (self.group0()[3] * other.group0()[0]) - (self.group0()[2] * other.group1()[1])),
                    ((self[e4315] * other.group2()[1]) - (self.group0()[3] * other.group0()[1]) - (self.group0()[0] * other.group1()[2])),
                    ((self[e4315] * other.group2()[2]) - (self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group1()[0])),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl InfixAntiWedge for SphereAtOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for SphereAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for SphereAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiDualNum> for SphereAtOrigin {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiFlatOrigin::from_groups(/* e321 */ (self.group0()[1] * other.group0()[0]));
    }
}
impl AntiWedge<AntiFlatPoint> for SphereAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiFlector> for SphereAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]])),
        );
    }
}
impl AntiWedge<AntiLine> for SphereAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[1]) * other.group1()));
    }
}
impl AntiWedge<AntiMotor> for SphereAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (Simd32x4::from(self.group0()[1]) * swizzle!(other.group1(), 3, 0, 1, 2)));
    }
}
impl AntiWedge<AntiPlane> for SphereAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[1] * other.group0()[3]));
    }
}
impl AntiWedge<AntiScalar> for SphereAtOrigin {
    type Output = SphereAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return SphereAtOrigin::from_groups(/* e3215, e1234 */ (self.group0() * Simd32x2::from(other[e12345])));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for SphereAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other.group0()[3]));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for SphereAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[3], other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for SphereAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[0]])),
        );
    }
}
impl AntiWedge<Circle> for SphereAtOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        4        0
    // no simd        3       12        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
            // e23, e31, e12
            ((Simd32x3::from(self.group0()[0]) * other.group0()) + (Simd32x3::from(self.group0()[1]) * other.group2())),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for SphereAtOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        4        0
    // no simd        3       12        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e23, e31, e12
            ((Simd32x3::from(self.group0()[0]) * other.group0()) + (Simd32x3::from(self.group0()[1]) * other.group2())),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for SphereAtOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for SphereAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            ((Simd32x3::from(self.group0()[0]) * other.group0()) + (Simd32x3::from(self.group0()[1]) * other.group1())),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for SphereAtOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[0]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * other.group1()),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for SphereAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            ((Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])) + (Simd32x3::from(self.group0()[1]) * other.group1())),
        );
    }
}
impl AntiWedge<Dipole> for SphereAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group2()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group2()[2])),
                (self.group0()[1] * other.group1()[3]),
            ]),
            // e5
            (self.group0()[0] * other.group1()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for SphereAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group1()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group1()[2])),
                (self.group0()[1] * other.group0()[3]),
            ]),
            // e5
            (self.group0()[0] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for SphereAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        6        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
            // e5
            (self.group0()[0] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for SphereAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(Simd32x3::from(self.group0()[0]) * other.group0()) + (Simd32x3::from(self.group0()[1]) * other.group1())),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for SphereAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        4        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (other.group0() * Simd32x4::from([self.group0()[0], self.group0()[0], self.group0()[0], self.group0()[1]]) * Simd32x4::from([-1.0, -1.0, -1.0, 1.0])),
            // e5
            (self.group0()[0] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for SphereAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(Simd32x3::from(self.group0()[0]) * other.group0()) + (Simd32x3::from(self.group0()[1]) * other.group2())),
        );
    }
}
impl AntiWedge<DualNum> for SphereAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        3        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[0])]),
            // e23, e31, e12, e3215
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[1])]),
            // e15, e25, e35, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[1] * other.group0()[1])]),
        );
    }
}
impl AntiWedge<FlatOrigin> for SphereAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (swizzle!(self.group0(), 1, 0) * Simd32x2::from(other[e45]) * Simd32x2::from([1.0, -1.0])));
    }
}
impl AntiWedge<FlatPoint> for SphereAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        6        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e5
            (self.group0()[0] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for SphereAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<Flector> for SphereAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[1]) * other.group1()),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[1]) * other.group0()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for SphereAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ (Simd32x4::from(self.group0()[1]) * swizzle!(other.group0(), 3, 0, 1, 2)));
    }
}
impl AntiWedge<FlectorOnOrigin> for SphereAtOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            (Simd32x4::from(self.group0()[1]) * swizzle!(other.group0(), 1, 2, 3, 0)),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[0]) * swizzle!(other.group0(), 1, 2, 3, 0) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Horizon> for SphereAtOrigin {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self.group0()[1] * other[e3215]));
    }
}
impl AntiWedge<Infinity> for SphereAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[1] * other[e5]));
    }
}
impl AntiWedge<Line> for SphereAtOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[1]) * other.group1()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * other.group0()),
        );
    }
}
impl AntiWedge<LineAtInfinity> for SphereAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<LineOnOrigin> for SphereAtOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * other.group0()),
        );
    }
}
impl AntiWedge<Motor> for SphereAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        4        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        6        0
    //  no simd        0       12        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[1] * other.group1()[1]),
                (self.group0()[1] * other.group1()[2]),
                (self.group0()[0] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            (other.group0() * Simd32x4::from([self.group0()[0], self.group0()[0], self.group0()[0], self.group0()[1]])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for SphereAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self.group0()[1]) * other.group0()));
    }
}
impl AntiWedge<MotorOnOrigin> for SphereAtOrigin {
    type Output = VersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOddAtOrigin::from_groups(
            // e41, e42, e43, e3215
            (other.group0() * Simd32x4::from([self.group0()[1], self.group0()[1], self.group0()[1], self.group0()[0]])),
            // e15, e25, e35, e1234
            (other.group0() * Simd32x4::from([self.group0()[0], self.group0()[0], self.group0()[0], self.group0()[1]])),
        );
    }
}
impl AntiWedge<MultiVector> for SphereAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       18        0
    //    simd3        1        6        0
    // Totals...
    // yes simd        6       24        0
    //  no simd        8       36        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([((self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other[e1])), 0.0]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group4()[0])),
                (-(self.group0()[0] * other.group3()[1]) + (self.group0()[1] * other.group4()[1])),
                (-(self.group0()[0] * other.group3()[2]) + (self.group0()[1] * other.group4()[2])),
                (self.group0()[1] * other.group3()[3]),
            ]),
            // e5
            (self.group0()[0] * other.group3()[3] * -1.0),
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[1] * other.group6()[0]),
                (self.group0()[1] * other.group6()[1]),
                (self.group0()[1] * other.group6()[2]),
                0.0,
            ]),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]])),
            // e23, e31, e12
            ((Simd32x3::from(self.group0()[0]) * other.group7()) + (Simd32x3::from(self.group0()[1]) * other.group8())),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group9()[0]) + (self.group0()[1] * other[e45]))]),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]])),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]) * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group0()[1] * other.group0()[1]), 0.0, 0.0, 0.0]),
            // e3215
            (self.group0()[0] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for SphereAtOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiLineOnOrigin::from_groups(/* e23, e31, e12 */ (Simd32x3::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for SphereAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullSphereAtOrigin> for SphereAtOrigin {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlatOrigin::from_groups(/* e321 */ (self.group0()[0] * other[e1234] * -1.0));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for SphereAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (Simd32x4::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for SphereAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(self.group0()[0]) * swizzle!(other.group0(), 3, 0, 1, 2) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<Origin> for SphereAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other[e4]));
    }
}
impl AntiWedge<Plane> for SphereAtOrigin {
    type Output = CircleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return CircleOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for SphereAtOrigin {
    type Output = CircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        3        0
    // no simd        0        9        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return CircleAtOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self.group0()[1]) * other.group0()),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[0]) * other.group0() * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<RoundPoint> for SphereAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ ((self.group0()[0] * other.group0()[3]) + (self.group0()[1] * other[e2])));
    }
}
impl AntiWedge<RoundPointAtOrigin> for SphereAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])));
    }
}
impl AntiWedge<Sphere> for SphereAtOrigin {
    type Output = CircleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        0        2        0
    // Totals...
    // yes simd        1        7        0
    //  no simd        1       11        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return CircleOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[1] * other.group0()[2]),
                (-(self.group0()[0] * other[e4315]) + (self.group0()[1] * other.group0()[3])),
            ]),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for SphereAtOrigin {
    type Output = AntiFlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiFlatOrigin::from_groups(/* e321 */ (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])));
    }
}
impl AntiWedge<SphereOnOrigin> for SphereAtOrigin {
    type Output = CircleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        2        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        4        0
    //  no simd        0       14        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return CircleOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (other.group0() * Simd32x4::from([self.group0()[1], self.group0()[1], self.group0()[1], self.group0()[0]]) * Simd32x4::from([1.0, 1.0, 1.0, -1.0])),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[0]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<VersorEven> for SphereAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       16        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[1] * other.group1()[1]),
                (self.group0()[1] * other.group1()[2]),
                ((self.group0()[0] * other.group3()[3]) + (self.group0()[1] * other.group2()[3])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group2()[0])),
                ((self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group2()[1])),
                ((self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group2()[2])),
                (self.group0()[0] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (self.group0()[1] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for SphereAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       16        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[1] * other.group1()[0]),
                (self.group0()[1] * other.group1()[1]),
                (self.group0()[1] * other.group1()[2]),
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other.group2()[3])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group2()[0])),
                ((self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group2()[1])),
                ((self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group2()[2])),
                (self.group0()[0] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (self.group0()[1] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for SphereAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        8        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        9        0
    //  no simd        0       12        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[1] * other.group2()[0]),
                (self.group0()[1] * other.group2()[1]),
                (self.group0()[1] * other.group2()[2]),
                (self.group0()[0] * other.group0()[0]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (self.group0()[1] * other.group0()[0]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for SphereAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        1        2        0
    // no simd        4        8        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(self.group0()[0]) * other.group0()) + (Simd32x4::from(self.group0()[1]) * other.group1())),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for SphereAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        4        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        6        0
    //  no simd        0       12        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (other.group1() * Simd32x4::from([self.group0()[1], self.group0()[1], self.group0()[1], self.group0()[0]])),
            // e23, e31, e12, e3215
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (self.group0()[1] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for SphereAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        1        2        0
    // no simd        4        8        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group2()[3]]))
                + (Simd32x4::from(self.group0()[1]) * other.group1())),
        );
    }
}
impl AntiWedge<VersorOdd> for SphereAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        4       14        0
    //  no simd        4       20        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[1] * other.group3()[0]),
                (self.group0()[1] * other.group3()[1]),
                (self.group0()[1] * other.group3()[2]),
                (-(self.group0()[0] * other.group2()[3]) + (self.group0()[1] * other.group3()[3])),
            ]),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group2()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group2()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group2()[2])),
                (self.group0()[1] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for SphereAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        4       14        0
    //  no simd        4       20        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[1] * other.group2()[0]),
                (self.group0()[1] * other.group2()[1]),
                (self.group0()[1] * other.group2()[2]),
                (-(self.group0()[0] * other.group1()[3]) + (self.group0()[1] * other.group2()[3])),
            ]),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group1()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group1()[2])),
                (self.group0()[1] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for SphereAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[1]) * other.group2()),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[3], other.group1()[3]])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for SphereAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        1        2        0
    // no simd        4        8        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (-(Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[3], other.group0()[0], other.group0()[1], other.group0()[2]]))
                + (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]]))),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for SphereAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        6        0
    // no simd        0       24        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (swizzle!(other.group1(), 1, 2, 3, 0)
                * Simd32x4::from([self.group0()[1], self.group0()[1], self.group0()[1], self.group0()[0]])
                * Simd32x4::from([1.0, 1.0, 1.0, -1.0])),
            // e235, e315, e125, e5
            (Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[3], other.group0()[3]]) * Simd32x4::from(-1.0)),
            // e1, e2, e3, e4
            (other.group0() * Simd32x4::from([self.group0()[0], self.group0()[0], self.group0()[0], self.group0()[1]]) * Simd32x4::from([-1.0, -1.0, -1.0, 1.0])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for SphereAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        1        2        0
    // no simd        4        8        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (-(Simd32x4::from(self.group0()[0]) * Simd32x4::from([other.group2()[3], other.group0()[0], other.group0()[1], other.group0()[2]]))
                + (Simd32x4::from(self.group0()[1]) * Simd32x4::from([other.group1()[3], other.group2()[0], other.group2()[1], other.group2()[2]]))),
        );
    }
}
impl InfixAntiWedge for SphereOnOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for SphereOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        9        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group1()[1]) * -1.0),
                    ((self.group0()[0] * other.group1()[2]) * -1.0),
                    ((self.group0()[1] * other.group1()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for SphereOnOrigin {
    type Output = AntiCircleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       12        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiCircleOnOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiDualNum> for SphereOnOrigin {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiFlatPoint::from_groups(/* e235, e315, e125, e321 */ (self.group0() * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatOrigin> for SphereOnOrigin {
    type Output = AntiLineOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiLineOnOrigin::from_groups(
            // e23, e31, e12
            (Simd32x3::from(other[e321]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for SphereOnOrigin {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        4        8        0
    //  no simd        6       12        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (-(Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e15, e25, e35
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for SphereOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       16        0
    //  no simd        9       19        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) * -1.0),
                    ((self.group0()[1] * other.group0()[3]) * -1.0),
                    ((self.group0()[2] * other.group0()[3]) * -1.0),
                    ((self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for SphereOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0] * -1.0),
            (self.group0()[1] * other.group0()[0] * -1.0),
            (self.group0()[2] * other.group0()[0] * -1.0),
            ((self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<AntiLine> for SphereOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for SphereOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<AntiMotor> for SphereOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group1()[3])),
            // e1, e2, e3, e5
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for SphereOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for SphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for SphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for SphereOnOrigin {
    type Output = SphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return SphereOnOrigin::from_groups(/* e4235, e4315, e4125, e1234 */ (self.group0() * Simd32x4::from(other[e12345])));
    }
}
impl AntiWedge<AntiSphereOnOrigin> for SphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for SphereOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2       15        0
    //  no simd        5       18        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3] * -1.0),
                (self.group0()[1] * other.group1()[3] * -1.0),
                (self.group0()[2] * other.group1()[3] * -1.0),
                0.0,
            ]),
            // e4, e1, e2, e3
            ((swizzle!(self.group0(), 2, 1, 2, 0) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                    ((self.group0()[2] * other.group1()[1]) * -1.0),
                    ((self.group0()[0] * other.group1()[2]) * -1.0),
                    ((self.group0()[1] * other.group1()[0]) * -1.0),
                ])),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for SphereOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2       15        0
    //  no simd        5       18        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[2])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3] * -1.0),
                (self.group0()[1] * other.group0()[3] * -1.0),
                (self.group0()[2] * other.group0()[3] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Circle> for SphereOnOrigin {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        3        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5       11        0
    //  no simd       14       24        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            ((Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group0()[3] * other.group2()[0]),
                    (self.group0()[3] * other.group2()[1]),
                    (self.group0()[3] * other.group2()[2]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35
            (-(swizzle!(other.group2(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group2(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for SphereOnOrigin {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        3        5        0
    // Totals...
    // yes simd        5       11        0
    //  no simd       11       21        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            ((Simd32x3::from(self.group0()[3]) * other.group1()) + (swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35
            (-(swizzle!(other.group2(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group2(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for SphereOnOrigin {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd3        1        3        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3        9        0
    //  no simd        8       18        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 0, 1, 2, 2) * swizzle!(other.group0(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for SphereOnOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        5        0
    // no simd        6       15        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group1()),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for SphereOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for SphereOnOrigin {
    type Output = DipoleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        2        4        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        9       18        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return DipoleOrthogonalOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12
            (-(Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])) + (Simd32x3::from(self.group0()[3]) * other.group1())),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<Dipole> for SphereOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3       11        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        5       13        0
    //  no simd       11       19        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group1()[1]) * -1.0),
                    ((self.group0()[0] * other.group1()[2]) * -1.0),
                    ((self.group0()[1] * other.group1()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for SphereOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       10        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for SphereOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       13        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                ((self.group0()[3] * other.group1()[1]) - (self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for SphereOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for SphereOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for SphereOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        3        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        4        7        0
    //  no simd       10       19        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 3, 3, 3, 2) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[2]]))
                + (swizzle!(self.group0(), 1, 2, 0, 0) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[0]]))
                + (Simd32x4::from([-1.0, -1.0, -1.0, 1.0])
                    * swizzle!(self.group0(), 2, 0, 1, 1)
                    * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[0], other.group0()[1]]))),
            // e5
            (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for SphereOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        5        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[0])]),
            // e23, e31, e12, e45
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlatOrigin> for SphereOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for SphereOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        3        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        4        0
    //  no simd        2        7        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e5
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for SphereOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Flector> for SphereOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       17        0
    //  no simd        5       20        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for SphereOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for SphereOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (self.group0()[3] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[2] * other.group0()[2])),
                ((self.group0()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0]),
            ]),
        );
    }
}
impl AntiWedge<Horizon> for SphereOnOrigin {
    type Output = AntiFlatPoint;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlatPoint::from_groups(/* e235, e315, e125, e321 */ (self.group0() * Simd32x4::from(other[e3215])));
    }
}
impl AntiWedge<Infinity> for SphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for SphereOnOrigin {
    type Output = Dipole;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        1        3        0
    // Totals...
    // yes simd        3        9        0
    //  no simd        5       15        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return Dipole::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group0()[3]) * other.group0()),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineAtInfinity> for SphereOnOrigin {
    type Output = AntiLine;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        3        0
    // no simd        3        9        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiLine::from_groups(
            // e23, e31, e12
            (Simd32x3::from(self.group0()[3]) * other.group0()),
            // e15, e25, e35
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for SphereOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Motor> for SphereOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       17        0
    //  no simd        5       20        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for SphereOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        3       10        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for SphereOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e1234, e4235, e4315, e4125
            (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for SphereOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       30        0
    //    simd3        3        7        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       19       41        0
    //  no simd       34       67        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group0()[3] * other[e1]) + (self.group0()[2] * other.group1()[2]) + (self.group0()[0] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group4()[0], other.group4()[1], other.group4()[2], other.group3()[3]]))
                + (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group5()[2], other.group5()[0], other.group5()[1], other.group3()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group5()[1]) * -1.0),
                    ((self.group0()[0] * other.group5()[2]) * -1.0),
                    ((self.group0()[1] * other.group5()[0]) * -1.0),
                    ((self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
                ])),
            // e5
            (-(self.group0()[2] * other.group4()[2]) - (self.group0()[0] * other.group4()[0]) - (self.group0()[1] * other.group4()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group7()[1], other.group7()[2], other.group7()[0], other.group6()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group6()[0]) + (self.group0()[1] * other.group7()[2])),
                    ((self.group0()[3] * other.group6()[1]) + (self.group0()[2] * other.group7()[0])),
                    ((self.group0()[3] * other.group6()[2]) + (self.group0()[0] * other.group7()[1])),
                    (-(self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
                ])),
            // e15, e25, e35
            (-(swizzle!(other.group8(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group8(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e23, e31, e12
            (-(Simd32x3::from(other.group6()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])) + (Simd32x3::from(self.group0()[3]) * other.group8())),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                (self.group0()[3] * other[e45]),
            ]),
            // e423, e431, e412
            (-(Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e235, e315, e125
            (Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e1234, e4235, e4315, e4125
            (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other.group0()[1])),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for SphereOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for SphereOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for SphereOnOrigin {
    type Output = NullCircleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullCircleAtOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(other[e1234]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]) * Simd32x3::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for SphereOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ Simd32x3::from([
            ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
            (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for SphereOnOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3] * -1.0),
            (self.group0()[1] * other.group0()[3] * -1.0),
            (self.group0()[2] * other.group0()[3] * -1.0),
            ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Plane> for SphereOnOrigin {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        7        0
    //    simd3        0        2        0
    // Totals...
    // yes simd        3        9        0
    //  no simd        3       13        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return Circle::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]])),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for SphereOnOrigin {
    type Output = CircleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        3        0
    // no simd        3        9        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return CircleOnOrigin::from_groups(
            // e423, e431, e412
            (Simd32x3::from(self.group0()[3]) * other.group0()),
            // e415, e425, e435
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<RoundPoint> for SphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self.group0()[3] * other[e2]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for SphereOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[1]));
    }
}
impl AntiWedge<Sphere> for SphereOnOrigin {
    type Output = Circle;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        7        0
    //    simd3        1        3        0
    // Totals...
    // yes simd        4       10        0
    //  no simd        6       16        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return Circle::from_groups(
            // e423, e431, e412
            (-(Simd32x3::from(other[e4315]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for SphereOnOrigin {
    type Output = CircleOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //    simd3        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0       11        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return CircleOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from([other.group0()[1], other.group0()[1], other.group0()[1], other.group0()[0]]) * Simd32x4::from([-1.0, -1.0, -1.0, 1.0])),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[0]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for SphereOnOrigin {
    type Output = CircleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        1        2        0
    // Totals...
    // yes simd        4        8        0
    //  no simd        6       12        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return CircleOnOrigin::from_groups(
            // e423, e431, e412
            (-(Simd32x3::from(other.group0()[3]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group0()[0], other.group0()[1], other.group0()[2]]))),
            // e415, e425, e435
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for SphereOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       23        0
    //    simd4        3        3        0
    // Totals...
    // yes simd        8       26        0
    //  no simd       17       35        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group3()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group3()[0]) + (self.group0()[1] * other.group3()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group0()[3] * other.group2()[0]),
                    (self.group0()[3] * other.group2()[1]),
                    (self.group0()[3] * other.group2()[2]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for SphereOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       26        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                ((self.group0()[3] * other.group1()[1]) - (self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group2()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for SphereOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       22        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       23        0
    //  no simd       11       26        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group2()[3]) + (self.group0()[2] * other.group0()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[2])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group0(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group0()[3] * other.group2()[0]),
                    (self.group0()[3] * other.group2()[1]),
                    (self.group0()[3] * other.group2()[2]),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (self.group0()[3] * other.group0()[0]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for SphereOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[2]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for SphereOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(self.group0(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[1] * other.group0()[2])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[2] * other.group0()[0])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e1234, e4235, e4315, e4125
            (swizzle!(self.group0(), 3, 0, 1, 2) * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for SphereOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       21        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       12       25        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group0()[1]) * -1.0),
                    ((self.group0()[0] * other.group0()[2]) * -1.0),
                    ((self.group0()[1] * other.group0()[0]) * -1.0),
                    ((self.group0()[2] * other.group2()[2]) + (self.group0()[0] * other.group2()[0]) + (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (-(self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group1()[0])),
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group1()[1])),
                (-(self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group1()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOdd> for SphereOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       27        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       11       29        0
    //  no simd       17       35        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (-(self.group0()[0] * other.group2()[3]) + (self.group0()[3] * other.group3()[0])),
                (-(self.group0()[1] * other.group2()[3]) + (self.group0()[3] * other.group3()[1])),
                (-(self.group0()[2] * other.group2()[3]) + (self.group0()[3] * other.group3()[2])),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group3()[2]) + (self.group0()[2] * other.group3()[1])),
                ((self.group0()[0] * other.group3()[2]) - (self.group0()[2] * other.group3()[0])),
                (-(self.group0()[0] * other.group3()[1]) + (self.group0()[1] * other.group3()[0])),
                (self.group0()[3] * other.group3()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group3()[3]),
                (self.group0()[1] * other.group3()[3]),
                (self.group0()[2] * other.group3()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[2] * other.group1()[1]) * -1.0),
                    ((self.group0()[0] * other.group1()[2]) * -1.0),
                    ((self.group0()[1] * other.group1()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for SphereOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       26        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (-(self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group2()[0])),
                (-(self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group2()[1])),
                (-(self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group2()[2])),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (self.group0()[3] * other.group2()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[3] * other.group0()[3]) + (self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for SphereOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       26        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group0()[1] * other.group2()[2]) + (self.group0()[2] * other.group2()[1])),
                ((self.group0()[0] * other.group2()[2]) - (self.group0()[2] * other.group2()[0])),
                (-(self.group0()[0] * other.group2()[1]) + (self.group0()[1] * other.group2()[0])),
                (self.group0()[3] * other.group2()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[1]) + (self.group0()[1] * other.group1()[2]) - (self.group0()[2] * other.group1()[1])),
                ((self.group0()[3] * other.group0()[2]) - (self.group0()[0] * other.group1()[2]) + (self.group0()[2] * other.group1()[0])),
                ((self.group0()[3] * other.group0()[3]) + (self.group0()[0] * other.group1()[1]) - (self.group0()[1] * other.group1()[0])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for SphereOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        4       14        0
    //  no simd        4       20        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[3]]) * Simd32x4::from([-1.0, -1.0, -1.0, 1.0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for SphereOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       15        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       16        0
    //  no simd        9       19        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (-(self.group0()[0] * other.group1()[0]) + (self.group0()[3] * other.group1()[1])),
                (-(self.group0()[1] * other.group1()[0]) + (self.group0()[3] * other.group1()[2])),
                (-(self.group0()[2] * other.group1()[0]) + (self.group0()[3] * other.group1()[3])),
                0.0,
            ]),
            // e415, e425, e435, e4
            ((swizzle!(self.group0(), 2, 0, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[3], other.group1()[1], other.group0()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group1()[3]) * -1.0),
                    ((self.group0()[2] * other.group1()[1]) * -1.0),
                    ((self.group0()[0] * other.group1()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[2]) + (self.group0()[0] * other.group0()[0]) + (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for SphereOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        2        6        0
    // Totals...
    // yes simd        4       12        0
    //  no simd       10       30        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group1()[3]]) * Simd32x4::from([-1.0, -1.0, -1.0, 1.0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 3, 3, 3, 2) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[2]]))
                + (swizzle!(self.group0(), 1, 2, 0, 0) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[0]]))
                + (Simd32x4::from([-1.0, -1.0, -1.0, 1.0])
                    * swizzle!(self.group0(), 2, 0, 1, 1)
                    * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[0], other.group0()[1]]))),
        );
    }
}
impl InfixAntiWedge for VersorEven {}
impl AntiWedge<AntiCircleOnOrigin> for VersorEven {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorEven {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group1()[2], self.group2()[2], self.group2()[0], self.group2()[1]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                    ((self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDualNum> for VersorEven {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        1        6        0
    //  no simd        1        9        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                ((self.group0()[3] * other.group0()[1]) + (self.group3()[3] * other.group0()[0])),
            ]),
            // e15, e25, e35, e3215
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for VersorEven {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other[e321]) * Simd32x4::from([self.group0()[3], self.group1()[0], self.group1()[1], self.group1()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorEven {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlector> for VersorEven {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e1, e2, e3, e5
            ((Simd32x4::from(self.group0()[3]) * other.group1())
                - (swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for VersorEven {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            ((self.group0()[3] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
            ((self.group0()[3] * other.group0()[2]) + (self.group1()[1] * other.group0()[0])),
            ((self.group0()[3] * other.group0()[3]) + (self.group1()[2] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorEven {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for VersorEven {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiMotor> for VersorEven {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group3()[3]]))
                + (Simd32x4::from(self.group0()[3]) * other.group0())
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group1()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for VersorEven {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for VersorEven {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for VersorEven {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiScalar> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other[e12345])),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other[e12345])),
            // e235, e315, e125, e5
            (self.group2() * Simd32x4::from(other[e12345])),
            // e1, e2, e3, e4
            (self.group3() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for VersorEven {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorEven {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group0()[3]) * other.group0())
                + (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group2()[0] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group2()[1] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group2()[2] * other.group1()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorEven {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group1()[2], self.group2()[2], self.group2()[0], self.group2()[1]]))
                + (Simd32x4::from(self.group0()[3]) * other.group1())
                + Simd32x4::from([
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3])),
                ])),
        );
    }
}
impl AntiWedge<Circle> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       21       32        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       22       34        0
    //  no simd       25       40        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[2] * other.group0()[1])
                        + (self.group2()[1] * other.group0()[2])
                        + (self.group1()[3] * other.group1()[0])
                        + (self.group1()[0] * other.group1()[3])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2])
                        + (self.group1()[3] * other.group1()[1])
                        + (self.group1()[1] * other.group1()[3])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group2()[1] * other.group0()[0])
                        + (self.group2()[0] * other.group0()[1])
                        + (self.group1()[3] * other.group1()[2])
                        + (self.group1()[2] * other.group1()[3])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       32        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       19       33        0
    //  no simd       22       36        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[2] * other.group0()[1])
                        + (self.group2()[1] * other.group0()[2])
                        + (self.group1()[3] * other.group1()[0])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2])
                        + (self.group1()[3] * other.group1()[1])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group2()[1] * other.group0()[0])
                        + (self.group2()[0] * other.group0()[1])
                        + (self.group1()[3] * other.group1()[2])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       16       28        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group1()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group1()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorEven {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group2()[1] * other.group0()[2]) - (self.group0()[1] * other.group1()[2])
                    + (self.group0()[2] * other.group1()[1])),
                ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group2()[0] * other.group0()[1]) - (self.group0()[0] * other.group1()[1])
                    + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[3] * other.group1()[0]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[3] * other.group1()[2]) + (self.group2()[0] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorEven {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       16       28        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2])
                        + (self.group0()[2] * other.group1()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])
                        - (self.group0()[2] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1])
                        + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<Dipole> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        9       17        0
    //  no simd        9       20        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[3] * other.group1()[3])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[3] * other.group0()[3])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorEven {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       10        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        6       11        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[3] * other.group0()[3])
                    - (self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorEven {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group1()[3] * other.group0()[3])
                    - (self.group2()[0] * other.group0()[0])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from(0.0),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorEven {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DualNum> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        3        0
    // Totals...
    // yes simd        1        8        0
    //  no simd        1       17        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[1]),
                (self.group2()[1] * other.group0()[1]),
                (self.group2()[2] * other.group0()[1]),
                ((self.group0()[3] * other.group0()[0]) + (self.group2()[3] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            (self.group3() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorEven {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        3        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([(self.group1()[3] * other[e45] * -1.0), 0.0, 0.0, 0.0]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other[e45])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<FlatPoint> for VersorEven {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[3] * other.group0()[3])
                    - (self.group0()[2] * other.group0()[2])
                    - (self.group0()[0] * other.group0()[0])
                    - (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorEven {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Flector> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       20        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       19       25        0
    //  no simd       31       40        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(other.group1(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group3()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group3()[2] * other.group1()[2]) + (self.group3()[1] * other.group1()[1]) + (self.group3()[0] * other.group1()[0])
                        - (self.group1()[3] * other.group0()[3])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e45
            ((self.group0() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[3]]))
                - (swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group1(), 0, 1, 2, 2))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[1] * other.group1()[1]) - (self.group1()[0] * other.group1()[0]))])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group1()[1])
                    + (self.group2()[1] * other.group1()[2])
                    + (self.group0()[3] * other.group0()[0])
                    + (self.group1()[0] * other.group1()[3])),
                ((self.group2()[2] * other.group1()[0]) - (self.group2()[0] * other.group1()[2]) + (self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group1()[3])),
                (-(self.group2()[1] * other.group1()[0])
                    + (self.group2()[0] * other.group1()[1])
                    + (self.group0()[3] * other.group0()[2])
                    + (self.group1()[2] * other.group1()[3])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorEven {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                ((self.group3()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group0()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       21        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       23        0
    //  no simd       15       29        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 3, 1, 2, 0) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[3]]))
                + (swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group3()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group3()[1] * other.group0()[2]) + (self.group3()[0] * other.group0()[1]))])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                (self.group1()[3] * other.group0()[3] * -1.0),
                (-(self.group1()[2] * other.group0()[3]) - (self.group1()[1] * other.group0()[2]) + (self.group0()[3] * other.group0()[0])
                    - (self.group1()[0] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[3]) - (self.group2()[2] * other.group0()[2])),
                (-(self.group2()[0] * other.group0()[3]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[1] * other.group0()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (self.group0()[3] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Horizon> for VersorEven {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group3()[3]])),
            // e15, e25, e35, e3215
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<Infinity> for VersorEven {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       20        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       21        0
    //  no simd       13       24        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group1()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group1()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorEven {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e1, e2, e3, e5
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<LineOnOrigin> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<Motor> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       21        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       16       26        0
    //  no simd       28       41        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group1())
                + (self.group2() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])),
                ])),
            // e1, e2, e3, e4
            ((self.group3() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group1()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group1()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorEven {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group0(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0]))])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       12       28        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                ((self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            ((other.group0() * Simd32x4::from([self.group1()[3], self.group1()[3], self.group1()[3], self.group3()[3]]))
                + Simd32x4::from([
                    (self.group3()[0] * other.group0()[3]),
                    (self.group3()[1] * other.group0()[3]),
                    (self.group3()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MultiVector> for VersorEven {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       45       60        0
    //    simd3        7       10        0
    //    simd4        6        8        0
    // Totals...
    // yes simd       58       78        0
    //  no simd       90      122        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group3()[3] * other[e45])
                    + (self.group3()[2] * other.group9()[3])
                    + (self.group3()[1] * other.group9()[2])
                    + (self.group3()[0] * other.group9()[1])
                    + (self.group2()[3] * other.group9()[0])
                    - (self.group2()[2] * other.group3()[2])
                    - (self.group2()[1] * other.group3()[1])
                    - (self.group2()[0] * other.group3()[0])
                    - (self.group1()[3] * other.group3()[3])
                    - (self.group1()[2] * other.group5()[2])
                    - (self.group1()[1] * other.group5()[1])
                    - (self.group1()[0] * other.group5()[0])
                    + (self.group0()[3] * other.group0()[0])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                (self.group0()[3] * other.group0()[1]),
            ]),
            // e1, e2, e3, e4
            ((self.group3() * Simd32x4::from(other.group0()[1])) + (Simd32x4::from(self.group0()[3]) * other.group1())
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group8()[2], other.group8()[0], other.group8()[1], other.group6()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[2] * other.group7()[1])
                        + (self.group2()[1] * other.group7()[2])
                        + (self.group1()[3] * other.group6()[0])
                        + (self.group1()[0] * other.group6()[3])
                        + (self.group0()[2] * other.group8()[1])),
                    ((self.group2()[2] * other.group7()[0]) - (self.group2()[0] * other.group7()[2])
                        + (self.group1()[3] * other.group6()[1])
                        + (self.group1()[1] * other.group6()[3])
                        + (self.group0()[0] * other.group8()[2])),
                    (-(self.group2()[1] * other.group7()[0])
                        + (self.group2()[0] * other.group7()[1])
                        + (self.group1()[3] * other.group6()[2])
                        + (self.group1()[2] * other.group6()[3])
                        + (self.group0()[1] * other.group8()[0])),
                    (-(self.group1()[2] * other.group7()[2])
                        - (self.group1()[1] * other.group7()[1])
                        - (self.group1()[0] * other.group7()[0])
                        - (self.group0()[0] * other.group6()[0])
                        - (self.group0()[1] * other.group6()[1])),
                ])),
            // e5
            ((self.group2()[3] * other.group0()[1])
                - (self.group2()[2] * other.group6()[2])
                - (self.group2()[1] * other.group6()[1])
                - (self.group2()[0] * other.group6()[0])
                - (self.group1()[2] * other.group8()[2])
                - (self.group1()[1] * other.group8()[1])
                + (self.group0()[3] * other[e1])
                - (self.group1()[0] * other.group8()[0])),
            // e41, e42, e43, e45
            ((Simd32x4::from(self.group0()[3]) * other.group3())
                - (swizzle!(other.group9(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group9()[0]) + (self.group0()[2] * other.group9()[2])),
                    ((self.group1()[1] * other.group9()[0]) + (self.group0()[0] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[0]) + (self.group0()[1] * other.group9()[1])),
                    (-(self.group1()[1] * other.group9()[2]) - (self.group1()[0] * other.group9()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * other.group4())
                + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                + Simd32x3::from([
                    (-(self.group2()[2] * other.group9()[2]) + (self.group2()[1] * other.group9()[3])),
                    ((self.group2()[2] * other.group9()[1]) - (self.group2()[0] * other.group9()[3])),
                    (-(self.group2()[1] * other.group9()[1]) + (self.group2()[0] * other.group9()[2])),
                ])),
            // e23, e31, e12
            ((Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))
                - (Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))
                + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * other.group5())),
            // e415, e425, e435, e321
            ((Simd32x4::from(self.group0()[3]) * other.group6()) + (self.group1() * Simd32x4::from(other.group0()[1]))),
            // e423, e431, e412
            ((Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])) + (Simd32x3::from(self.group0()[3]) * other.group7())),
            // e235, e315, e125
            ((Simd32x3::from(self.group0()[3]) * other.group8()) + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[3]) * other.group9()),
            // e3215
            (self.group0()[3] * other[e45]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorEven {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorEven {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorEven {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]])),
            // e23, e31, e12, e1234
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorEven {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group1()[2], self.group2()[2], self.group2()[0], self.group2()[1]]))
                + (swizzle!(other.group0(), 3, 2, 0, 1) * Simd32x4::from([self.group0()[3], self.group2()[1], self.group2()[2], self.group2()[0]]))
                + Simd32x4::from([(-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorEven {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group2()[3]]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<Origin> for VersorEven {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       23        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       11       26        0
    //  no simd       17       35        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group3()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[1]) * -1.0),
                    ((self.group3()[2] * other.group0()[2]) + (self.group3()[0] * other.group0()[0]) + (self.group3()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group0(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[3]),
                    (self.group0()[2] * other.group0()[3]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       27        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                ((self.group3()[2] * other.group0()[2]) + (self.group3()[0] * other.group0()[0]) + (self.group3()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for VersorEven {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e5
            (self.group0()[3] * other[e2]),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for VersorEven {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (Simd32x2::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Scalar> for VersorEven {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Scalar) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[scalar]));
    }
}
impl AntiWedge<Sphere> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       27        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       15       31        0
    //  no simd       24       43        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other[e4315]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + (swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group3()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[1]) * -1.0),
                    ((self.group3()[2] * other.group0()[2]) + (self.group3()[1] * other.group0()[1]) + (self.group3()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group0(), 0, 1, 2, 2))
                + Simd32x4::from([
                    ((self.group2()[0] * other[e4315]) + (self.group0()[0] * other.group0()[3])),
                    ((self.group2()[1] * other[e4315]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group2()[2] * other[e4315]) + (self.group0()[2] * other.group0()[3])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                (self.group0()[3] * other[e4315]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorEven {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       16        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[1] * other.group0()[1]),
                (self.group1()[2] * other.group0()[1]),
                ((self.group2()[3] * other.group0()[1]) + (self.group3()[3] * other.group0()[0])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) + (self.group2()[0] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[0]) + (self.group2()[2] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
            ]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       23        0
    //    simd4        3        3        0
    // Totals...
    // yes simd        8       26        0
    //  no simd       17       35        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 3, 3, 3, 2) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group3()[2]]))
                + (swizzle!(other.group0(), 1, 2, 0, 1) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group3()[1]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[1]) * -1.0),
                    ((self.group2()[3] * other.group0()[3]) + (self.group3()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group0(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group2()[0] * other.group0()[3]),
                    (self.group2()[1] * other.group0()[3]),
                    (self.group2()[2] * other.group0()[3]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       20       29        0
    //    simd4        7        8        0
    // Totals...
    // yes simd       27       37        0
    //  no simd       48       61        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e415, e425, e435, e321
            ((Simd32x4::from(self.group0()[3]) * other.group1()) + (self.group1() * Simd32x4::from(other.group0()[3]))),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group2())
                + (self.group2() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2])
                        - (self.group2()[1] * other.group1()[1])
                        - (self.group2()[0] * other.group1()[0])
                        - (self.group1()[2] * other.group2()[2])
                        - (self.group1()[1] * other.group2()[1])
                        - (self.group1()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            ((self.group3() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (Simd32x4::from(self.group0()[3]) * other.group3())
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2])
                        + (self.group1()[3] * other.group1()[0])
                        + (self.group1()[0] * other.group1()[3])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[0])
                        + (self.group1()[3] * other.group1()[1])
                        + (self.group1()[1] * other.group1()[3])
                        + (self.group0()[0] * other.group2()[2])),
                    ((self.group2()[0] * other.group0()[1])
                        + (self.group1()[3] * other.group1()[2])
                        + (self.group1()[2] * other.group1()[3])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       17       30        0
    //    simd4        6        6        0
    // Totals...
    // yes simd       23       36        0
    //  no simd       41       54        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group2())
                + (self.group2() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2])
                        - (self.group2()[1] * other.group1()[1])
                        - (self.group2()[0] * other.group1()[0])
                        - (self.group1()[2] * other.group2()[2])
                        - (self.group1()[1] * other.group2()[1])
                        - (self.group1()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            ((self.group3() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (other.group1() * Simd32x4::from([self.group1()[3], self.group1()[3], self.group1()[3], self.group0()[3]]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       20        0
    //    simd4        5        7        0
    // Totals...
    // yes simd       20       27        0
    //  no simd       35       48        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e415, e425, e435, e321
            ((Simd32x4::from(self.group0()[3]) * other.group1()) + (self.group1() * Simd32x4::from(other.group0()[0]))),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group2())
                + (self.group2() * Simd32x4::from(other.group0()[0]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2])
                        - (self.group2()[1] * other.group1()[1])
                        - (self.group2()[0] * other.group1()[0])
                        - (self.group1()[2] * other.group2()[2])
                        - (self.group1()[1] * other.group2()[1])
                        - (self.group1()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            ((self.group3() * Simd32x4::from(other.group0()[0]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group1()[0])
                        + (self.group1()[0] * other.group1()[3])
                        + (self.group0()[3] * other.group0()[1])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[3] * other.group1()[1])
                        + (self.group1()[1] * other.group1()[3])
                        + (self.group0()[3] * other.group0()[2])
                        + (self.group0()[0] * other.group2()[2])),
                    ((self.group1()[3] * other.group1()[2])
                        + (self.group1()[2] * other.group1()[3])
                        + (self.group0()[3] * other.group0()[3])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorEven {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       20        0
    //  no simd       15       26        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group2()[1], self.group2()[2], self.group2()[0], self.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                    (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorEven {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       29        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       16       32        0
    //  no simd       25       41        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                ((self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((self.group3() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (other.group1() * Simd32x4::from([self.group1()[3], self.group1()[3], self.group1()[3], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorEven {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       21        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       15       24        0
    //  no simd       21       33        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (Simd32x4::from(self.group0()[3]) * other.group2())
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2])
                        + (self.group0()[2] * other.group1()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])
                        - (self.group0()[2] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1])
                        + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOdd> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       24       33        0
    //    simd4        6        7        0
    // Totals...
    // yes simd       30       40        0
    //  no simd       48       61        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + (Simd32x4::from(self.group0()[3]) * other.group0())
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group2()[2]]))
                + (swizzle!(other.group3(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group3()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group3()[2] * other.group3()[2]) + (self.group3()[1] * other.group3()[1]) + (self.group3()[0] * other.group3()[0])
                        - (self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group3(), 0, 1, 2, 2))
                + (self.group0() * Simd32x4::from([other.group3()[3], other.group3()[3], other.group3()[3], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group2()[0] * other.group2()[3]) + (self.group0()[3] * other.group1()[0])),
                    ((self.group2()[1] * other.group2()[3]) + (self.group0()[3] * other.group1()[1])),
                    ((self.group2()[2] * other.group2()[3]) + (self.group0()[3] * other.group1()[2])),
                    (-(self.group1()[1] * other.group3()[1]) - (self.group1()[0] * other.group3()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group3()[1])
                    + (self.group2()[1] * other.group3()[2])
                    + (self.group0()[3] * other.group2()[0])
                    + (self.group1()[0] * other.group3()[3])),
                ((self.group2()[2] * other.group3()[0]) - (self.group2()[0] * other.group3()[2]) + (self.group0()[3] * other.group2()[1]) + (self.group1()[1] * other.group3()[3])),
                (-(self.group2()[1] * other.group3()[0])
                    + (self.group2()[0] * other.group3()[1])
                    + (self.group0()[3] * other.group2()[2])
                    + (self.group1()[2] * other.group3()[3])),
                (self.group0()[3] * other.group2()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group3()),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       30        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       23       36        0
    //  no simd       38       54        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + (swizzle!(other.group2(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group3()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    ((self.group3()[2] * other.group2()[2]) + (self.group3()[1] * other.group2()[1]) + (self.group3()[0] * other.group2()[0])
                        - (self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[3] * other.group0()[3])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e45
            ((self.group0() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group0()[3]]))
                - (swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group2(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group2()[0] * other.group1()[3]),
                    (self.group2()[1] * other.group1()[3]),
                    (self.group2()[2] * other.group1()[3]),
                    (-(self.group1()[1] * other.group2()[1]) - (self.group1()[0] * other.group2()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group2()[1])
                    + (self.group2()[1] * other.group2()[2])
                    + (self.group0()[3] * other.group1()[0])
                    + (self.group1()[0] * other.group2()[3])),
                ((self.group2()[2] * other.group2()[0]) - (self.group2()[0] * other.group2()[2]) + (self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group2()[3])),
                (-(self.group2()[1] * other.group2()[0])
                    + (self.group2()[0] * other.group2()[1])
                    + (self.group0()[3] * other.group1()[2])
                    + (self.group1()[2] * other.group2()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       19       27        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       23       32        0
    //  no simd       35       47        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group0()[3]]))
                + (swizzle!(other.group2(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group3()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group3()[2] * other.group2()[2]) + (self.group3()[1] * other.group2()[1]) + (self.group3()[0] * other.group2()[0])
                        - (self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        + (self.group0()[3] * other.group0()[0])
                        - (self.group0()[0] * other.group0()[1])
                        - (self.group0()[1] * other.group0()[2])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group2(), 0, 1, 2, 2))
                + (self.group0() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group1()[1] * other.group2()[1]) - (self.group1()[0] * other.group2()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group2()[1])
                    + (self.group2()[1] * other.group2()[2])
                    + (self.group0()[3] * other.group0()[1])
                    + (self.group1()[0] * other.group2()[3])),
                ((self.group2()[2] * other.group2()[0]) - (self.group2()[0] * other.group2()[2]) + (self.group0()[3] * other.group0()[2]) + (self.group1()[1] * other.group2()[3])),
                (-(self.group2()[1] * other.group2()[0])
                    + (self.group2()[0] * other.group2()[1])
                    + (self.group0()[3] * other.group0()[3])
                    + (self.group1()[2] * other.group2()[3])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorEven {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group3()[3]]))
                + (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group2()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group0()[3]) + (self.group2()[1] * other.group1()[3])),
                ((self.group0()[2] * other.group0()[3]) + (self.group2()[2] * other.group1()[3])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorEven {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       28        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       16       31        0
    //  no simd       25       40        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group1(), 0, 0, 0, 3) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group3()[2]]))
                + (swizzle!(other.group1(), 2, 3, 1, 2) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group3()[1]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) - (self.group0()[1] * other.group1()[3])),
                    ((self.group0()[3] * other.group0()[1]) - (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[2]) - (self.group0()[0] * other.group1()[2])),
                    ((self.group3()[0] * other.group1()[1]) + (self.group2()[3] * other.group1()[0])
                        - (self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group1()[3] * other.group0()[3])
                        - (self.group2()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group1(), 1, 2, 3, 3))
                + Simd32x4::from([
                    (self.group2()[0] * other.group1()[0]),
                    (self.group2()[1] * other.group1()[0]),
                    (self.group2()[2] * other.group1()[0]),
                    (-(self.group1()[1] * other.group1()[2]) + (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group1()[3]) - (self.group2()[2] * other.group1()[2])),
                (-(self.group2()[0] * other.group1()[3]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[1] * other.group1()[1])),
                (self.group0()[3] * other.group1()[0]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (self.group0()[3] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorEven {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       27        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       20       29        0
    //  no simd       26       35        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group0()[3]) * other.group0())
                + (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group3()[3] * other.group1()[3])
                        - (self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group2()[0] * other.group2()[3]) + (self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group2()[1] * other.group2()[3]) + (self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group2()[2] * other.group2()[3]) + (self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group2()[0]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[3] * other.group2()[1]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[3] * other.group2()[2]) + (self.group1()[2] * other.group1()[3])),
                (self.group0()[3] * other.group2()[3]),
            ]),
        );
    }
}
impl InfixAntiWedge for VersorEvenAligningOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for VersorEvenAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorEvenAligningOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group1()[2], self.group2()[2], self.group2()[0], self.group2()[1]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                    ((self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDualNum> for VersorEvenAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        1        6        0
    //  no simd        1        9        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group0()[0])),
            ]),
            // e15, e25, e35, e3215
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for VersorEvenAligningOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other[e321]) * Simd32x4::from([self.group0()[3], self.group1()[0], self.group1()[1], self.group1()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorEvenAligningOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlector> for VersorEvenAligningOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e1, e2, e3, e5
            ((Simd32x4::from(self.group0()[3]) * other.group1())
                - (swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for VersorEvenAligningOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            ((self.group0()[3] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
            ((self.group0()[3] * other.group0()[2]) + (self.group1()[1] * other.group0()[0])),
            ((self.group0()[3] * other.group0()[3]) + (self.group1()[2] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorEvenAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for VersorEvenAligningOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiMotor> for VersorEvenAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + (Simd32x4::from(self.group0()[3]) * other.group0())
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group1()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for VersorEvenAligningOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for VersorEvenAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for VersorEvenAligningOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiScalar> for VersorEvenAligningOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other[e12345])),
            // e415, e425, e435, e4
            (self.group1() * Simd32x4::from(other[e12345])),
            // e235, e315, e125, e5
            (self.group2() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for VersorEvenAligningOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorEvenAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group0()[3]) * other.group0())
                + (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group2()[0] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group2()[1] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group2()[2] * other.group1()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorEvenAligningOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group1()[2], self.group2()[2], self.group2()[0], self.group2()[1]]))
                + (Simd32x4::from(self.group0()[3]) * other.group1())
                + Simd32x4::from([
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3])),
                ])),
        );
    }
}
impl AntiWedge<Circle> for VersorEvenAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       29        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       19       31        0
    //  no simd       22       37        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[2] * other.group0()[1])
                        + (self.group2()[1] * other.group0()[2])
                        + (self.group1()[0] * other.group1()[3])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2])
                        + (self.group1()[1] * other.group1()[3])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group2()[1] * other.group0()[0])
                        + (self.group2()[0] * other.group0()[1])
                        + (self.group1()[2] * other.group1()[3])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorEvenAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       29        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       16       30        0
    //  no simd       19       33        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[2] * other.group0()[1]) + (self.group2()[1] * other.group0()[2]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2]) + (self.group0()[0] * other.group2()[2])),
                    (-(self.group2()[1] * other.group0()[0]) + (self.group2()[0] * other.group0()[1]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorEvenAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       17        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       10       19        0
    //  no simd       13       25        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorEvenAligningOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group2()[1] * other.group0()[2]) - (self.group0()[1] * other.group1()[2])
                    + (self.group0()[2] * other.group1()[1])),
                ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group2()[0] * other.group0()[1]) - (self.group0()[0] * other.group1()[1])
                    + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for VersorEvenAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       21        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorEvenAligningOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       16       28        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2])
                        + (self.group0()[2] * other.group1()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])
                        - (self.group0()[2] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1])
                        + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<Dipole> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       15        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        8       16        0
    //  no simd        8       19        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       13        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorEvenAligningOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        9        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        5       13        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from(0.0),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DualNum> for VersorEvenAligningOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        1        7        0
    //  no simd        1       13        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e415, e425, e435, e4
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[1]),
                (self.group2()[1] * other.group0()[1]),
                (self.group2()[2] * other.group0()[1]),
                ((self.group0()[3] * other.group0()[0]) + (self.group2()[3] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorEvenAligningOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self.group0()[3] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for VersorEvenAligningOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorEvenAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Flector> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       21        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       15       24        0
    //  no simd       21       33        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(other.group1(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1]))])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group1()[1])
                    + (self.group2()[1] * other.group1()[2])
                    + (self.group0()[3] * other.group0()[0])
                    + (self.group1()[0] * other.group1()[3])),
                ((self.group2()[2] * other.group1()[0]) - (self.group2()[0] * other.group1()[2]) + (self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group1()[3])),
                (-(self.group2()[1] * other.group1()[0])
                    + (self.group2()[0] * other.group1()[1])
                    + (self.group0()[3] * other.group0()[2])
                    + (self.group1()[2] * other.group1()[3])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorEvenAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group0()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOddAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       11        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       13        0
    //  no simd       12       19        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorOddAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(other.group0(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group0(), 2, 3, 1, 0))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[1] * other.group0()[2]) - (self.group1()[0] * other.group0()[1]))])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[3]) - (self.group2()[2] * other.group0()[2])),
                (-(self.group2()[0] * other.group0()[3]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[1] * other.group0()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (self.group0()[3] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Horizon> for VersorEvenAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
            // e15, e25, e35, e3215
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<Infinity> for VersorEvenAligningOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for VersorEvenAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       18        0
    //  no simd       10       21        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[1]),
                    (self.group0()[0] * other.group1()[2]),
                    (self.group0()[1] * other.group1()[0]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorEvenAligningOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e1, e2, e3, e5
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<LineOnOrigin> for VersorEvenAligningOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<Motor> for VersorEvenAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       18        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       22       34        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                0.0,
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group1())
                + (self.group2() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])),
                ])),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[1]),
                    (self.group0()[0] * other.group1()[2]),
                    (self.group0()[1] * other.group1()[0]),
                    ((self.group1()[3] * other.group0()[3]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorEvenAligningOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group0(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0]))])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for VersorEvenAligningOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e415, e425, e435, e4
            ((other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                ((self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for VersorEvenAligningOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       38       57        0
    //    simd3        6        9        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       49       72        0
    //  no simd       76      108        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group2()[3] * other.group9()[0]) - (self.group2()[2] * other.group3()[2]) - (self.group2()[1] * other.group3()[1]) - (self.group2()[0] * other.group3()[0])
                    + (self.group1()[3] * other[e45])
                    - (self.group1()[2] * other.group5()[2])
                    - (self.group1()[1] * other.group5()[1])
                    - (self.group1()[0] * other.group5()[0])
                    + (self.group0()[3] * other.group0()[0])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                (self.group0()[3] * other.group0()[1]),
            ]),
            // e1, e2, e3, e4
            ((self.group1() * Simd32x4::from([other.group6()[3], other.group6()[3], other.group6()[3], other.group0()[1]])) + (Simd32x4::from(self.group0()[3]) * other.group1())
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group8()[2], other.group8()[0], other.group8()[1], other.group6()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[2] * other.group7()[1]) + (self.group2()[1] * other.group7()[2]) + (self.group0()[2] * other.group8()[1])),
                    ((self.group2()[2] * other.group7()[0]) - (self.group2()[0] * other.group7()[2]) + (self.group0()[0] * other.group8()[2])),
                    (-(self.group2()[1] * other.group7()[0]) + (self.group2()[0] * other.group7()[1]) + (self.group0()[1] * other.group8()[0])),
                    (-(self.group1()[2] * other.group7()[2])
                        - (self.group1()[1] * other.group7()[1])
                        - (self.group1()[0] * other.group7()[0])
                        - (self.group0()[0] * other.group6()[0])
                        - (self.group0()[1] * other.group6()[1])),
                ])),
            // e5
            ((self.group2()[3] * other.group0()[1])
                - (self.group2()[2] * other.group6()[2])
                - (self.group2()[1] * other.group6()[1])
                - (self.group2()[0] * other.group6()[0])
                - (self.group1()[2] * other.group8()[2])
                - (self.group1()[1] * other.group8()[1])
                + (self.group0()[3] * other[e1])
                - (self.group1()[0] * other.group8()[0])),
            // e41, e42, e43, e45
            ((Simd32x4::from(self.group0()[3]) * other.group3())
                - (swizzle!(other.group9(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group9()[0]) + (self.group0()[2] * other.group9()[2])),
                    ((self.group1()[1] * other.group9()[0]) + (self.group0()[0] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[0]) + (self.group0()[1] * other.group9()[1])),
                    (-(self.group1()[1] * other.group9()[2]) - (self.group1()[0] * other.group9()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * other.group4())
                + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                + Simd32x3::from([
                    (-(self.group2()[2] * other.group9()[2]) + (self.group2()[1] * other.group9()[3])),
                    ((self.group2()[2] * other.group9()[1]) - (self.group2()[0] * other.group9()[3])),
                    (-(self.group2()[1] * other.group9()[1]) + (self.group2()[0] * other.group9()[2])),
                ])),
            // e23, e31, e12
            ((Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))
                + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(self.group0()[3]) * other.group5())),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group6()[0]) + (self.group1()[0] * other.group0()[1])),
                ((self.group0()[3] * other.group6()[1]) + (self.group1()[1] * other.group0()[1])),
                ((self.group0()[3] * other.group6()[2]) + (self.group1()[2] * other.group0()[1])),
                (self.group0()[3] * other.group6()[3]),
            ]),
            // e423, e431, e412
            ((Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])) + (Simd32x3::from(self.group0()[3]) * other.group7())),
            // e235, e315, e125
            ((Simd32x3::from(self.group0()[3]) * other.group8()) + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[3]) * other.group9()),
            // e3215
            (self.group0()[3] * other[e45]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorEvenAligningOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorEvenAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorEvenAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]])),
            // e23, e31, e12, e1234
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorEvenAligningOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group1()[2], self.group2()[2], self.group2()[0], self.group2()[1]]))
                + (swizzle!(other.group0(), 3, 2, 0, 1) * Simd32x4::from([self.group0()[3], self.group2()[1], self.group2()[2], self.group2()[0]]))
                + Simd32x4::from([(-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorEvenAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group2()[3]]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<Origin> for VersorEvenAligningOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       22        0
    //    simd4        0        1        0
    // Totals...
    // yes simd       11       23        0
    //  no simd       11       26        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOddAligningOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorOddAligningOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for VersorEvenAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e5
            (self.group0()[3] * other[e2]),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for VersorEvenAligningOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (Simd32x2::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Scalar> for VersorEvenAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Scalar) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[scalar]));
    }
}
impl AntiWedge<Sphere> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       26        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       15       28        0
    //  no simd       18       34        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((self.group1() * Simd32x4::from([other[e4315], other[e4315], other[e4315], other.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                    (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                    (self.group2()[3] * other[e4315]),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group2()[0] * other[e4315])),
                ((self.group0()[1] * other.group0()[3]) + (self.group2()[1] * other[e4315])),
                ((self.group0()[2] * other.group0()[3]) + (self.group2()[2] * other[e4315])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                (self.group0()[3] * other[e4315]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       16        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[1] * other.group0()[1]),
                (self.group1()[2] * other.group0()[1]),
                ((self.group1()[3] * other.group0()[0]) + (self.group2()[3] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) + (self.group2()[0] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[0]) + (self.group2()[2] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
            ]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       26        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                ((self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group2()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for VersorEvenAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       17       30        0
    //    simd4        6        6        0
    // Totals...
    // yes simd       23       36        0
    //  no simd       41       54        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group2())
                + (self.group2() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2])
                        - (self.group2()[1] * other.group1()[1])
                        - (self.group2()[0] * other.group1()[0])
                        - (self.group1()[2] * other.group2()[2])
                        - (self.group1()[1] * other.group2()[1])
                        - (self.group1()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group2()[1], self.group2()[2], self.group2()[0], self.group1()[3]]))
                + (Simd32x4::from(self.group0()[3]) * other.group3())
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group1()[3]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group1()[2] * other.group1()[3]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorEvenAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       23        0
    //    simd4        6        6        0
    // Totals...
    // yes simd       20       29        0
    //  no simd       38       47        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                0.0,
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group2())
                + (self.group2() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2])
                        - (self.group2()[1] * other.group1()[1])
                        - (self.group2()[0] * other.group1()[0])
                        - (self.group1()[2] * other.group2()[2])
                        - (self.group1()[1] * other.group2()[1])
                        - (self.group1()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group2()[1], self.group2()[2], self.group2()[0], self.group1()[3]]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * Simd32x4::from([other.group2()[1], other.group2()[2], other.group2()[0], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorEvenAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       21        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       16       26        0
    //  no simd       28       41        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[0])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[0])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[0])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[3]) * other.group2())
                + (self.group2() * Simd32x4::from(other.group0()[0]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2])
                        - (self.group2()[1] * other.group1()[1])
                        - (self.group2()[0] * other.group1()[0])
                        - (self.group1()[2] * other.group2()[2])
                        - (self.group1()[1] * other.group2()[1])
                        - (self.group1()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            ((self.group1() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[0]]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group0()[3]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorEvenAligningOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       20        0
    //  no simd       15       26        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group2()[1], self.group2()[2], self.group2()[0], self.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                    (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorEvenAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       26        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       16       28        0
    //  no simd       22       34        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                ((self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group2()[1], self.group2()[2], self.group2()[0], self.group1()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0]) + (self.group0()[3] * other.group1()[3])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorEvenAligningOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       21        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       15       24        0
    //  no simd       21       33        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (Simd32x4::from(self.group0()[3]) * other.group2())
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2])
                        + (self.group0()[2] * other.group1()[1])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])
                        - (self.group0()[2] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1])
                        + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOdd> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       21       30        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       26       36        0
    //  no simd       41       54        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + (Simd32x4::from(self.group0()[3]) * other.group0())
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group2()[2]]))
                + (swizzle!(other.group3(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            ((self.group0() * Simd32x4::from([other.group3()[3], other.group3()[3], other.group3()[3], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group2()[0] * other.group2()[3]) + (self.group0()[3] * other.group1()[0])),
                    ((self.group2()[1] * other.group2()[3]) + (self.group0()[3] * other.group1()[1])),
                    ((self.group2()[2] * other.group2()[3]) + (self.group0()[3] * other.group1()[2])),
                    (-(self.group1()[2] * other.group3()[2]) - (self.group1()[1] * other.group3()[1]) - (self.group1()[0] * other.group3()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group3()[1])
                    + (self.group2()[1] * other.group3()[2])
                    + (self.group0()[3] * other.group2()[0])
                    + (self.group1()[0] * other.group3()[3])),
                ((self.group2()[2] * other.group3()[0]) - (self.group2()[0] * other.group3()[2]) + (self.group0()[3] * other.group2()[1]) + (self.group1()[1] * other.group3()[3])),
                (-(self.group2()[1] * other.group3()[0])
                    + (self.group2()[0] * other.group3()[1])
                    + (self.group0()[3] * other.group2()[2])
                    + (self.group1()[2] * other.group3()[3])),
                (self.group0()[3] * other.group2()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group3()),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       27        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       19       32        0
    //  no simd       31       47        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + (swizzle!(other.group2(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group0()[0]),
                    (self.group0()[3] * other.group0()[1]),
                    (self.group0()[3] * other.group0()[2]),
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e45
            ((self.group0() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group0()[3]]))
                + Simd32x4::from([
                    (self.group2()[0] * other.group1()[3]),
                    (self.group2()[1] * other.group1()[3]),
                    (self.group2()[2] * other.group1()[3]),
                    (-(self.group1()[2] * other.group2()[2]) - (self.group1()[1] * other.group2()[1]) - (self.group1()[0] * other.group2()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group2()[1])
                    + (self.group2()[1] * other.group2()[2])
                    + (self.group0()[3] * other.group1()[0])
                    + (self.group1()[0] * other.group2()[3])),
                ((self.group2()[2] * other.group2()[0]) - (self.group2()[0] * other.group2()[2]) + (self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group2()[3])),
                (-(self.group2()[1] * other.group2()[0])
                    + (self.group2()[0] * other.group2()[1])
                    + (self.group0()[3] * other.group1()[2])
                    + (self.group1()[2] * other.group2()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       24        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       19       28        0
    //  no simd       28       40        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group0()[3]]))
                + (swizzle!(other.group2(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) - (self.group1()[0] * other.group1()[0])
                        + (self.group0()[3] * other.group0()[0])
                        - (self.group0()[0] * other.group0()[1])
                        - (self.group0()[1] * other.group0()[2])),
                ])),
            // e23, e31, e12, e45
            ((self.group0() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group1()[2] * other.group2()[2]) - (self.group1()[1] * other.group2()[1]) - (self.group1()[0] * other.group2()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group2()[1])
                    + (self.group2()[1] * other.group2()[2])
                    + (self.group0()[3] * other.group0()[1])
                    + (self.group1()[0] * other.group2()[3])),
                ((self.group2()[2] * other.group2()[0]) - (self.group2()[0] * other.group2()[2]) + (self.group0()[3] * other.group0()[2]) + (self.group1()[1] * other.group2()[3])),
                (-(self.group2()[1] * other.group2()[0])
                    + (self.group2()[0] * other.group2()[1])
                    + (self.group0()[3] * other.group0()[3])
                    + (self.group1()[2] * other.group2()[3])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group1()[3]]))
                + (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group2()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group0()[3]) + (self.group2()[1] * other.group1()[3])),
                ((self.group0()[2] * other.group0()[3]) + (self.group2()[2] * other.group1()[3])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       29        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       15       30        0
    //  no simd       18       33        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group1()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) - (self.group0()[1] * other.group1()[3]) + (self.group0()[2] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[3]) - (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[2]) - (self.group0()[0] * other.group1()[2]) + (self.group0()[1] * other.group1()[1])),
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group2()[0] * other.group1()[0]),
                (self.group2()[1] * other.group1()[0]),
                (self.group2()[2] * other.group1()[0]),
                (-(self.group1()[2] * other.group1()[3]) - (self.group1()[1] * other.group1()[2]) + (self.group0()[3] * other.group0()[3])
                    - (self.group1()[0] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group1()[3]) - (self.group2()[2] * other.group1()[2])),
                (-(self.group2()[0] * other.group1()[3]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[1] * other.group1()[1])),
                (self.group0()[3] * other.group1()[0]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (self.group0()[3] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorEvenAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       27        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       20       29        0
    //  no simd       26       35        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group0()[3]) * other.group0())
                + (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) - (self.group2()[0] * other.group0()[0])
                        + (self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group2()[0] * other.group2()[3]) + (self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group2()[1] * other.group2()[3]) + (self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group2()[2] * other.group2()[3]) + (self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group2()[0]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[3] * other.group2()[1]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[3] * other.group2()[2]) + (self.group1()[2] * other.group1()[3])),
                (self.group0()[3] * other.group2()[3]),
            ]),
        );
    }
}
impl InfixAntiWedge for VersorEvenAtInfinity {}
impl AntiWedge<AntiCircleOnOrigin> for VersorEvenAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorEvenAtInfinity {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group1()[2], self.group2()[2], self.group2()[0], self.group2()[1]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                    ((self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDualNum> for VersorEvenAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[1])]),
            // e15, e25, e35, e3215
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[0]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for VersorEvenAtInfinity {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other[e321]) * Simd32x4::from([self.group0()[0], self.group1()[0], self.group1()[1], self.group1()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorEvenAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e1, e2, e3, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for VersorEvenAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e1, e2, e3, e5
            ((Simd32x4::from(self.group0()[0]) * other.group1())
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for VersorEvenAtInfinity {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0]),
            ((self.group0()[0] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[2]) + (self.group1()[1] * other.group0()[0])),
            ((self.group0()[0] * other.group0()[3]) + (self.group1()[2] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorEvenAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for VersorEvenAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0]),
            (self.group0()[0] * other.group0()[1]),
            (self.group0()[0] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiMotor> for VersorEvenAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) + (self.group0()[0] * other.group0()[3])
                    - (self.group1()[0] * other.group0()[0])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[0]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[0] * other.group1()[1]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[0] * other.group1()[2]) + (self.group1()[2] * other.group1()[3])),
                (self.group0()[0] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for VersorEvenAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0]),
            (self.group0()[0] * other.group0()[1]),
            (self.group0()[0] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) + (self.group0()[0] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for VersorEvenAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for VersorEvenAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<AntiScalar> for VersorEvenAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            (self.group0() * Simd32x4::from(other[e12345])),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other[e12345])),
            // e235, e315, e125, e5
            (self.group2() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for VersorEvenAtInfinity {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorEvenAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group0()[0]) * other.group0())
                + (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[0]) + (self.group2()[0] * other.group1()[3])),
                ((self.group0()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[3])),
                ((self.group0()[0] * other.group1()[2]) + (self.group2()[2] * other.group1()[3])),
                (self.group0()[0] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorEvenAtInfinity {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group1()[2], self.group2()[2], self.group2()[0], self.group2()[1]]))
                + (Simd32x4::from(self.group0()[0]) * other.group1())
                + Simd32x4::from([
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3])),
                ])),
        );
    }
}
impl AntiWedge<Circle> for VersorEvenAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       27        0
    //    simd4        0        1        0
    // Totals...
    // yes simd       16       28        0
    //  no simd       16       31        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[0]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group2()[0]),
                (self.group0()[0] * other.group2()[1]),
                (self.group0()[0] * other.group2()[2]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1])
                    + (self.group2()[1] * other.group0()[2])
                    + (self.group1()[0] * other.group1()[3])
                    + (self.group1()[3] * other.group1()[0])),
                ((self.group2()[2] * other.group0()[0]) - (self.group2()[0] * other.group0()[2]) + (self.group1()[1] * other.group1()[3]) + (self.group1()[3] * other.group1()[1])),
                (-(self.group2()[1] * other.group0()[0])
                    + (self.group2()[0] * other.group0()[1])
                    + (self.group1()[2] * other.group1()[3])
                    + (self.group1()[3] * other.group1()[2])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorEvenAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       27        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group2()[0]),
                (self.group0()[0] * other.group2()[1]),
                (self.group0()[0] * other.group2()[2]),
                (-(self.group2()[2] * other.group1()[2])
                    - (self.group2()[1] * other.group1()[1])
                    - (self.group2()[0] * other.group1()[0])
                    - (self.group1()[2] * other.group2()[2])
                    - (self.group1()[0] * other.group2()[0])
                    - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[3] * other.group1()[0]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[3] * other.group1()[2]) + (self.group2()[0] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorEvenAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       15        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        8       16        0
    //  no simd        8       19        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                ((self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
            ]),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorEvenAtInfinity {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for VersorEvenAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       21        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[3] * other.group1()[0]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[3] * other.group1()[2]) + (self.group2()[0] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorEvenAtInfinity {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       14        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        7       16        0
    //  no simd       10       22        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<Dipole> for VersorEvenAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       13        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        6       14        0
    //  no simd        6       17        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[3] * other.group1()[3])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[0]) * other.group1()),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group2()[0]),
                (self.group0()[0] * other.group2()[1]),
                (self.group0()[0] * other.group2()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorEvenAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       11        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group1()[3] * other.group0()[3])
                    - (self.group2()[0] * other.group0()[0])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorEvenAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        7        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       11        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[3] * other.group0()[3])
                    - (self.group1()[2] * other.group0()[2])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group1()[1] * other.group0()[1])),
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorEvenAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorEvenAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group1()[3] * other.group0()[3])
                    - (self.group2()[0] * other.group0()[0])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from(0.0),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorEvenAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[0] * other.group2()[0]),
                (self.group0()[0] * other.group2()[1]),
                (self.group0()[0] * other.group2()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DualNum> for VersorEvenAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        1        7        0
    //  no simd        1       13        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e415, e425, e435, e321
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[1]),
                (self.group2()[1] * other.group0()[1]),
                (self.group2()[2] * other.group0()[1]),
                ((self.group0()[0] * other.group0()[0]) + (self.group2()[3] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorEvenAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        3        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([(self.group1()[3] * other[e45] * -1.0), 0.0, 0.0, 0.0]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other[e45])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<FlatPoint> for VersorEvenAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            (swizzle!(other.group0(), 3, 0, 1, 2)
                * Simd32x4::from([self.group1()[3], self.group0()[0], self.group0()[0], self.group0()[0]])
                * Simd32x4::from([-1.0, 1.0, 1.0, 1.0])),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[0] * other.group0()[3])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorEvenAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ (Simd32x3::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<Flector> for VersorEvenAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3       14        0
    //    simd4        3        4        0
    // Totals...
    // yes simd        6       18        0
    //  no simd       15       30        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((swizzle!(other.group1(), 2, 2, 0, 1) * Simd32x4::from([self.group0()[3], self.group2()[1], self.group2()[2], self.group2()[0]]))
                + (swizzle!(self.group0(), 1, 0, 0, 0) * Simd32x4::from([other.group1()[0], other.group0()[0], other.group0()[1], other.group0()[2]]))
                + (swizzle!(other.group1(), 1, 3, 3, 3) * Simd32x4::from([self.group0()[2], self.group1()[0], self.group1()[1], self.group1()[2]]))
                - Simd32x4::from([
                    (self.group1()[3] * other.group0()[3]),
                    (self.group2()[2] * other.group1()[1]),
                    (self.group2()[0] * other.group1()[2]),
                    (self.group2()[1] * other.group1()[0]),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0] * -1.0),
                (self.group1()[3] * other.group1()[1] * -1.0),
                (self.group1()[3] * other.group1()[2] * -1.0),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[0] * other.group0()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[0]) * other.group1()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorEvenAtInfinity {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return FlectorAtInfinity::from_groups(/* e15, e25, e35, e3215 */ Simd32x4::from([
            ((self.group0()[0] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
            ((self.group0()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
            ((self.group0()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
            (self.group0()[0] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorEvenAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       15        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       17        0
    //  no simd       12       23        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            (-(swizzle!(other.group0(), 0, 2, 3, 1) * Simd32x4::from([self.group1()[3], self.group2()[2], self.group2()[0], self.group2()[1]]))
                + (swizzle!(other.group0(), 3, 3, 1, 2) * Simd32x4::from([self.group0()[3], self.group2()[1], self.group2()[2], self.group2()[0]]))
                + Simd32x4::from([((self.group0()[1] * other.group0()[1]) + (self.group0()[2] * other.group0()[2])), 0.0, 0.0, 0.0])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                (self.group1()[3] * other.group0()[3] * -1.0),
                (-(self.group1()[2] * other.group0()[3]) - (self.group1()[1] * other.group0()[2]) + (self.group0()[0] * other.group0()[0])
                    - (self.group1()[0] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                (self.group0()[0] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Horizon> for VersorEvenAtInfinity {
    type Output = FlectorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return FlectorAtInfinity::from_groups(
            // e15, e25, e35, e3215
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[0]])),
        );
    }
}
impl AntiWedge<Infinity> for VersorEvenAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[0] * other[e5]));
    }
}
impl AntiWedge<Line> for VersorEvenAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorEvenAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0]),
            (self.group0()[0] * other.group0()[1]),
            (self.group0()[0] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for VersorEvenAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<Motor> for VersorEvenAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                ((self.group0()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                ((self.group0()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                ((self.group0()[3] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[0]) * other.group1())
                + (self.group2() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorEvenAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group0()[0] * other.group0()[0]),
            (self.group0()[0] * other.group0()[1]),
            (self.group0()[0] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) + (self.group0()[0] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for VersorEvenAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       21        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                ((self.group0()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                ((self.group0()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                ((self.group0()[3] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[0] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                ((self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for VersorEvenAtInfinity {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       37       52        0
    //    simd3        5        8        0
    //    simd4        3        5        0
    // Totals...
    // yes simd       45       65        0
    //  no simd       64       96        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group2()[3] * other.group9()[0])
                    - (self.group2()[2] * other.group3()[2])
                    - (self.group2()[1] * other.group3()[1])
                    - (self.group2()[0] * other.group3()[0])
                    - (self.group1()[3] * other.group3()[3])
                    - (self.group1()[2] * other.group5()[2])
                    - (self.group1()[1] * other.group5()[1])
                    - (self.group1()[0] * other.group5()[0])
                    + (self.group0()[3] * other.group9()[3])
                    + (self.group0()[2] * other.group9()[2])
                    + (self.group0()[0] * other.group0()[0])
                    + (self.group0()[1] * other.group9()[1])),
                (self.group0()[0] * other.group0()[1]),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group0()[0]) * other.group1())
                + Simd32x4::from([
                    (-(self.group2()[2] * other.group7()[1])
                        + (self.group2()[1] * other.group7()[2])
                        + (self.group1()[3] * other.group6()[0])
                        + (self.group1()[0] * other.group6()[3])
                        + (self.group0()[1] * other.group0()[1])),
                    ((self.group2()[2] * other.group7()[0]) - (self.group2()[0] * other.group7()[2])
                        + (self.group1()[3] * other.group6()[1])
                        + (self.group1()[1] * other.group6()[3])
                        + (self.group0()[2] * other.group0()[1])),
                    (-(self.group2()[1] * other.group7()[0])
                        + (self.group2()[0] * other.group7()[1])
                        + (self.group1()[3] * other.group6()[2])
                        + (self.group1()[2] * other.group6()[3])
                        + (self.group0()[3] * other.group0()[1])),
                    (-(self.group1()[2] * other.group7()[2]) - (self.group1()[1] * other.group7()[1]) - (self.group1()[0] * other.group7()[0])),
                ])),
            // e5
            ((self.group2()[3] * other.group0()[1])
                - (self.group2()[2] * other.group6()[2])
                - (self.group2()[1] * other.group6()[1])
                - (self.group2()[0] * other.group6()[0])
                - (self.group1()[2] * other.group8()[2])
                - (self.group1()[1] * other.group8()[1])
                + (self.group0()[0] * other[e1])
                - (self.group1()[0] * other.group8()[0])),
            // e41, e42, e43, e45
            ((Simd32x4::from(self.group0()[0]) * other.group3())
                + Simd32x4::from([
                    (self.group1()[0] * other.group9()[0]),
                    (self.group1()[1] * other.group9()[0]),
                    (self.group1()[2] * other.group9()[0]),
                    (-(self.group1()[2] * other.group9()[3]) - (self.group1()[1] * other.group9()[2]) - (self.group1()[0] * other.group9()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[0]) * other.group4())
                + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                + Simd32x3::from([
                    (-(self.group2()[2] * other.group9()[2]) + (self.group2()[1] * other.group9()[3])),
                    ((self.group2()[2] * other.group9()[1]) - (self.group2()[0] * other.group9()[3])),
                    (-(self.group2()[1] * other.group9()[1]) + (self.group2()[0] * other.group9()[2])),
                ])),
            // e23, e31, e12
            ((Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]])) + (Simd32x3::from(self.group0()[0]) * other.group5())
                - (Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e415, e425, e435, e321
            ((Simd32x4::from(self.group0()[0]) * other.group6()) + (self.group1() * Simd32x4::from(other.group0()[1]))),
            // e423, e431, e412
            (Simd32x3::from(self.group0()[0]) * other.group7()),
            // e235, e315, e125
            ((Simd32x3::from(self.group0()[0]) * other.group8()) + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[0]) * other.group9()),
            // e3215
            (self.group0()[0] * other[e45]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorEvenAtInfinity {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
            ]),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorEvenAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorEvenAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]])),
            // e23, e31, e12, e1234
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[0]])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorEvenAtInfinity {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
            // e4, e1, e2, e3
            (-(swizzle!(other.group0(), 2, 1, 2, 0) * Simd32x4::from([self.group1()[2], self.group2()[2], self.group2()[0], self.group2()[1]]))
                + (swizzle!(other.group0(), 3, 2, 0, 1) * Simd32x4::from([self.group0()[0], self.group2()[1], self.group2()[2], self.group2()[0]]))
                + Simd32x4::from([(-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorEvenAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((other.group0() * Simd32x4::from([self.group0()[0], self.group0()[0], self.group0()[0], self.group2()[3]]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[0]])),
        );
    }
}
impl AntiWedge<Origin> for VersorEvenAtInfinity {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group0()[0] * other[e4]));
    }
}
impl AntiWedge<Plane> for VersorEvenAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        2        5        0
    // Totals...
    // yes simd        4       14        0
    //  no simd       10       29        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((swizzle!(other.group0(), 2, 3, 0, 3) * Simd32x4::from([self.group0()[3], self.group1()[0], self.group2()[2], self.group1()[2]]))
                + (swizzle!(other.group0(), 0, 2, 3, 1) * Simd32x4::from([self.group0()[1], self.group2()[1], self.group1()[1], self.group2()[0]]))
                + (Simd32x4::from([1.0, -1.0, -1.0, -1.0])
                    * swizzle!(other.group0(), 1, 1, 2, 0)
                    * Simd32x4::from([self.group0()[2], self.group2()[2], self.group2()[0], self.group2()[1]]))),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[0]) * other.group0()),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorEvenAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       21        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for VersorEvenAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e5
            (self.group0()[0] * other[e2]),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for VersorEvenAtInfinity {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (Simd32x2::from(self.group0()[0]) * other.group0()));
    }
}
impl AntiWedge<Scalar> for VersorEvenAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Scalar) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[0] * other[scalar]));
    }
}
impl AntiWedge<Sphere> for VersorEvenAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       22        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       11       24        0
    //  no simd       14       30        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other[e4315]),
                (self.group1()[1] * other[e4315]),
                (self.group1()[2] * other[e4315]),
                ((self.group2()[3] * other[e4315]) + (self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group0(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group2()[0] * other[e4315]),
                    (self.group2()[1] * other[e4315]),
                    (self.group2()[2] * other[e4315]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[0] * other.group0()[3]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[3]) + (self.group2()[0] * other.group0()[1])),
                (self.group0()[0] * other[e4315]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[0]) * other.group0()),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorEvenAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        8        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        9        0
    //  no simd        0       12        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group2()[0] * other.group0()[1]),
                (self.group2()[1] * other.group0()[1]),
                (self.group2()[2] * other.group0()[1]),
                (self.group0()[0] * other.group0()[0]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
            ]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorEvenAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       22        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       23        0
    //  no simd       11       26        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group2()[3] * other.group0()[3]) + (self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group0(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group2()[0] * other.group0()[3]),
                    (self.group2()[1] * other.group0()[3]),
                    (self.group2()[2] * other.group0()[3]),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (self.group0()[0] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for VersorEvenAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       20        0
    //    simd4        5        7        0
    // Totals...
    // yes simd       20       27        0
    //  no simd       35       48        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e415, e425, e435, e321
            ((Simd32x4::from(self.group0()[0]) * other.group1()) + (self.group1() * Simd32x4::from(other.group0()[3]))),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[0]) * other.group2())
                + (self.group2() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2])
                        - (self.group2()[1] * other.group1()[1])
                        - (self.group2()[0] * other.group1()[0])
                        - (self.group1()[2] * other.group2()[2])
                        - (self.group1()[1] * other.group2()[1])
                        - (self.group1()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (Simd32x4::from(self.group0()[0]) * other.group3())
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2])
                        + (self.group1()[3] * other.group1()[0])
                        + (self.group1()[0] * other.group1()[3])
                        + (self.group0()[1] * other.group0()[3])),
                    ((self.group2()[2] * other.group0()[0])
                        + (self.group1()[3] * other.group1()[1])
                        + (self.group1()[1] * other.group1()[3])
                        + (self.group0()[2] * other.group0()[3])),
                    ((self.group2()[0] * other.group0()[1])
                        + (self.group1()[3] * other.group1()[2])
                        + (self.group1()[2] * other.group1()[3])
                        + (self.group0()[3] * other.group0()[3])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorEvenAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       21        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       16       26        0
    //  no simd       28       41        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[0] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[0] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[0]) * other.group2())
                + (self.group2() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2])
                        - (self.group2()[1] * other.group1()[1])
                        - (self.group2()[0] * other.group1()[0])
                        - (self.group1()[2] * other.group2()[2])
                        - (self.group1()[1] * other.group2()[1])
                        - (self.group1()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[3] * other.group1()[0])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[3] * other.group1()[2])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorEvenAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       19        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       17       23        0
    //  no simd       26       35        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                ((self.group1()[3] * other.group1()[0]) + (self.group1()[0] * other.group1()[3]) + (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                ((self.group1()[3] * other.group1()[1]) + (self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group0()[2]) + (self.group0()[2] * other.group0()[0])),
                ((self.group1()[3] * other.group1()[2]) + (self.group1()[2] * other.group1()[3]) + (self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
            ]),
            // e415, e425, e435, e321
            ((Simd32x4::from(self.group0()[0]) * other.group1()) + (self.group1() * Simd32x4::from(other.group0()[0]))),
            // e235, e315, e125, e5
            ((Simd32x4::from(self.group0()[0]) * other.group2())
                + (self.group2() * Simd32x4::from(other.group0()[0]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2])
                        - (self.group2()[1] * other.group1()[1])
                        - (self.group2()[0] * other.group1()[0])
                        - (self.group1()[2] * other.group2()[2])
                        - (self.group1()[1] * other.group2()[1])
                        - (self.group1()[0] * other.group2()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorEvenAtInfinity {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       14        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[0] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[0] * other.group1()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group2()[1], self.group2()[2], self.group2()[0], self.group0()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0]))])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorEvenAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       22        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       12       25        0
    //  no simd       18       34        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[0] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[0] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                ((self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (swizzle!(self.group0(), 1, 2, 3, 0) * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[3] * other.group1()[0])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[3] * other.group1()[2])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorEvenAtInfinity {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       15        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        9       18        0
    //  no simd       15       27        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[0]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[0] * other.group1()[0]),
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[0] * other.group1()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (Simd32x4::from(self.group0()[0]) * other.group2())
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[0] * other.group0()[3])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[1] * other.group0()[3])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[2] * other.group0()[3])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOdd> for VersorEvenAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       19       28        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       23       33        0
    //  no simd       35       48        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 0, 0, 0, 3) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group3()[2]]))
                + (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        + (self.group0()[2] * other.group3()[1])
                        + (self.group0()[0] * other.group0()[3])
                        + (self.group0()[1] * other.group3()[0])),
                ])),
            // e23, e31, e12, e45
            ((Simd32x4::from(self.group0()[0]) * other.group1()) - (swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group3(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group2()[0] * other.group2()[3]),
                    (self.group2()[1] * other.group2()[3]),
                    (self.group2()[2] * other.group2()[3]),
                    (-(self.group1()[1] * other.group3()[1]) - (self.group1()[0] * other.group3()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group3()[1])
                    + (self.group2()[1] * other.group3()[2])
                    + (self.group0()[0] * other.group2()[0])
                    + (self.group1()[0] * other.group3()[3])),
                ((self.group2()[2] * other.group3()[0]) - (self.group2()[0] * other.group3()[2]) + (self.group0()[0] * other.group2()[1]) + (self.group1()[1] * other.group3()[3])),
                (-(self.group2()[1] * other.group3()[0])
                    + (self.group2()[0] * other.group3()[1])
                    + (self.group0()[0] * other.group2()[2])
                    + (self.group1()[2] * other.group3()[3])),
                (self.group0()[0] * other.group2()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[0]) * other.group3()),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorEvenAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       25        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       19       29        0
    //  no simd       28       41        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 0, 0, 0, 3) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group2()[2]]))
                + (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[3] * other.group0()[3])
                        + (self.group0()[1] * other.group2()[0])
                        + (self.group0()[2] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group2(), 0, 1, 2, 2))
                + Simd32x4::from([
                    (self.group2()[0] * other.group1()[3]),
                    (self.group2()[1] * other.group1()[3]),
                    (self.group2()[2] * other.group1()[3]),
                    (-(self.group1()[1] * other.group2()[1]) + (self.group0()[0] * other.group0()[3]) - (self.group1()[0] * other.group2()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[2] * other.group2()[1])
                    + (self.group2()[1] * other.group2()[2])
                    + (self.group0()[0] * other.group1()[0])
                    + (self.group1()[0] * other.group2()[3])),
                ((self.group2()[2] * other.group2()[0]) - (self.group2()[0] * other.group2()[2]) + (self.group0()[0] * other.group1()[1]) + (self.group1()[1] * other.group2()[3])),
                (-(self.group2()[1] * other.group2()[0])
                    + (self.group2()[0] * other.group2()[1])
                    + (self.group0()[0] * other.group1()[2])
                    + (self.group1()[2] * other.group2()[3])),
                (self.group0()[0] * other.group1()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[0]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorEvenAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       10       19        0
    //  no simd       25       37        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((swizzle!(other.group2(), 2, 2, 0, 1) * Simd32x4::from([self.group0()[3], self.group2()[1], self.group2()[2], self.group2()[0]]))
                + (swizzle!(self.group0(), 2, 0, 0, 0) * Simd32x4::from([other.group2()[1], other.group0()[1], other.group0()[2], other.group0()[3]]))
                + (swizzle!(other.group2(), 0, 3, 3, 3) * Simd32x4::from([self.group0()[1], self.group1()[0], self.group1()[1], self.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        + (self.group0()[0] * other.group0()[0])),
                    ((self.group2()[2] * other.group2()[1]) * -1.0),
                    ((self.group2()[0] * other.group2()[2]) * -1.0),
                    ((self.group2()[1] * other.group2()[0]) * -1.0),
                ])),
            // e23, e31, e12, e45
            ((Simd32x4::from(self.group0()[0]) * other.group1()) - (swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group2(), 0, 1, 2, 2))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[1] * other.group2()[1]) - (self.group1()[0] * other.group2()[0]))])),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[0]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorEvenAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       18        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[0]),
                    (self.group0()[0] * other.group0()[1]),
                    (self.group0()[0] * other.group0()[2]),
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group2()[0] * other.group1()[3]),
                (self.group2()[1] * other.group1()[3]),
                (self.group2()[2] * other.group1()[3]),
                (self.group0()[0] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[0] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[0] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group0()[0] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorEvenAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       22        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       13       25        0
    //  no simd       22       34        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 0, 0, 0, 3) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + (Simd32x4::from(other.group1()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[3] * other.group0()[3])
                        + (self.group0()[1] * other.group1()[1])
                        + (self.group0()[2] * other.group1()[2])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 3, 3, 3, 2) * swizzle!(other.group1(), 1, 2, 3, 3))
                + Simd32x4::from([
                    (self.group2()[0] * other.group1()[0]),
                    (self.group2()[1] * other.group1()[0]),
                    (self.group2()[2] * other.group1()[0]),
                    (-(self.group1()[1] * other.group1()[2]) + (self.group0()[0] * other.group0()[3]) - (self.group1()[0] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[1] * other.group1()[3]) - (self.group2()[2] * other.group1()[2])),
                (-(self.group2()[0] * other.group1()[3]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[1] * other.group1()[1])),
                (self.group0()[0] * other.group1()[0]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group1()[1]),
                (self.group0()[0] * other.group1()[2]),
                (self.group0()[0] * other.group1()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorEvenAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group0()[0]) * other.group0())
                + (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[0]) + (self.group2()[0] * other.group2()[3])),
                ((self.group0()[0] * other.group1()[1]) + (self.group2()[1] * other.group2()[3])),
                ((self.group0()[0] * other.group1()[2]) + (self.group2()[2] * other.group2()[3])),
                (self.group0()[0] * other.group1()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[0]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[0] * other.group2()[1]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[0] * other.group2()[2]) + (self.group1()[2] * other.group1()[3])),
                (self.group0()[0] * other.group2()[3]),
            ]),
        );
    }
}
impl InfixAntiWedge for VersorEvenAtOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiDualNum> for VersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group0() * Simd32x4::from(other.group0()[0])));
    }
}
impl AntiWedge<AntiFlatPoint> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for VersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            ((self.group0()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for VersorEvenAtOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            (self.group0() * Simd32x4::from(other[e12345])),
            // e235, e315, e125, e5
            (self.group1() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[0] * other.group1()[3]),
            (self.group1()[1] * other.group1()[3]),
            (self.group1()[2] * other.group1()[3]),
            ((self.group1()[3] * other.group1()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Circle> for VersorEvenAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       13       18        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorEvenAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       13       18        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorEvenAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4        9        0
    //  no simd        7       12        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[1]),
                    (self.group0()[0] * other.group1()[2]),
                    (self.group0()[1] * other.group1()[0]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        3        4        0
    // no simd        9       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group1()[2], self.group1()[0], self.group1()[1]]))
                + (swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[0]]))
                - (swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for VersorEvenAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        2        2        0
    // Totals...
    // yes simd        5        8        0
    //  no simd        9       12        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))
                + Simd32x3::from([
                    (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2])),
                    (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<Dipole> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for VersorEvenAtOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125, e5
            (self.group1() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPoint> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for VersorEvenAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       11        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       13        0
    //  no simd       12       19        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group1(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1]))])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[2] * other.group1()[1])),
                (-(self.group1()[0] * other.group1()[2]) + (self.group1()[2] * other.group1()[0])),
                ((self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorEvenAtOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       12        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            Simd32x3::from([
                (-(self.group0()[1] * other.group0()[3]) + (self.group0()[2] * other.group0()[2])),
                ((self.group0()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[1])),
                (-(self.group0()[0] * other.group0()[2]) + (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            Simd32x3::from([
                ((self.group1()[1] * other.group0()[3]) - (self.group1()[2] * other.group0()[2])),
                (-(self.group1()[0] * other.group0()[3]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<Horizon> for VersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group0() * Simd32x4::from(other[e3215])));
    }
}
impl AntiWedge<Line> for VersorEvenAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4        9        0
    //  no simd        7       12        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[1]),
                    (self.group0()[0] * other.group1()[2]),
                    (self.group0()[1] * other.group1()[0]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for VersorEvenAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        2        3        0
    // no simd        4        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (-(Simd32x2::from(other.group0()[2]) * Simd32x2::from([self.group0()[2], self.group1()[2]]))
                - (Simd32x2::from(other.group0()[0]) * Simd32x2::from([self.group0()[0], self.group1()[0]]))
                - (Simd32x2::from(other.group0()[1]) * Simd32x2::from([self.group0()[1], self.group1()[1]]))),
        );
    }
}
impl AntiWedge<Motor> for VersorEvenAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       14        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[0], other.group0()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1]))])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for VersorEvenAtOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                ((self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for VersorEvenAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       20       32        0
    //    simd3        1        4        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       23       38        0
    //  no simd       31       52        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other.group9()[0]) - (self.group1()[2] * other.group3()[2]) - (self.group1()[1] * other.group3()[1]) - (self.group1()[0] * other.group3()[0])
                    + (self.group0()[3] * other[e45])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group8()[2], other.group8()[0], other.group8()[1], other.group6()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * Simd32x4::from([other.group8()[1], other.group8()[2], other.group8()[0], other.group0()[1]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group7()[1]) + (self.group1()[1] * other.group7()[2])),
                    ((self.group1()[2] * other.group7()[0]) - (self.group1()[0] * other.group7()[2])),
                    (-(self.group1()[1] * other.group7()[0]) + (self.group1()[0] * other.group7()[1])),
                    (-(self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
                ])),
            // e5
            ((self.group1()[3] * other.group0()[1]) - (self.group1()[2] * other.group6()[2]) - (self.group1()[0] * other.group6()[0]) - (self.group1()[1] * other.group6()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from([
                ((self.group1()[1] * other.group9()[3]) - (self.group1()[2] * other.group9()[2])),
                (-(self.group1()[0] * other.group9()[3]) + (self.group1()[2] * other.group9()[1])),
                ((self.group1()[0] * other.group9()[2]) - (self.group1()[1] * other.group9()[1])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                + (Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from(0.0),
            // e423, e431, e412
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group1()[2], self.group1()[0], self.group1()[1]]))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorEvenAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group1() * Simd32x4::from(other[e1234])));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[0] * other.group0()[3]),
            (self.group1()[1] * other.group0()[3]),
            (self.group1()[2] * other.group0()[3]),
            ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Plane> for VersorEvenAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorEvenAtOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        2        4        0
    // no simd        6       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
            // e15, e25, e35
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group1()[2], self.group1()[0], self.group1()[1]]))),
        );
    }
}
impl AntiWedge<Sphere> for VersorEvenAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       16        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        7       18        0
    //  no simd       10       24        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group0(), 1, 2, 0, 3))
                + (Simd32x4::from([
                    (self.group0()[1] * other.group0()[2]),
                    (self.group0()[2] * other.group0()[0]),
                    (self.group0()[0] * other.group0()[1]),
                    (self.group1()[3] * other[e4315]),
                ]) * Simd32x4::from([-1.0, -1.0, -1.0, 1.0]))),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group1()[0] * other[e4315])),
                ((self.group0()[1] * other.group0()[3]) + (self.group1()[1] * other[e4315])),
                ((self.group0()[2] * other.group0()[3]) + (self.group1()[2] * other[e4315])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        1        2        0
    // no simd        4        8        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((self.group0() * Simd32x4::from(other.group0()[0])) + (self.group1() * Simd32x4::from(other.group0()[1]))),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorEvenAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for VersorEvenAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       20        0
    //  no simd       15       26        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[0], self.group0()[3]]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1]) + (self.group0()[2] * other.group2()[1])),
                    (-(self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorEvenAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       20        0
    //  no simd       15       26        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[0], self.group0()[3]]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1]) + (self.group0()[2] * other.group2()[1])),
                    (-(self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorEvenAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       14        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                ((self.group1()[3] * other.group0()[0]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * Simd32x4::from([other.group2()[1], other.group2()[2], other.group2()[0], other.group0()[0]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1]))])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       12        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorEvenAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       19        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       20        0
    //  no simd        9       23        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[0], self.group0()[3]]))
                + Simd32x4::from([
                    ((self.group1()[2] * other.group0()[1]) * -1.0),
                    ((self.group1()[0] * other.group0()[2]) * -1.0),
                    ((self.group1()[1] * other.group0()[0]) * -1.0),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorEvenAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       12        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for VersorEvenAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group2()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group3(), 1, 2, 0, 3))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[3] * other.group2()[3])
                        - (self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group3()[3]) + (self.group1()[0] * other.group2()[3])),
                ((self.group0()[1] * other.group3()[3]) + (self.group1()[1] * other.group2()[3])),
                ((self.group0()[2] * other.group3()[3]) + (self.group1()[2] * other.group2()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group3()[2]) - (self.group1()[2] * other.group3()[1])),
                (-(self.group1()[0] * other.group3()[2]) + (self.group1()[2] * other.group3()[0])),
                ((self.group1()[0] * other.group3()[1]) - (self.group1()[1] * other.group3()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorEvenAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group2(), 1, 2, 0, 3))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[1] * other.group2()[3]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[2] * other.group2()[3]) + (self.group1()[2] * other.group1()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[2] * other.group2()[1])),
                (-(self.group1()[0] * other.group2()[2]) + (self.group1()[2] * other.group2()[0])),
                ((self.group1()[0] * other.group2()[1]) - (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorEvenAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       11        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       13        0
    //  no simd       12       19        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group0()[3]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group2(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2]))])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[2] * other.group2()[1])),
                (-(self.group1()[0] * other.group2()[2]) + (self.group1()[2] * other.group2()[0])),
                ((self.group1()[0] * other.group2()[1]) - (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        7        8        0
    //  no simd       13       14        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (self.group1() * Simd32x4::from(other.group1()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorEvenAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       19        0
    //  no simd        9       22        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group1(), 2, 3, 1, 0) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group1()[3]) * -1.0),
                    ((self.group0()[2] * other.group1()[1]) * -1.0),
                    ((self.group0()[0] * other.group1()[2]) * -1.0),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[1] * other.group1()[0]),
                (self.group1()[2] * other.group1()[0]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[3]) - (self.group1()[2] * other.group1()[2])),
                (-(self.group1()[0] * other.group1()[3]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[1] * other.group1()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorEvenAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        7        8        0
    //  no simd       13       14        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((self.group0() * Simd32x4::from(other.group1()[3]))
                + (self.group1() * Simd32x4::from(other.group2()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl InfixAntiWedge for VersorEvenOnOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for VersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2       10        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiDualNum> for VersorEvenOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        1        6        0
    //  no simd        1        9        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[0]),
                (self.group0()[1] * other.group0()[0]),
                (self.group0()[2] * other.group0()[0]),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group0()[0])),
            ]),
            // e15, e25, e35, e3215
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for VersorEvenOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other[e321]) * Simd32x4::from([self.group0()[3], self.group1()[0], self.group1()[1], self.group1()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorEvenOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlector> for VersorEvenOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e1, e2, e3, e5
            ((Simd32x4::from(self.group0()[3]) * other.group1())
                - (swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for VersorEvenOnOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            ((self.group0()[3] * other.group0()[1]) + (self.group1()[0] * other.group0()[0])),
            ((self.group0()[3] * other.group0()[2]) + (self.group1()[1] * other.group0()[0])),
            ((self.group0()[3] * other.group0()[3]) + (self.group1()[2] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorEvenOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for VersorEvenOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiMotor> for VersorEvenOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + (Simd32x4::from(self.group0()[3]) * other.group0())
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group1()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for VersorEvenOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for VersorEvenOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for VersorEvenOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiScalar> for VersorEvenOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other[e12345])),
            // e415, e425, e435, e4
            (self.group1() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for VersorEvenOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorEvenOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group0()[3]) * other.group0())
                + Simd32x4::from([
                    (self.group1()[0] * other.group1()[3]),
                    (self.group1()[1] * other.group1()[3]),
                    (self.group1()[2] * other.group1()[3]),
                    (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) - (self.group1()[0] * other.group1()[0])),
                ])),
            // e23, e31, e12, e1234
            (Simd32x4::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorEvenOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4, e1, e2, e3
            ((Simd32x4::from(self.group0()[3]) * other.group1())
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                ])),
        );
    }
}
impl AntiWedge<Circle> for VersorEvenOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       10       22        0
    //  no simd       13       28        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group1()[3]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[1] * other.group1()[3]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group1()[2] * other.group1()[3]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorEvenOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       20        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       21        0
    //  no simd       10       24        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group2()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group2()[1]),
                    (self.group0()[0] * other.group2()[2]),
                    (self.group0()[1] * other.group2()[0]),
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorEvenOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       14        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        7       16        0
    //  no simd       10       22        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorEvenOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                (-(self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for VersorEvenOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorEvenOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       18        0
    //    simd4        0        1        0
    // Totals...
    // yes simd       10       19        0
    //  no simd       10       22        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                ((self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<Dipole> for VersorEvenOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       13        0
    //  no simd        5       16        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[3]) * other.group1()),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorEvenOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       10        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorEvenOnOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        9        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        5       13        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
            ]),
            // e23, e31, e12, e45
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorEvenOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorEvenOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ (Simd32x4::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorEvenOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<DualNum> for VersorEvenOnOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e415, e425, e435, e4
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125, e5
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[0])]),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorEvenOnOrigin {
    type Output = FlatOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return FlatOrigin::from_groups(/* e45 */ (self.group0()[3] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for VersorEvenOnOrigin {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        7        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[3])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorEvenOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Flector> for VersorEvenOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       15        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        9       18        0
    //  no simd       15       27        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(other.group1(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1]))])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group1()[3]),
                (self.group0()[1] * other.group1()[3]),
                (self.group0()[2] * other.group1()[3]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group1()[3])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorEvenOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group0()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorEvenOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(other.group0(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group0(), 2, 3, 1, 0))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[1] * other.group0()[2]) - (self.group1()[0] * other.group0()[1]))])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (self.group0()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<Horizon> for VersorEvenOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
            // e15, e25, e35, e3215
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<Infinity> for VersorEvenOnOrigin {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group0()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for VersorEvenOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        4       15        0
    //  no simd        7       18        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[1]),
                    (self.group0()[0] * other.group1()[2]),
                    (self.group0()[1] * other.group1()[0]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorEvenOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e1, e2, e3, e5
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<LineOnOrigin> for VersorEvenOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from(0.0),
            // e415, e425, e435, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<Motor> for VersorEvenOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       19        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        9       21        0
    //  no simd       12       27        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group1()[1]),
                    (self.group0()[0] * other.group1()[2]),
                    (self.group0()[1] * other.group1()[0]),
                    ((self.group1()[3] * other.group0()[3]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorEvenOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e1, e2, e3, e5
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * swizzle!(other.group0(), 1, 2, 0, 3))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0]))])),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for VersorEvenOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e415, e425, e435, e4
            ((other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MultiVector> for VersorEvenOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       21       37        0
    //    simd3        3        7        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       29       50        0
    //  no simd       50       82        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other[e45]) - (self.group1()[2] * other.group5()[2]) - (self.group1()[1] * other.group5()[1]) - (self.group1()[0] * other.group5()[0])
                    + (self.group0()[3] * other.group0()[0])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                (self.group0()[3] * other.group0()[1]),
            ]),
            // e1, e2, e3, e4
            ((self.group1() * Simd32x4::from([other.group6()[3], other.group6()[3], other.group6()[3], other.group0()[1]])) + (Simd32x4::from(self.group0()[3]) * other.group1())
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group8()[2], other.group8()[0], other.group8()[1], other.group6()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group8()[1]),
                    (self.group0()[0] * other.group8()[2]),
                    (self.group0()[1] * other.group8()[0]),
                    (-(self.group1()[2] * other.group7()[2])
                        - (self.group1()[1] * other.group7()[1])
                        - (self.group1()[0] * other.group7()[0])
                        - (self.group0()[0] * other.group6()[0])
                        - (self.group0()[1] * other.group6()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group8()[2]) - (self.group1()[1] * other.group8()[1]) + (self.group0()[3] * other[e1]) - (self.group1()[0] * other.group8()[0])),
            // e41, e42, e43, e45
            ((Simd32x4::from(self.group0()[3]) * other.group3())
                - (swizzle!(other.group9(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group9()[0]) + (self.group0()[2] * other.group9()[2])),
                    ((self.group1()[1] * other.group9()[0]) + (self.group0()[0] * other.group9()[3])),
                    ((self.group1()[2] * other.group9()[0]) + (self.group0()[1] * other.group9()[1])),
                    (-(self.group1()[1] * other.group9()[2]) - (self.group1()[0] * other.group9()[1])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * other.group4()) + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])) + (Simd32x3::from(self.group0()[3]) * other.group5())),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group6()[0]) + (self.group1()[0] * other.group0()[1])),
                ((self.group0()[3] * other.group6()[1]) + (self.group1()[1] * other.group0()[1])),
                ((self.group0()[3] * other.group6()[2]) + (self.group1()[2] * other.group0()[1])),
                (self.group0()[3] * other.group6()[3]),
            ]),
            // e423, e431, e412
            ((Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])) + (Simd32x3::from(self.group0()[3]) * other.group7())),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[3]) * other.group8()),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[3]) * other.group9()),
            // e3215
            (self.group0()[3] * other[e45]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorEvenOnOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorEvenOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ (Simd32x3::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorEvenOnOrigin {
    type Output = NullVersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullVersorOddAtOrigin::from_groups(
            // e41, e42, e43, e1234
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]])),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorEvenOnOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorEvenOnOrigin {
    type Output = NullVersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return NullVersorOddAtOrigin::from_groups(/* e41, e42, e43, e1234 */ Simd32x4::from([
            ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
            ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
            ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
            (self.group0()[3] * other.group0()[3]),
        ]));
    }
}
impl AntiWedge<Origin> for VersorEvenOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group0()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for VersorEvenOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       17        0
    //  no simd        5       20        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorEvenOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for VersorEvenOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        1        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        2        0
    //  no simd        0        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e5
            (self.group0()[3] * other[e2]),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for VersorEvenOnOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        1        0
    // no simd        0        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return RoundPointAtOrigin::from_groups(/* e4, e5 */ (Simd32x2::from(self.group0()[3]) * other.group0()));
    }
}
impl AntiWedge<Scalar> for VersorEvenOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Scalar) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[scalar]));
    }
}
impl AntiWedge<Sphere> for VersorEvenOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       20        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        8       21        0
    //  no simd        8       24        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[0] * other[e4315]) - (self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group1()[1] * other[e4315]) + (self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                ((self.group1()[2] * other[e4315]) - (self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (self.group0()[3] * other[e4315]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorEvenOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        4        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        6        0
    //  no simd        0       12        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (self.group1() * Simd32x4::from([other.group0()[1], other.group0()[1], other.group0()[1], other.group0()[0]])),
            // e23, e31, e12, e3215
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
            ]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorEvenOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        5       10        0
    //  no simd        8       16        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(other.group0(), 2, 0, 1, 2) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group0()[2] * other.group0()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group0()[1] * other.group0()[0])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[3]) * swizzle!(other.group0(), 3, 0, 1, 2)),
        );
    }
}
impl AntiWedge<VersorEven> for VersorEvenOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       29        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       16       32        0
    //  no simd       25       41        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[1] * other.group2()[1]) + (self.group0()[3] * other.group2()[3])
                    - (self.group1()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((self.group1() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[3]])) + (Simd32x4::from(self.group0()[3]) * other.group3())
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group2()[1]),
                    (self.group0()[0] * other.group2()[2]),
                    (self.group0()[1] * other.group2()[0]),
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorEvenOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       26        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       16       28        0
    //  no simd       22       34        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[1] * other.group2()[1]) + (self.group0()[3] * other.group2()[3])
                    - (self.group1()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + (swizzle!(self.group0(), 2, 0, 1, 3) * Simd32x4::from([other.group2()[1], other.group2()[2], other.group2()[0], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[3] * other.group0()[3])
                        - (self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorEvenOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       22        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       12       25        0
    //  no simd       18       34        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e415, e425, e435, e321
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[0])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[0])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[0])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0]),
                (self.group0()[3] * other.group2()[1]),
                (self.group0()[3] * other.group2()[2]),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[1] * other.group2()[1]) + (self.group0()[3] * other.group2()[3])
                    - (self.group1()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((self.group1() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[0]]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group0()[3]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorEvenOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       19        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       20        0
    //  no simd        9       23        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group0(), 2, 0, 1, 3) * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[0], other.group0()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group1()[2]) * -1.0),
                    ((self.group0()[2] * other.group1()[0]) * -1.0),
                    ((self.group0()[0] * other.group1()[1]) * -1.0),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorEvenOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       13        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group0()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e415, e425, e435, e4
            ((Simd32x4::from(self.group0()[3]) * other.group1())
                + (self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorEvenOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       19        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       12       21        0
    //  no simd       15       27        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group0()[3]) * other.group2())
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
                    ((self.group1()[2] * other.group0()[3]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOdd> for VersorEvenOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       21        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       13       26        0
    //  no simd       25       41        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((self.group1() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group3()[3]])) + (Simd32x4::from(self.group0()[3]) * other.group0())
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group2()[2]]))
                + Simd32x4::from([
                    (self.group0()[2] * other.group3()[1]),
                    (self.group0()[0] * other.group3()[2]),
                    (self.group0()[1] * other.group3()[0]),
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            ((self.group0() * Simd32x4::from([other.group3()[3], other.group3()[3], other.group3()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group1()[2] * other.group3()[2]) - (self.group1()[1] * other.group3()[1]) - (self.group1()[0] * other.group3()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group2()[0]) + (self.group1()[0] * other.group3()[3])),
                ((self.group0()[3] * other.group2()[1]) + (self.group1()[1] * other.group3()[3])),
                ((self.group0()[3] * other.group2()[2]) + (self.group1()[2] * other.group3()[3])),
                (self.group0()[3] * other.group2()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group3()),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorEvenOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       22        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       12       25        0
    //  no simd       18       34        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((self.group1() * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group2()[3]]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group0()[0] * other.group2()[3]),
                (self.group0()[1] * other.group2()[3]),
                (self.group0()[2] * other.group2()[3]),
                (-(self.group1()[2] * other.group2()[2]) - (self.group1()[1] * other.group2()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group1()[0] * other.group2()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group2()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group2()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group2()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorEvenOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       18        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       22       34        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group0()[3]]))
                + (swizzle!(other.group2(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1]) - (self.group1()[0] * other.group1()[0])
                        + (self.group0()[3] * other.group0()[0])
                        - (self.group0()[0] * other.group0()[1])
                        - (self.group0()[1] * other.group0()[2])),
                ])),
            // e23, e31, e12, e45
            ((self.group0() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[3] * other.group1()[0]),
                    (self.group0()[3] * other.group1()[1]),
                    (self.group0()[3] * other.group1()[2]),
                    (-(self.group1()[2] * other.group2()[2]) - (self.group1()[1] * other.group2()[1]) - (self.group1()[0] * other.group2()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[0] * other.group2()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[1] * other.group2()[3])),
                ((self.group0()[3] * other.group0()[3]) + (self.group1()[2] * other.group2()[3])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (Simd32x4::from(self.group0()[3]) * other.group2()),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorEvenOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group1()[3]),
                    (self.group1()[1] * other.group1()[3]),
                    (self.group1()[2] * other.group1()[3]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group0()[3] * other.group1()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorEvenOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            ((Simd32x4::from(self.group0()[3]) * other.group0())
                - (swizzle!(other.group1(), 3, 1, 2, 3) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[0], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group1()[0]) + (self.group0()[2] * other.group1()[2])),
                    ((self.group1()[1] * other.group1()[0]) + (self.group0()[0] * other.group1()[3])),
                    ((self.group1()[2] * other.group1()[0]) + (self.group0()[1] * other.group1()[1])),
                    (-(self.group1()[1] * other.group1()[2]) - (self.group1()[0] * other.group1()[1])),
                ])),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(self.group0()[3]) * other.group1()),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorEvenOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group0()[3]) * other.group0())
                + (self.group1() * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) + (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group1()[3]) + (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group1()[3]) + (self.group0()[3] * other.group1()[2])),
                (self.group0()[3] * other.group1()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group2()[0]) + (self.group1()[0] * other.group1()[3])),
                ((self.group0()[3] * other.group2()[1]) + (self.group1()[1] * other.group1()[3])),
                ((self.group0()[3] * other.group2()[2]) + (self.group1()[2] * other.group1()[3])),
                (self.group0()[3] * other.group2()[3]),
            ]),
        );
    }
}
impl InfixAntiWedge for VersorEvenOrthogonalOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiDualNum> for VersorEvenOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group2()[3]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<AntiMotor> for VersorEvenOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group1()[3]),
            (self.group0()[1] * other.group1()[3]),
            (self.group0()[2] * other.group1()[3]),
            ((self.group2()[3] * other.group1()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<AntiScalar> for VersorEvenOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other[e12345])),
            // e235, e315, e125, e5
            (self.group1() * Simd32x4::from(other[e12345])),
            // e1, e2, e3, e4
            (self.group2() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[0] * other.group1()[3]),
            (self.group1()[1] * other.group1()[3]),
            (self.group1()[2] * other.group1()[3]),
            ((self.group1()[3] * other.group1()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Circle> for VersorEvenOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       13       18        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1])
                        + (self.group1()[1] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[0])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[1])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0])
                        + (self.group1()[0] * other.group0()[1])
                        + (self.group0()[3] * other.group1()[2])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorEvenOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       13       18        0
    //  no simd       16       21        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1])
                        + (self.group1()[1] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[0])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[1])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0])
                        + (self.group1()[0] * other.group0()[1])
                        + (self.group0()[3] * other.group1()[2])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorEvenOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        3        4        0
    // no simd        9       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group1()[2], self.group1()[0], self.group1()[1]]))
                + (swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[0]]))
                - (swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) + (self.group0()[3] * other.group1()[0]) + (self.group1()[1] * other.group0()[2])),
                ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[1]) - (self.group1()[0] * other.group0()[2])),
                (-(self.group1()[1] * other.group0()[0]) + (self.group0()[3] * other.group1()[2]) + (self.group1()[0] * other.group0()[1])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd3        2        2        0
    // Totals...
    // yes simd        5        8        0
    //  no simd        9       12        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group1(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group1(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))
                + Simd32x3::from([
                    (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2])),
                    (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<Dipole> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[3] * other.group1()[3])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6        7        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[3] * other.group0()[3])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group2()[2])
                - (self.group0()[0] * other.group2()[0])
                - (self.group0()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for VersorEvenOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e235, e315, e125, e5
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e1, e2, e3, e4
            (self.group2() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for VersorEvenOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[3]]))
                + (swizzle!(other.group1(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[2] * other.group1()[2]) + (self.group2()[1] * other.group1()[1]) + (self.group2()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group0()[2])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group1()[3]) - (self.group0()[3] * other.group1()[0])),
                ((self.group0()[1] * other.group1()[3]) - (self.group0()[3] * other.group1()[1])),
                ((self.group0()[2] * other.group1()[3]) - (self.group0()[3] * other.group1()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[2] * other.group1()[1])),
                (-(self.group1()[0] * other.group1()[2]) + (self.group1()[2] * other.group1()[0])),
                ((self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorEvenOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[0] * other.group0()[3]),
            (self.group0()[1] * other.group0()[3]),
            (self.group0()[2] * other.group0()[3]),
            ((self.group2()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       14        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       16        0
    //  no simd       12       22        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * swizzle!(other.group0(), 3, 1, 2, 0))
                + (swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group2()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group2()[1] * other.group0()[2]) + (self.group2()[0] * other.group0()[1]))])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[3]) - (self.group1()[2] * other.group0()[2])),
                (-(self.group1()[0] * other.group0()[3]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[1] * other.group0()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Horizon> for VersorEvenOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group2()[3]])),
        );
    }
}
impl AntiWedge<Line> for VersorEvenOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group0()[2], self.group0()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Motor> for VersorEvenOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       15        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        9       18        0
    //  no simd       15       27        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            ((self.group2() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group0()[0]) + (self.group0()[2] * other.group1()[1])),
                    ((self.group0()[3] * other.group0()[1]) + (self.group0()[0] * other.group1()[2])),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group1()[0])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
            ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
            (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            ((other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group2()[3]]))
                + Simd32x4::from([
                    (self.group2()[0] * other.group0()[3]),
                    (self.group2()[1] * other.group0()[3]),
                    (self.group2()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<MultiVector> for VersorEvenOrthogonalOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       30       43        0
    //    simd3        2        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       34       50        0
    //  no simd       44       66        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group2()[3] * other[e45])
                    + (self.group2()[2] * other.group9()[3])
                    + (self.group2()[1] * other.group9()[2])
                    + (self.group2()[0] * other.group9()[1])
                    + (self.group1()[3] * other.group9()[0])
                    - (self.group1()[2] * other.group3()[2])
                    - (self.group1()[1] * other.group3()[1])
                    - (self.group1()[0] * other.group3()[0])
                    - (self.group0()[3] * other.group3()[3])
                    - (self.group0()[2] * other.group4()[2])
                    - (self.group0()[0] * other.group4()[0])
                    - (self.group0()[1] * other.group4()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((self.group2() * Simd32x4::from(other.group0()[1]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group8()[2], other.group8()[0], other.group8()[1], other.group6()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group7()[1])
                        + (self.group1()[1] * other.group7()[2])
                        + (self.group0()[3] * other.group6()[0])
                        + (self.group0()[2] * other.group8()[1])),
                    ((self.group1()[2] * other.group7()[0]) - (self.group1()[0] * other.group7()[2])
                        + (self.group0()[3] * other.group6()[1])
                        + (self.group0()[0] * other.group8()[2])),
                    (-(self.group1()[1] * other.group7()[0])
                        + (self.group1()[0] * other.group7()[1])
                        + (self.group0()[3] * other.group6()[2])
                        + (self.group0()[1] * other.group8()[0])),
                    (-(self.group0()[0] * other.group6()[0]) - (self.group0()[1] * other.group6()[1])),
                ])),
            // e5
            ((self.group1()[3] * other.group0()[1]) - (self.group1()[2] * other.group6()[2]) - (self.group1()[0] * other.group6()[0]) - (self.group1()[1] * other.group6()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                (-(self.group0()[1] * other.group9()[3]) + (self.group0()[2] * other.group9()[2])),
                ((self.group0()[0] * other.group9()[3]) - (self.group0()[2] * other.group9()[1])),
                (-(self.group0()[0] * other.group9()[2]) + (self.group0()[1] * other.group9()[1])),
                0.0,
            ]),
            // e15, e25, e35
            Simd32x3::from([
                ((self.group1()[1] * other.group9()[3]) - (self.group1()[2] * other.group9()[2])),
                (-(self.group1()[0] * other.group9()[3]) + (self.group1()[2] * other.group9()[1])),
                ((self.group1()[0] * other.group9()[2]) - (self.group1()[1] * other.group9()[1])),
            ]),
            // e23, e31, e12
            ((Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                + (Simd32x3::from(other[e45]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]]))
                - (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e423, e431, e412
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[0], self.group0()[1], self.group0()[2]])),
            // e235, e315, e125
            (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]])),
            // e1234, e4235, e4315, e4125
            Simd32x4::from(0.0),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group1()[2], self.group1()[0], self.group1()[1]]))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorEvenOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ (self.group1() * Simd32x4::from(other[e1234])));
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
            (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
            ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[0] * other.group0()[3]),
            (self.group1()[1] * other.group0()[3]),
            (self.group1()[2] * other.group0()[3]),
            ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Plane> for VersorEvenOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       21        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       12       25        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group2()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[1]) * -1.0),
                    ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) - (self.group0()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) - (self.group0()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) - (self.group0()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       21        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group0()[1] * other.group0()[2]) + (self.group0()[2] * other.group0()[1])),
                ((self.group0()[0] * other.group0()[2]) - (self.group0()[2] * other.group0()[0])),
                (-(self.group0()[0] * other.group0()[1]) + (self.group0()[1] * other.group0()[0])),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Sphere> for VersorEvenOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       25        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       13       26        0
    //  no simd       16       29        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group2()[3]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[1]) * -1.0),
                    ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other[e4315]) + (self.group2()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[0] * other[e4315]) + (self.group0()[0] * other.group0()[3]) - (self.group0()[3] * other.group0()[0])),
                ((self.group1()[1] * other[e4315]) + (self.group0()[1] * other.group0()[3]) - (self.group0()[3] * other.group0()[1])),
                ((self.group1()[2] * other[e4315]) + (self.group0()[2] * other.group0()[3]) - (self.group0()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        1        2        0
    // no simd        4        8        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group2()[3]]))
                + (self.group1() * Simd32x4::from(other.group0()[1]))),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       21        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       12       25        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group2()[2]]))
                + Simd32x4::from([
                    ((self.group0()[1] * other.group0()[2]) * -1.0),
                    ((self.group0()[2] * other.group0()[0]) * -1.0),
                    ((self.group0()[0] * other.group0()[1]) * -1.0),
                    ((self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (-(self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                (-(self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                (-(self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for VersorEvenOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       21        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       15       24        0
    //  no simd       21       33        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((self.group2() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1])
                        + (self.group1()[1] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[0])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[1])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0])
                        + (self.group1()[0] * other.group0()[1])
                        + (self.group0()[3] * other.group1()[2])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorEvenOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       21        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       15       24        0
    //  no simd       21       33        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((self.group2() * Simd32x4::from(other.group0()[3]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1])
                        + (self.group1()[1] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[0])
                        + (self.group0()[2] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2])
                        + (self.group0()[3] * other.group1()[1])
                        + (self.group0()[0] * other.group2()[2])),
                    (-(self.group1()[1] * other.group0()[0])
                        + (self.group1()[0] * other.group0()[1])
                        + (self.group0()[3] * other.group1()[2])
                        + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorEvenOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       15        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        9       18        0
    //  no simd       15       27        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                ((self.group1()[3] * other.group0()[0]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((self.group2() * Simd32x4::from(other.group0()[0]))
                - (swizzle!(self.group0(), 1, 2, 0, 2) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group0()[3] * other.group1()[0]) + (self.group0()[2] * other.group2()[1])),
                    ((self.group0()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[2])),
                    ((self.group0()[3] * other.group1()[2]) + (self.group0()[1] * other.group2()[0])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       12        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       19        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       12       21        0
    //  no simd       15       27        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group0() * Simd32x4::from(other.group0()[3])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            ((self.group2() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[1]) + (self.group0()[3] * other.group1()[0]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[3] * other.group1()[1]) - (self.group1()[0] * other.group0()[2])),
                    (-(self.group1()[1] * other.group0()[0]) + (self.group0()[3] * other.group1()[2]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       12        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            (-(self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[1] * other.group1()[2]) + (self.group0()[2] * other.group1()[1])),
            ((self.group1()[2] * other.group0()[0]) - (self.group1()[0] * other.group0()[2]) + (self.group0()[0] * other.group1()[2]) - (self.group0()[2] * other.group1()[0])),
            (-(self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[1]) - (self.group0()[0] * other.group1()[1]) + (self.group0()[1] * other.group1()[0])),
        ]));
    }
}
impl AntiWedge<VersorOdd> for VersorEvenOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       25        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       20       27        0
    //  no simd       26       33        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group1()[3]]))
                + (swizzle!(other.group3(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[2] * other.group3()[2])
                        + (self.group2()[1] * other.group3()[1])
                        + (self.group2()[0] * other.group3()[0])
                        + (self.group1()[3] * other.group2()[3])
                        - (self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group2()[3]) + (self.group0()[0] * other.group3()[3]) - (self.group0()[3] * other.group3()[0])),
                ((self.group1()[1] * other.group2()[3]) + (self.group0()[1] * other.group3()[3]) - (self.group0()[3] * other.group3()[1])),
                ((self.group1()[2] * other.group2()[3]) + (self.group0()[2] * other.group3()[3]) - (self.group0()[3] * other.group3()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group3()[2]) - (self.group1()[2] * other.group3()[1])),
                (-(self.group1()[0] * other.group3()[2]) + (self.group1()[2] * other.group3()[0])),
                ((self.group1()[0] * other.group3()[1]) - (self.group1()[1] * other.group3()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorEvenOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       25        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       20       27        0
    //  no simd       26       33        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group0()[3]]))
                + (swizzle!(other.group2(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[2] * other.group2()[2])
                        + (self.group2()[1] * other.group2()[1])
                        + (self.group2()[0] * other.group2()[0])
                        + (self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[3]) + (self.group0()[0] * other.group2()[3]) - (self.group0()[3] * other.group2()[0])),
                ((self.group1()[1] * other.group1()[3]) + (self.group0()[1] * other.group2()[3]) - (self.group0()[3] * other.group2()[1])),
                ((self.group1()[2] * other.group1()[3]) + (self.group0()[2] * other.group2()[3]) - (self.group0()[3] * other.group2()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[2] * other.group2()[1])),
                (-(self.group1()[0] * other.group2()[2]) + (self.group1()[2] * other.group2()[0])),
                ((self.group1()[0] * other.group2()[1]) - (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorEvenOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[3]]))
                + (swizzle!(other.group2(), 1, 2, 0, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[2] * other.group2()[2]) + (self.group2()[1] * other.group2()[1]) + (self.group2()[0] * other.group2()[0])
                        - (self.group0()[2] * other.group0()[3])
                        - (self.group0()[0] * other.group0()[1])
                        - (self.group0()[1] * other.group0()[2])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group2()[3]) - (self.group0()[3] * other.group2()[0])),
                ((self.group0()[1] * other.group2()[3]) - (self.group0()[3] * other.group2()[1])),
                ((self.group0()[2] * other.group2()[3]) - (self.group0()[3] * other.group2()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[2] * other.group2()[1])),
                (-(self.group1()[0] * other.group2()[2]) + (self.group1()[2] * other.group2()[0])),
                ((self.group1()[0] * other.group2()[1]) - (self.group1()[1] * other.group2()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        7        8        0
    //  no simd       13       14        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group2()[3]]))
                + (self.group1() * Simd32x4::from(other.group1()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorEvenOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(self.group0(), 1, 2, 0, 3) * Simd32x4::from([other.group1()[3], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + (swizzle!(other.group1(), 2, 3, 1, 3) * Simd32x4::from([self.group0()[2], self.group0()[0], self.group0()[1], self.group2()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[1] * other.group1()[2]) + (self.group2()[0] * other.group1()[1]) + (self.group1()[3] * other.group1()[0])
                        - (self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (-(self.group0()[3] * other.group1()[1]) + (self.group1()[0] * other.group1()[0])),
                (-(self.group0()[3] * other.group1()[2]) + (self.group1()[1] * other.group1()[0])),
                (-(self.group0()[3] * other.group1()[3]) + (self.group1()[2] * other.group1()[0])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[1] * other.group1()[3]) - (self.group1()[2] * other.group1()[2])),
                (-(self.group1()[0] * other.group1()[3]) + (self.group1()[2] * other.group1()[1])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[1] * other.group1()[1])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorEvenOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        7        8        0
    //  no simd       13       14        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group2()[3]]))
                + (self.group1() * Simd32x4::from(other.group2()[3]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl InfixAntiWedge for VersorOdd {}
impl AntiWedge<AntiCircleOnOrigin> for VersorOdd {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group3(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group3()[3] * other.group0()[0]) - (self.group3()[2] * other.group1()[1])),
                    (-(self.group3()[3] * other.group0()[1]) - (self.group3()[0] * other.group1()[2])),
                    (-(self.group3()[3] * other.group0()[2]) - (self.group3()[1] * other.group1()[0])),
                    ((self.group3()[0] * other.group0()[0]) + (self.group3()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorOdd {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[2]]))
                + Simd32x4::from([
                    (self.group3()[1] * other.group0()[2]),
                    (self.group3()[2] * other.group0()[0]),
                    (self.group3()[0] * other.group0()[1]),
                    (-(self.group2()[1] * other.group0()[1]) - (self.group1()[3] * other.group0()[3]) - (self.group2()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (-(self.group3()[0] * other.group0()[3]) + (self.group3()[3] * other.group0()[0])),
                (-(self.group3()[1] * other.group0()[3]) + (self.group3()[3] * other.group0()[1])),
                (-(self.group3()[2] * other.group0()[3]) + (self.group3()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDualNum> for VersorOdd {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group2()[3]])),
            // e1, e2, e3, e5
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for VersorOdd {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e321]) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorOdd {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (-(Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group2()[3] * other.group0()[0]),
                    (self.group2()[3] * other.group0()[1]),
                    (self.group2()[3] * other.group0()[2]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group3()[1] * other.group0()[2]) + (self.group3()[2] * other.group0()[1])),
                ((self.group3()[0] * other.group0()[2]) - (self.group3()[2] * other.group0()[0])),
                (-(self.group3()[0] * other.group0()[1]) + (self.group3()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for VersorOdd {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                - (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group3()[2] * other.group1()[2]) + (self.group3()[1] * other.group1()[1]) + (self.group3()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group0()[2])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group3()[1] * other.group0()[2]) + (self.group3()[2] * other.group0()[1])),
                ((self.group3()[0] * other.group0()[2]) - (self.group3()[2] * other.group0()[0])),
                (-(self.group3()[0] * other.group0()[1]) + (self.group3()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for VersorOdd {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group3()[0] * other.group0()[0] * -1.0),
            (self.group3()[1] * other.group0()[0] * -1.0),
            (self.group3()[2] * other.group0()[0] * -1.0),
            ((self.group3()[2] * other.group0()[3]) + (self.group3()[1] * other.group0()[2]) - (self.group1()[3] * other.group0()[0]) + (self.group3()[0] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorOdd {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group3(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[3] * other.group1()[0]) + (self.group3()[1] * other.group0()[2])),
                    ((self.group3()[2] * other.group0()[0]) + (self.group2()[3] * other.group1()[1])),
                    ((self.group2()[3] * other.group1()[2]) + (self.group3()[0] * other.group0()[1])),
                    (-(self.group3()[0] * other.group1()[0]) - (self.group3()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for VersorOdd {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group3()[1], self.group3()[2], self.group3()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group3()[2], self.group3()[0], self.group3()[1]]))),
        );
    }
}
impl AntiWedge<AntiMotor> for VersorOdd {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group2()[3]])),
            // e1, e2, e3, e5
            (-(swizzle!(self.group3(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group3()[1] * other.group0()[2]) + (self.group2()[3] * other.group1()[0])),
                    ((self.group3()[2] * other.group0()[0]) + (self.group2()[3] * other.group1()[1])),
                    ((self.group3()[0] * other.group0()[1]) + (self.group2()[3] * other.group1()[2])),
                    (-(self.group3()[1] * other.group1()[1]) - (self.group3()[0] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for VersorOdd {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group3()[1] * other.group0()[2]) - (self.group3()[2] * other.group0()[1])),
            (-(self.group3()[0] * other.group0()[2]) + (self.group3()[2] * other.group0()[0])),
            ((self.group3()[0] * other.group0()[1]) - (self.group3()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for VersorOdd {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group3()[2] * other.group0()[2]) + (self.group3()[1] * other.group0()[1]) + (self.group2()[3] * other.group0()[3]) + (self.group3()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for VersorOdd {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group3()[2] * other.group0()[2]) + (self.group3()[0] * other.group0()[0]) + (self.group3()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (self.group0() * Simd32x4::from(other[e12345])),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other[e12345])),
            // e15, e25, e35, e1234
            (self.group2() * Simd32x4::from(other[e12345])),
            // e4235, e4315, e4125, e3215
            (self.group3() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for VersorOdd {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group3()[3] * other.group0()[3]) + (self.group3()[2] * other.group0()[2]) + (self.group3()[0] * other.group0()[0]) + (self.group3()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorOdd {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        6       12        0
    //  no simd       12       24        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group3() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            ((swizzle!(self.group3(), 2, 1, 2, 0) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                - (swizzle!(other.group1(), 3, 1, 3, 0) * Simd32x4::from([self.group1()[3], self.group3()[2], self.group2()[1], self.group3()[1]]))
                + Simd32x4::from([
                    ((self.group3()[1] * other.group0()[1]) + (self.group3()[0] * other.group0()[0])),
                    (-(self.group3()[3] * other.group0()[0]) - (self.group2()[0] * other.group1()[3])),
                    (-(self.group3()[3] * other.group0()[1]) - (self.group3()[0] * other.group1()[2])),
                    (-(self.group3()[3] * other.group0()[2]) - (self.group2()[2] * other.group1()[3])),
                ])),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorOdd {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group3(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group3()[2] * other.group1()[3]) + (self.group3()[1] * other.group1()[2]) + (self.group3()[0] * other.group1()[1])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group1()[3] * other.group0()[3])
                        - (self.group2()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (-(self.group3()[0] * other.group0()[3]) + (self.group3()[3] * other.group0()[0])),
                (-(self.group3()[1] * other.group0()[3]) + (self.group3()[3] * other.group0()[1])),
                (-(self.group3()[2] * other.group0()[3]) + (self.group3()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Circle> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       25       36        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       26       37        0
    //  no simd       29       40        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group3()[2] * other.group0()[1]) + (self.group2()[3] * other.group1()[0]) + (self.group3()[1] * other.group0()[2])),
                ((self.group3()[2] * other.group0()[0]) + (self.group2()[3] * other.group1()[1]) - (self.group3()[0] * other.group0()[2])),
                (-(self.group3()[1] * other.group0()[0]) + (self.group2()[3] * other.group1()[2]) + (self.group3()[0] * other.group0()[1])),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[3] * other.group1()[3])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group3(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    ((self.group3()[3] * other.group0()[0]) + (self.group2()[3] * other.group2()[0])),
                    ((self.group3()[3] * other.group0()[1]) + (self.group2()[3] * other.group2()[1])),
                    ((self.group3()[3] * other.group0()[2]) + (self.group2()[3] * other.group2()[2])),
                    (-(self.group3()[0] * other.group1()[0]) - (self.group3()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group3()[3] * other.group1()[0]) - (self.group3()[1] * other.group2()[2]) + (self.group3()[2] * other.group2()[1])),
                ((self.group3()[3] * other.group1()[1]) + (self.group3()[0] * other.group2()[2]) - (self.group3()[2] * other.group2()[0])),
                ((self.group3()[3] * other.group1()[2]) - (self.group3()[0] * other.group2()[1]) + (self.group3()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       25       36        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group3()[2] * other.group0()[1]) + (self.group2()[3] * other.group1()[0]) + (self.group3()[1] * other.group0()[2])),
                ((self.group3()[2] * other.group0()[0]) + (self.group2()[3] * other.group1()[1]) - (self.group3()[0] * other.group0()[2])),
                (-(self.group3()[1] * other.group0()[0]) + (self.group2()[3] * other.group1()[2]) + (self.group3()[0] * other.group0()[1])),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                ((self.group2()[3] * other.group2()[0]) + (self.group3()[3] * other.group0()[0])),
                ((self.group2()[3] * other.group2()[1]) + (self.group3()[3] * other.group0()[1])),
                ((self.group2()[3] * other.group2()[2]) + (self.group3()[3] * other.group0()[2])),
                (-(self.group3()[2] * other.group1()[2]) - (self.group3()[0] * other.group1()[0]) - (self.group3()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group3()[3] * other.group1()[0]) - (self.group3()[1] * other.group2()[2]) + (self.group3()[2] * other.group2()[1])),
                ((self.group3()[3] * other.group1()[1]) + (self.group3()[0] * other.group2()[2]) - (self.group3()[2] * other.group2()[0])),
                ((self.group3()[3] * other.group1()[2]) - (self.group3()[0] * other.group2()[1]) + (self.group3()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       24        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       14       25        0
    //  no simd       17       28        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group1()[3] * other.group0()[3])
                    - (self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group3(), 0, 1, 2, 2) * swizzle!(other.group0(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group2()[3] * other.group1()[0]),
                    (self.group2()[3] * other.group1()[1]),
                    (self.group2()[3] * other.group1()[2]),
                    (-(self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group3()[3] * other.group0()[0]) - (self.group3()[1] * other.group1()[2]) + (self.group3()[2] * other.group1()[1])),
                ((self.group3()[3] * other.group0()[1]) + (self.group3()[0] * other.group1()[2]) - (self.group3()[2] * other.group1()[0])),
                ((self.group3()[3] * other.group0()[2]) - (self.group3()[0] * other.group1()[1]) + (self.group3()[1] * other.group1()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorOdd {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       14       24        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group3()[1] * other.group0()[2]) - (self.group3()[2] * other.group0()[1])),
                (-(self.group3()[0] * other.group0()[2]) + (self.group3()[2] * other.group0()[0])),
                ((self.group3()[0] * other.group0()[1]) - (self.group3()[1] * other.group0()[0])),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) + (self.group3()[3] * other.group0()[0])),
                ((self.group2()[3] * other.group1()[1]) + (self.group3()[3] * other.group0()[1])),
                ((self.group2()[3] * other.group1()[2]) + (self.group3()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group3()[1] * other.group1()[2]) + (self.group3()[2] * other.group1()[1])),
                ((self.group3()[0] * other.group1()[2]) - (self.group3()[2] * other.group1()[0])),
                (-(self.group3()[0] * other.group1()[1]) + (self.group3()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group3()[2] * other.group0()[1]) + (self.group2()[3] * other.group1()[0]) + (self.group3()[1] * other.group0()[2])),
                ((self.group3()[2] * other.group0()[0]) + (self.group2()[3] * other.group1()[1]) - (self.group3()[0] * other.group0()[2])),
                (-(self.group3()[1] * other.group0()[0]) + (self.group2()[3] * other.group1()[2]) + (self.group3()[0] * other.group0()[1])),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group3()[3] * other.group0()[0]),
                (self.group3()[3] * other.group0()[1]),
                (self.group3()[3] * other.group0()[2]),
                (-(self.group3()[2] * other.group1()[2]) - (self.group3()[0] * other.group1()[0]) - (self.group3()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group3()[3] * other.group1()[0]),
                (self.group3()[3] * other.group1()[1]),
                (self.group3()[3] * other.group1()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorOdd {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       24        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       15       25        0
    //  no simd       18       28        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[2]]))
                + Simd32x4::from([
                    (self.group3()[1] * other.group0()[2]),
                    (self.group3()[2] * other.group0()[0]),
                    (self.group3()[0] * other.group0()[1]),
                    (-(self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[3] * other.group0()[3])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group3()[3] * other.group0()[0]) + (self.group2()[3] * other.group1()[0]) - (self.group3()[0] * other.group0()[3])),
                ((self.group3()[3] * other.group0()[1]) + (self.group2()[3] * other.group1()[1]) - (self.group3()[1] * other.group0()[3])),
                ((self.group3()[3] * other.group0()[2]) + (self.group2()[3] * other.group1()[2]) - (self.group3()[2] * other.group0()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group3()[1] * other.group1()[2]) + (self.group3()[2] * other.group1()[1])),
                ((self.group3()[0] * other.group1()[2]) - (self.group3()[2] * other.group1()[0])),
                (-(self.group3()[0] * other.group1()[1]) + (self.group3()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Dipole> for VersorOdd {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       14        0
    //  no simd       15       20        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + (swizzle!(self.group3(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group3()[3] * other.group0()[0]) - (self.group3()[2] * other.group1()[1])),
                    (-(self.group3()[3] * other.group0()[1]) - (self.group3()[0] * other.group1()[2])),
                    (-(self.group3()[3] * other.group0()[2]) - (self.group3()[1] * other.group1()[0])),
                    ((self.group3()[1] * other.group0()[1]) + (self.group3()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group3()[3] * other.group1()[3]) - (self.group3()[2] * other.group2()[2]) - (self.group3()[0] * other.group2()[0]) - (self.group3()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorOdd {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       14        0
    //  no simd        9       17        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    ((self.group3()[3] * other.group0()[0]) * -1.0),
                    ((self.group3()[3] * other.group0()[1]) * -1.0),
                    ((self.group3()[3] * other.group0()[2]) * -1.0),
                    ((self.group3()[2] * other.group0()[2]) + (self.group3()[1] * other.group0()[1]) + (self.group3()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group3()[3] * other.group0()[3]) - (self.group3()[2] * other.group1()[2]) - (self.group3()[0] * other.group1()[0]) - (self.group3()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorOdd {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       14        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group3()[2] * other.group0()[1]) + (self.group2()[3] * other.group1()[0]) + (self.group3()[1] * other.group0()[2])),
                ((self.group3()[2] * other.group0()[0]) + (self.group2()[3] * other.group1()[1]) - (self.group3()[0] * other.group0()[2])),
                (-(self.group3()[1] * other.group0()[0]) + (self.group2()[3] * other.group1()[2]) + (self.group3()[0] * other.group0()[1])),
                (self.group2()[3] * other.group0()[3]),
            ]),
            // e5
            (-(self.group3()[3] * other.group0()[3]) - (self.group3()[2] * other.group1()[2]) - (self.group3()[0] * other.group1()[0]) - (self.group3()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorOdd {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) - (self.group3()[3] * other.group0()[0])),
                ((self.group2()[3] * other.group1()[1]) - (self.group3()[3] * other.group0()[1])),
                ((self.group2()[3] * other.group1()[2]) - (self.group3()[3] * other.group0()[2])),
                ((self.group3()[2] * other.group0()[2]) + (self.group3()[0] * other.group0()[0]) + (self.group3()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group3()[2] * other.group1()[2]) - (self.group3()[0] * other.group1()[0]) - (self.group3()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorOdd {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       12        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group3()[3] * other.group0()[0] * -1.0),
                (self.group3()[3] * other.group0()[1] * -1.0),
                (self.group3()[3] * other.group0()[2] * -1.0),
                ((self.group3()[2] * other.group0()[2]) + (self.group3()[1] * other.group0()[1]) + (self.group2()[3] * other.group0()[3]) + (self.group3()[0] * other.group0()[0])),
            ]),
            // e5
            (self.group3()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorOdd {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       13       18        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group3(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group3()[3] * other.group0()[0]) - (self.group3()[2] * other.group1()[1]) + (self.group2()[3] * other.group2()[0])),
                    (-(self.group3()[3] * other.group0()[1]) + (self.group2()[3] * other.group2()[1]) - (self.group3()[0] * other.group1()[2])),
                    (-(self.group3()[3] * other.group0()[2]) - (self.group3()[1] * other.group1()[0]) + (self.group2()[3] * other.group2()[2])),
                    ((self.group3()[0] * other.group0()[0]) + (self.group3()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group3()[2] * other.group2()[2]) - (self.group3()[0] * other.group2()[0]) - (self.group3()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        3        0
    // Totals...
    // yes simd        1        8        0
    //  no simd        1       17        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                ((self.group0()[3] * other.group0()[1]) + (self.group2()[3] * other.group0()[0])),
            ]),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e15, e25, e35, e1234
            (self.group2() * Simd32x4::from(other.group0()[1])),
            // e4235, e4315, e4125, e3215
            (self.group3() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorOdd {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (Simd32x2::from(other[e45]) * Simd32x2::from([self.group2()[3], self.group3()[3]]) * Simd32x2::from([1.0, -1.0])),
        );
    }
}
impl AntiWedge<FlatPoint> for VersorOdd {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        4        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        5        0
    //  no simd        3        8        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group2()[3]) * other.group0()),
            // e5
            (-(self.group3()[3] * other.group0()[3]) - (self.group3()[2] * other.group0()[2]) - (self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorOdd {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group2()[3] * other.group0()[0]),
            (self.group2()[3] * other.group0()[1]),
            (self.group2()[3] * other.group0()[2]),
            (-(self.group3()[2] * other.group0()[2]) - (self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Flector> for VersorOdd {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       12       24        0
    //    simd4        4        4        0
    // Totals...
    // yes simd       16       28        0
    //  no simd       28       40        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group2()[3] * other.group1()[0]),
                (self.group2()[3] * other.group1()[1]),
                (self.group2()[3] * other.group1()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group3()[1] * other.group1()[2]) + (self.group3()[2] * other.group1()[1])),
                ((self.group3()[0] * other.group1()[2]) - (self.group3()[2] * other.group1()[0])),
                (-(self.group3()[0] * other.group1()[1]) + (self.group3()[1] * other.group1()[0])),
                (self.group2()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            ((swizzle!(other.group1(), 3, 3, 3, 2) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group2()[2]]))
                - (Simd32x4::from(self.group3()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group3()[2] * other.group0()[2]) - (self.group3()[1] * other.group0()[1]) - (self.group3()[0] * other.group0()[0])
                        + (self.group2()[1] * other.group1()[1])
                        + (self.group1()[3] * other.group1()[3])
                        + (self.group2()[0] * other.group1()[0])),
                ])),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group2()[3]) * other.group0())
                - (swizzle!(other.group1(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group1()[2])),
                    ((self.group1()[2] * other.group1()[0]) + (self.group0()[1] * other.group1()[3])),
                    ((self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorOdd {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group2()[3]])),
            // e1, e2, e3, e5
            ((Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group2()[3] * other.group0()[0]),
                    (self.group2()[3] * other.group0()[1]),
                    (self.group2()[3] * other.group0()[2]),
                    (-(self.group3()[2] * other.group0()[2]) - (self.group3()[1] * other.group0()[1]) - (self.group3()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorOdd {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       21        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       23        0
    //  no simd       15       29        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (self.group2()[3] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group3()[1] * other.group0()[3]) + (self.group3()[2] * other.group0()[2])),
                ((self.group3()[0] * other.group0()[3]) - (self.group3()[2] * other.group0()[1])),
                (-(self.group3()[0] * other.group0()[2]) + (self.group3()[1] * other.group0()[1])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group3()[3] * other.group0()[1] * -1.0),
                (self.group3()[3] * other.group0()[2] * -1.0),
                (self.group3()[3] * other.group0()[3] * -1.0),
                (-(self.group3()[3] * other.group0()[0])
                    + (self.group2()[2] * other.group0()[3])
                    + (self.group2()[0] * other.group0()[1])
                    + (self.group2()[1] * other.group0()[2])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 3, 1, 2, 0) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[0], self.group2()[3]]))
                - (swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2]))])),
        );
    }
}
impl AntiWedge<Horizon> for VersorOdd {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group2()[3]])),
            // e1, e2, e3, e5
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]])),
        );
    }
}
impl AntiWedge<Infinity> for VersorOdd {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group2()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group2()[3] * other.group1()[0]),
                (self.group2()[3] * other.group1()[1]),
                (self.group2()[3] * other.group1()[2]),
                (-(self.group3()[2] * other.group0()[2]) - (self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group3()[3] * other.group0()[0]) - (self.group3()[1] * other.group1()[2]) + (self.group3()[2] * other.group1()[1])),
                ((self.group3()[3] * other.group0()[1]) + (self.group3()[0] * other.group1()[2]) - (self.group3()[2] * other.group1()[0])),
                ((self.group3()[3] * other.group0()[2]) - (self.group3()[0] * other.group1()[1]) + (self.group3()[1] * other.group1()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorOdd {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group3()[1] * other.group0()[2]) + (self.group3()[2] * other.group0()[1])),
                ((self.group3()[0] * other.group0()[2]) - (self.group3()[2] * other.group0()[0])),
                (-(self.group3()[0] * other.group0()[1]) + (self.group3()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<LineOnOrigin> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group3()[2] * other.group0()[2]) - (self.group3()[0] * other.group0()[0]) - (self.group3()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group3()[3] * other.group0()[0]),
                (self.group3()[3] * other.group0()[1]),
                (self.group3()[3] * other.group0()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<Motor> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       25        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       19       29        0
    //  no simd       28       41        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e45
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group2()[3] * other.group1()[0]),
                    (self.group2()[3] * other.group1()[1]),
                    (self.group2()[3] * other.group1()[2]),
                    (-(self.group3()[2] * other.group0()[2]) - (self.group3()[1] * other.group0()[1]) - (self.group3()[0] * other.group0()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group3()[3] * other.group0()[0]) + (self.group3()[2] * other.group1()[1]) + (self.group2()[0] * other.group0()[3]) - (self.group3()[1] * other.group1()[2])),
                ((self.group3()[3] * other.group0()[1]) - (self.group3()[2] * other.group1()[0]) + (self.group2()[1] * other.group0()[3]) + (self.group3()[0] * other.group1()[2])),
                ((self.group3()[3] * other.group0()[2]) + (self.group3()[1] * other.group1()[0]) + (self.group2()[2] * other.group0()[3]) - (self.group3()[0] * other.group1()[1])),
                (self.group2()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group3() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorOdd {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       13        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                ((self.group2()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group3()[1] * other.group0()[2]) + (self.group3()[2] * other.group0()[1])),
                ((self.group3()[0] * other.group0()[2]) - (self.group3()[2] * other.group0()[0])),
                (-(self.group3()[0] * other.group0()[1]) + (self.group3()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       20        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       12       28        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group2()[3] * other.group0()[0]),
                    (self.group2()[3] * other.group0()[1]),
                    (self.group2()[3] * other.group0()[2]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group3()[2] * other.group0()[2]) - (self.group3()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3])
                    - (self.group3()[0] * other.group0()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[0] * other.group0()[3]) + (self.group3()[3] * other.group0()[0])),
                ((self.group2()[1] * other.group0()[3]) + (self.group3()[3] * other.group0()[1])),
                ((self.group2()[2] * other.group0()[3]) + (self.group3()[3] * other.group0()[2])),
                (self.group2()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group3() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for VersorOdd {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       38       53        0
    //    simd3        8       12        0
    //    simd4        7        8        0
    // Totals...
    // yes simd       53       73        0
    //  no simd       90      121        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group3()[3] * other.group1()[3])
                    + (self.group3()[2] * other.group1()[2])
                    + (self.group3()[1] * other.group1()[1])
                    + (self.group3()[0] * other.group1()[0])
                    + (self.group2()[3] * other[e1])
                    - (self.group2()[2] * other.group7()[2])
                    - (self.group2()[1] * other.group7()[1])
                    - (self.group2()[0] * other.group7()[0])
                    - (self.group1()[3] * other.group6()[3])
                    - (self.group1()[2] * other.group6()[2])
                    - (self.group1()[1] * other.group6()[1])
                    - (self.group1()[0] * other.group6()[0])
                    + (self.group0()[3] * other.group0()[1])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group3(), 1, 2, 0, 2) * Simd32x4::from([other.group5()[2], other.group5()[0], other.group5()[1], other.group3()[2]]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group4()[0], other.group4()[1], other.group4()[2], other.group3()[3]]))
                - (Simd32x4::from(other.group9()[0]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group9(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group3()[3] * other.group3()[0]) - (self.group3()[2] * other.group5()[1]) + (self.group0()[0] * other[e45]) + (self.group1()[1] * other.group9()[3])),
                    (-(self.group3()[3] * other.group3()[1]) - (self.group3()[0] * other.group5()[2]) + (self.group1()[2] * other.group9()[1]) + (self.group0()[1] * other[e45])),
                    (-(self.group3()[3] * other.group3()[2]) - (self.group3()[1] * other.group5()[0]) + (self.group0()[2] * other[e45]) + (self.group1()[0] * other.group9()[2])),
                    ((self.group3()[1] * other.group3()[1]) + (self.group3()[0] * other.group3()[0])
                        - (self.group0()[0] * other.group9()[1])
                        - (self.group0()[1] * other.group9()[2])),
                ])),
            // e5
            (-(self.group3()[3] * other.group3()[3]) - (self.group3()[2] * other.group4()[2]) - (self.group3()[1] * other.group4()[1]) - (self.group3()[0] * other.group4()[0])
                + (self.group2()[2] * other.group9()[3])
                + (self.group2()[1] * other.group9()[2])
                + (self.group1()[3] * other[e45])
                + (self.group2()[0] * other.group9()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group3(), 2, 0, 1, 2) * Simd32x4::from([other.group7()[1], other.group7()[2], other.group7()[0], other.group6()[2]]))
                + (Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group3()[1] * other.group7()[2]) + (self.group2()[3] * other.group6()[0])),
                    ((self.group3()[2] * other.group7()[0]) + (self.group2()[3] * other.group6()[1])),
                    ((self.group3()[0] * other.group7()[1]) + (self.group2()[3] * other.group6()[2])),
                    (-(self.group3()[1] * other.group6()[1]) - (self.group3()[0] * other.group6()[0])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group3()[3]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))
                + (swizzle!(other.group8(), 1, 2, 0) * Simd32x3::from([self.group3()[2], self.group3()[0], self.group3()[1]]))
                + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))
                - (swizzle!(other.group8(), 2, 0, 1) * Simd32x3::from([self.group3()[1], self.group3()[2], self.group3()[0]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group3()[3]) * other.group7()) - (Simd32x3::from(other.group6()[3]) * Simd32x3::from([self.group3()[0], self.group3()[1], self.group3()[2]]))
                + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                + (Simd32x3::from(self.group2()[3]) * other.group8())),
            // e415, e425, e435, e321
            (-(swizzle!(self.group3(), 1, 2, 0, 3) * swizzle!(other.group9(), 3, 1, 2, 0))
                + Simd32x4::from([
                    (self.group3()[2] * other.group9()[2]),
                    (self.group3()[0] * other.group9()[3]),
                    (self.group3()[1] * other.group9()[1]),
                    (self.group2()[3] * other[e45]),
                ])),
            // e423, e431, e412
            ((Simd32x3::from(self.group2()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))
                - (Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group3()[0], self.group3()[1], self.group3()[2]]))),
            // e235, e315, e125
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group3()[0], self.group3()[1], self.group3()[2]]))
                - (Simd32x3::from(self.group3()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group2()[3], self.group3()[0], self.group3()[1], self.group3()[2]])),
            // e3215
            (self.group3()[3] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorOdd {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group3()[1] * other.group0()[2]) - (self.group3()[2] * other.group0()[1])),
                (-(self.group3()[0] * other.group0()[2]) + (self.group3()[2] * other.group0()[0])),
                ((self.group3()[0] * other.group0()[1]) - (self.group3()[1] * other.group0()[0])),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group3()[3] * other.group0()[0]),
                (self.group3()[3] * other.group0()[1]),
                (self.group3()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorOdd {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group3()[3] * other.group0()[0] * -1.0),
            (self.group3()[3] * other.group0()[1] * -1.0),
            (self.group3()[3] * other.group0()[2] * -1.0),
            ((self.group3()[2] * other.group0()[2]) + (self.group3()[0] * other.group0()[0]) + (self.group3()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorOdd {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group3() * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[3], self.group2()[0], self.group2()[1], self.group2()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorOdd {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group3(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group3()[3] * other.group0()[0]),
                (self.group3()[3] * other.group0()[1]),
                (self.group3()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorOdd {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        3       12        0
    //  no simd        6       21        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group3() * Simd32x4::from(other.group0()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            (-(Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[3], self.group2()[0], self.group2()[1], self.group2()[2]]))
                + Simd32x4::from([
                    ((self.group3()[2] * other.group0()[2]) + (self.group3()[1] * other.group0()[1]) + (self.group3()[0] * other.group0()[0])),
                    ((self.group3()[3] * other.group0()[0]) * -1.0),
                    ((self.group3()[3] * other.group0()[1]) * -1.0),
                    ((self.group3()[3] * other.group0()[2]) * -1.0),
                ])),
        );
    }
}
impl AntiWedge<Origin> for VersorOdd {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group3()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for VersorOdd {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       27        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       11       29        0
    //  no simd       17       35        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group3()[1] * other.group0()[2]) + (self.group3()[2] * other.group0()[1])),
                ((self.group3()[0] * other.group0()[2]) - (self.group3()[2] * other.group0()[0])),
                (-(self.group3()[0] * other.group0()[1]) + (self.group3()[1] * other.group0()[0])),
                (self.group2()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            ((swizzle!(other.group0(), 3, 3, 3, 2) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group2()[2]]))
                + Simd32x4::from([
                    ((self.group3()[3] * other.group0()[0]) * -1.0),
                    ((self.group3()[3] * other.group0()[1]) * -1.0),
                    ((self.group3()[3] * other.group0()[2]) * -1.0),
                    ((self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group0()[0])),
                ])),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorOdd {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       27        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group3()[1] * other.group0()[2]) + (self.group3()[2] * other.group0()[1])),
                ((self.group3()[0] * other.group0()[2]) - (self.group3()[2] * other.group0()[0])),
                (-(self.group3()[0] * other.group0()[1]) + (self.group3()[1] * other.group0()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group3()[3] * other.group0()[0] * -1.0),
                (self.group3()[3] * other.group0()[1] * -1.0),
                (self.group3()[3] * other.group0()[2] * -1.0),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for VersorOdd {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self.group3()[3] * other.group0()[3])
                + (self.group3()[2] * other.group0()[2])
                + (self.group3()[1] * other.group0()[1])
                + (self.group2()[3] * other[e2])
                + (self.group3()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for VersorOdd {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ ((self.group2()[3] * other.group0()[1]) + (self.group3()[3] * other.group0()[0])));
    }
}
impl AntiWedge<Sphere> for VersorOdd {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       23        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       13       28        0
    //  no simd       25       43        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group2()[3] * other.group0()[0]) - (self.group3()[0] * other[e4315])),
                ((self.group2()[3] * other.group0()[1]) - (self.group3()[1] * other[e4315])),
                ((self.group2()[3] * other.group0()[2]) - (self.group3()[2] * other[e4315])),
                0.0,
            ]),
            // e415, e425, e435, e321
            (-(swizzle!(self.group3(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other[e4315]]))
                + (swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[3]]))),
            // e235, e315, e125, e5
            ((swizzle!(other.group0(), 3, 3, 3, 2) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group2()[2]]))
                + Simd32x4::from([
                    ((self.group3()[3] * other.group0()[0]) * -1.0),
                    ((self.group3()[3] * other.group0()[1]) * -1.0),
                    ((self.group3()[3] * other.group0()[2]) * -1.0),
                    ((self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group0()[0])),
                ])),
            // e1, e2, e3, e4
            (-(Simd32x4::from(other[e4315]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorOdd {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        4       17        0
    //  no simd        4       20        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group3()[0] * other.group0()[1] * -1.0),
                (self.group3()[1] * other.group0()[1] * -1.0),
                (self.group3()[2] * other.group0()[1] * -1.0),
                ((self.group2()[3] * other.group0()[0]) - (self.group3()[3] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group1()[3]])),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) - (self.group2()[0] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[0]) - (self.group2()[2] * other.group0()[1])),
                (self.group1()[3] * other.group0()[1] * -1.0),
            ]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorOdd {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       28        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       11       30        0
    //  no simd       17       36        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group2()[3] * other.group0()[0]) - (self.group3()[0] * other.group0()[3])),
                ((self.group2()[3] * other.group0()[1]) - (self.group3()[1] * other.group0()[3])),
                ((self.group2()[3] * other.group0()[2]) - (self.group3()[2] * other.group0()[3])),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group3()[1] * other.group0()[2]) + (self.group3()[2] * other.group0()[1])),
                ((self.group3()[0] * other.group0()[2]) - (self.group3()[2] * other.group0()[0])),
                (-(self.group3()[0] * other.group0()[1]) + (self.group3()[1] * other.group0()[0])),
                (self.group3()[3] * other.group0()[3] * -1.0),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group3()[3] * other.group0()[0] * -1.0),
                (self.group3()[3] * other.group0()[1] * -1.0),
                (self.group3()[3] * other.group0()[2] * -1.0),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEven> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       24       33        0
    //    simd4        6        7        0
    // Totals...
    // yes simd       30       40        0
    //  no simd       48       61        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[2]]))
                + (swizzle!(self.group3(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group3()[3]]))
                + (self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group3()[2] * other.group3()[2]) + (self.group3()[1] * other.group3()[1]) + (self.group3()[0] * other.group3()[0])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            ((other.group0() * Simd32x4::from([self.group3()[3], self.group3()[3], self.group3()[3], self.group1()[3]]))
                - (swizzle!(self.group3(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group2()[3] * other.group2()[0])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group2()[3] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group2()[3] * other.group2()[2])),
                    (-(self.group3()[1] * other.group1()[1]) - (self.group3()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group3()[3] * other.group1()[0]) + (self.group3()[2] * other.group2()[1]) + (self.group2()[0] * other.group0()[3]) - (self.group3()[1] * other.group2()[2])),
                ((self.group3()[3] * other.group1()[1]) - (self.group3()[2] * other.group2()[0]) + (self.group2()[1] * other.group0()[3]) + (self.group3()[0] * other.group2()[2])),
                ((self.group3()[3] * other.group1()[2]) + (self.group3()[1] * other.group2()[0]) + (self.group2()[2] * other.group0()[3]) - (self.group3()[0] * other.group2()[1])),
                (self.group2()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group3() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       21       30        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       26       36        0
    //  no simd       41       54        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[2]]))
                + (swizzle!(self.group3(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                + (self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            ((other.group0() * Simd32x4::from([self.group3()[3], self.group3()[3], self.group3()[3], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group0()[3]) + (self.group2()[3] * other.group2()[0])),
                    ((self.group1()[1] * other.group0()[3]) + (self.group2()[3] * other.group2()[1])),
                    ((self.group1()[2] * other.group0()[3]) + (self.group2()[3] * other.group2()[2])),
                    (-(self.group3()[2] * other.group1()[2]) - (self.group3()[1] * other.group1()[1]) - (self.group3()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group3()[3] * other.group1()[0]) + (self.group3()[2] * other.group2()[1]) + (self.group2()[0] * other.group0()[3]) - (self.group3()[1] * other.group2()[2])),
                ((self.group3()[3] * other.group1()[1]) - (self.group3()[2] * other.group2()[0]) + (self.group2()[1] * other.group0()[3]) + (self.group3()[0] * other.group2()[2])),
                ((self.group3()[3] * other.group1()[2]) + (self.group3()[1] * other.group2()[0]) + (self.group2()[2] * other.group0()[3]) - (self.group3()[0] * other.group2()[1])),
                (self.group2()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group3() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       19       28        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       23       33        0
    //  no simd       35       48        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 0, 0, 0, 3) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group3()[2]]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group3()[1] * other.group0()[2]) + (self.group3()[0] * other.group0()[1])
                        - (self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        + (self.group0()[3] * other.group0()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group3(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + (self.group1() * Simd32x4::from(other.group0()[0]))
                + Simd32x4::from([
                    (self.group2()[3] * other.group2()[0]),
                    (self.group2()[3] * other.group2()[1]),
                    (self.group2()[3] * other.group2()[2]),
                    (-(self.group3()[1] * other.group1()[1]) - (self.group3()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group3()[3] * other.group1()[0]) + (self.group3()[2] * other.group2()[1]) + (self.group2()[0] * other.group0()[0]) - (self.group3()[1] * other.group2()[2])),
                ((self.group3()[3] * other.group1()[1]) - (self.group3()[2] * other.group2()[0]) + (self.group2()[1] * other.group0()[0]) + (self.group3()[0] * other.group2()[2])),
                ((self.group3()[3] * other.group1()[2]) + (self.group3()[1] * other.group2()[0]) + (self.group2()[2] * other.group0()[0]) - (self.group3()[0] * other.group2()[1])),
                (self.group2()[3] * other.group0()[0]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group3() * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorOdd {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group3(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[3] * other.group1()[3])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) + (self.group3()[3] * other.group0()[0])),
                ((self.group2()[3] * other.group1()[1]) + (self.group3()[3] * other.group0()[1])),
                ((self.group2()[3] * other.group1()[2]) + (self.group3()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group3()[1] * other.group1()[2]) + (self.group3()[2] * other.group1()[1])),
                ((self.group3()[0] * other.group1()[2]) - (self.group3()[2] * other.group1()[0])),
                (-(self.group3()[0] * other.group1()[1]) + (self.group3()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorOdd {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       21        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       13       26        0
    //  no simd       25       41        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[2]]))
                + (swizzle!(self.group3(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                + (self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group2()[3] * other.group1()[0]),
                    (self.group2()[3] * other.group1()[1]),
                    (self.group2()[3] * other.group1()[2]),
                    (-(self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])),
                ])),
            // e23, e31, e12, e45
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group3()[3] * other.group0()[0]),
                    (self.group3()[3] * other.group0()[1]),
                    (self.group3()[3] * other.group0()[2]),
                    (-(self.group3()[2] * other.group1()[2]) - (self.group3()[1] * other.group1()[1]) - (self.group3()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[0] * other.group0()[3]) + (self.group3()[3] * other.group1()[0])),
                ((self.group2()[1] * other.group0()[3]) + (self.group3()[3] * other.group1()[1])),
                ((self.group2()[2] * other.group0()[3]) + (self.group3()[3] * other.group1()[2])),
                (self.group2()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group3() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorOdd {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       25        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       20       27        0
    //  no simd       26       33        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group3(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group2()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group3()[2] * other.group2()[2])
                        + (self.group3()[1] * other.group2()[1])
                        + (self.group3()[0] * other.group2()[0])
                        + (self.group2()[3] * other.group1()[3])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[3] * other.group0()[3])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group3()[3] * other.group0()[0]) + (self.group2()[3] * other.group1()[0]) - (self.group3()[0] * other.group0()[3])),
                ((self.group3()[3] * other.group0()[1]) + (self.group2()[3] * other.group1()[1]) - (self.group3()[1] * other.group0()[3])),
                ((self.group3()[3] * other.group0()[2]) + (self.group2()[3] * other.group1()[2]) - (self.group3()[2] * other.group0()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group3()[1] * other.group1()[2]) + (self.group3()[2] * other.group1()[1])),
                ((self.group3()[0] * other.group1()[2]) - (self.group3()[2] * other.group1()[0])),
                (-(self.group3()[0] * other.group1()[1]) + (self.group3()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOdd> for VersorOdd {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       20       28        0
    //    simd4        7        8        0
    // Totals...
    // yes simd       27       36        0
    //  no simd       48       60        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group2()[3] * other.group3()[0]) - (self.group3()[0] * other.group2()[3])),
                ((self.group2()[3] * other.group3()[1]) - (self.group3()[1] * other.group2()[3])),
                ((self.group2()[3] * other.group3()[2]) - (self.group3()[2] * other.group2()[3])),
                0.0,
            ]),
            // e415, e425, e435, e321
            (-(swizzle!(self.group3(), 1, 2, 0, 3) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group2()[3]]))
                + (swizzle!(other.group3(), 1, 2, 0, 3) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[3]]))),
            // e235, e315, e125, e5
            ((swizzle!(other.group3(), 3, 3, 3, 2) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group2()[2]]))
                - (Simd32x4::from(self.group3()[3]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group3()[2] * other.group2()[2]) - (self.group3()[1] * other.group2()[1]) - (self.group3()[0] * other.group2()[0])
                        + (self.group2()[1] * other.group3()[1])
                        + (self.group1()[3] * other.group3()[3])
                        + (self.group2()[0] * other.group3()[0])),
                ])),
            // e1, e2, e3, e4
            ((swizzle!(self.group3(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                - (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group3(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group3()[3] * other.group0()[0]) - (self.group3()[2] * other.group1()[1])
                        + (self.group0()[0] * other.group3()[3])
                        + (self.group1()[1] * other.group3()[2])),
                    (-(self.group3()[3] * other.group0()[1]) - (self.group3()[0] * other.group1()[2])
                        + (self.group1()[2] * other.group3()[0])
                        + (self.group0()[1] * other.group3()[3])),
                    (-(self.group3()[3] * other.group0()[2]) - (self.group3()[1] * other.group1()[0])
                        + (self.group0()[2] * other.group3()[3])
                        + (self.group1()[0] * other.group3()[1])),
                    ((self.group3()[1] * other.group0()[1]) + (self.group3()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group3()[0])
                        - (self.group0()[1] * other.group3()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorOdd {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       26        0
    //    simd4        6        7        0
    // Totals...
    // yes simd       24       33        0
    //  no simd       42       54        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group2()[3] * other.group2()[0]) - (self.group3()[0] * other.group1()[3])),
                ((self.group2()[3] * other.group2()[1]) - (self.group3()[1] * other.group1()[3])),
                ((self.group2()[3] * other.group2()[2]) - (self.group3()[2] * other.group1()[3])),
                0.0,
            ]),
            // e415, e425, e435, e321
            (-(swizzle!(self.group3(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[3]]))
                + (swizzle!(other.group2(), 1, 2, 0, 3) * Simd32x4::from([self.group3()[2], self.group3()[0], self.group3()[1], self.group2()[3]]))),
            // e235, e315, e125, e5
            ((swizzle!(other.group2(), 3, 3, 3, 2) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group2()[2]]))
                - (Simd32x4::from(self.group3()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group3()[2] * other.group1()[2]) - (self.group3()[1] * other.group1()[1]) - (self.group3()[0] * other.group1()[0])
                        + (self.group2()[1] * other.group2()[1])
                        + (self.group1()[3] * other.group2()[3])
                        + (self.group2()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                - (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group3()[3] * other.group0()[0]) + (self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                    (-(self.group3()[3] * other.group0()[1]) + (self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3])),
                    (-(self.group3()[3] * other.group0()[2]) + (self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                    ((self.group3()[2] * other.group0()[2]) + (self.group3()[1] * other.group0()[1]) + (self.group3()[0] * other.group0()[0])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorOdd {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       30        0
    //    simd4        4        4        0
    // Totals...
    // yes simd       22       34        0
    //  no simd       34       46        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group2()[3] * other.group2()[0]),
                (self.group2()[3] * other.group2()[1]),
                (self.group2()[3] * other.group2()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group3()[1] * other.group2()[2]) + (self.group3()[2] * other.group2()[1])),
                ((self.group3()[0] * other.group2()[2]) - (self.group3()[2] * other.group2()[0])),
                (-(self.group3()[0] * other.group2()[1]) + (self.group3()[1] * other.group2()[0])),
                (self.group2()[3] * other.group2()[3]),
            ]),
            // e235, e315, e125, e5
            ((swizzle!(other.group2(), 3, 3, 3, 2) * Simd32x4::from([self.group3()[0], self.group3()[1], self.group3()[2], self.group2()[2]]))
                - (Simd32x4::from(self.group3()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group3()[2] * other.group0()[3]) - (self.group3()[1] * other.group0()[2]) - (self.group3()[0] * other.group0()[1])
                        + (self.group2()[1] * other.group2()[1])
                        + (self.group1()[3] * other.group2()[3])
                        + (self.group2()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            ((swizzle!(other.group1(), 2, 0, 1, 3) * Simd32x4::from([self.group3()[1], self.group3()[2], self.group3()[0], self.group2()[3]]))
                - (swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group3()[2] * other.group1()[1])
                        + (self.group2()[3] * other.group0()[1])
                        + (self.group0()[0] * other.group2()[3])
                        + (self.group1()[1] * other.group2()[2])),
                    (-(self.group3()[0] * other.group1()[2])
                        + (self.group2()[3] * other.group0()[2])
                        + (self.group1()[2] * other.group2()[0])
                        + (self.group0()[1] * other.group2()[3])),
                    (-(self.group3()[1] * other.group1()[0])
                        + (self.group2()[3] * other.group0()[3])
                        + (self.group0()[2] * other.group2()[3])
                        + (self.group1()[0] * other.group2()[1])),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorOdd {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       23        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       25        0
    //  no simd       16       31        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group3()[0] * other.group1()[3] * -1.0),
                (self.group3()[1] * other.group1()[3] * -1.0),
                (self.group3()[2] * other.group1()[3] * -1.0),
                ((self.group2()[3] * other.group0()[3]) - (self.group3()[3] * other.group1()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group3()[0] * other.group0()[3]),
                (self.group3()[1] * other.group0()[3]),
                (self.group3()[2] * other.group0()[3]),
                (-(self.group3()[2] * other.group1()[2]) - (self.group3()[1] * other.group1()[1]) + (self.group1()[3] * other.group0()[3])
                    - (self.group3()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 3, 3, 3, 2) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group3()[2]]))
                - (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                + Simd32x4::from([
                    (-(self.group3()[3] * other.group0()[0]) + (self.group2()[3] * other.group1()[0])),
                    (-(self.group3()[3] * other.group0()[1]) + (self.group2()[3] * other.group1()[1])),
                    (-(self.group3()[3] * other.group0()[2]) + (self.group2()[3] * other.group1()[2])),
                    ((self.group3()[1] * other.group0()[1]) + (self.group3()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorOdd {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       17       36        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       19       38        0
    //  no simd       25       44        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[1]) - (self.group3()[0] * other.group1()[0])),
                ((self.group2()[3] * other.group1()[2]) - (self.group3()[1] * other.group1()[0])),
                ((self.group2()[3] * other.group1()[3]) - (self.group3()[2] * other.group1()[0])),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group3()[1] * other.group1()[3]) + (self.group3()[2] * other.group1()[2])),
                ((self.group3()[0] * other.group1()[3]) - (self.group3()[2] * other.group1()[1])),
                (-(self.group3()[0] * other.group1()[2]) + (self.group3()[1] * other.group1()[1])),
                (self.group3()[3] * other.group1()[0] * -1.0),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group3()[3] * other.group1()[1] * -1.0),
                (self.group3()[3] * other.group1()[2] * -1.0),
                (self.group3()[3] * other.group1()[3] * -1.0),
                (-(self.group3()[3] * other.group0()[3])
                    + (self.group2()[2] * other.group1()[3])
                    + (self.group2()[0] * other.group1()[1])
                    + (self.group2()[1] * other.group1()[2])),
            ]),
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group1()[0]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (swizzle!(other.group1(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group3()[3] * other.group0()[0]) + (self.group1()[1] * other.group1()[3])),
                    (-(self.group3()[3] * other.group0()[1]) + (self.group1()[2] * other.group1()[1])),
                    (-(self.group3()[3] * other.group0()[2]) + (self.group1()[0] * other.group1()[2])),
                    ((self.group3()[2] * other.group0()[2])
                        + (self.group3()[1] * other.group0()[1])
                        + (self.group3()[0] * other.group0()[0])
                        + (self.group2()[3] * other.group0()[3])
                        - (self.group0()[0] * other.group1()[1])
                        - (self.group0()[1] * other.group1()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorOdd {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       29        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       16       31        0
    //  no simd       22       37        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group3()[0] * other.group2()[3] * -1.0),
                (self.group3()[1] * other.group2()[3] * -1.0),
                (self.group3()[2] * other.group2()[3] * -1.0),
                ((self.group2()[3] * other.group1()[3]) - (self.group3()[3] * other.group2()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group3()[0] * other.group1()[3]),
                (self.group3()[1] * other.group1()[3]),
                (self.group3()[2] * other.group1()[3]),
                (-(self.group3()[2] * other.group2()[2]) - (self.group3()[1] * other.group2()[1]) + (self.group1()[3] * other.group1()[3])
                    - (self.group3()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group3(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                - (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                + Simd32x4::from([
                    (-(self.group3()[3] * other.group0()[0]) - (self.group3()[2] * other.group1()[1])
                        + (self.group2()[3] * other.group2()[0])
                        + (self.group0()[0] * other.group1()[3])),
                    (-(self.group3()[3] * other.group0()[1]) - (self.group3()[0] * other.group1()[2])
                        + (self.group2()[3] * other.group2()[1])
                        + (self.group0()[1] * other.group1()[3])),
                    (-(self.group3()[3] * other.group0()[2]) - (self.group3()[1] * other.group1()[0])
                        + (self.group2()[3] * other.group2()[2])
                        + (self.group0()[2] * other.group1()[3])),
                    ((self.group3()[1] * other.group0()[1]) + (self.group3()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl InfixAntiWedge for VersorOddAligningOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for VersorOddAligningOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group2()[2] * other.group1()[1])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group2()[1] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorOddAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group1()[1] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (-(self.group2()[0] * other.group0()[3]) + (self.group2()[3] * other.group0()[0])),
                (-(self.group2()[1] * other.group0()[3]) + (self.group2()[3] * other.group0()[1])),
                (-(self.group2()[2] * other.group0()[3]) + (self.group2()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDualNum> for VersorOddAligningOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]])),
            // e1, e2, e3, e5
            (self.group0() * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for VersorOddAligningOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e321]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorOddAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (-(Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group1()[3] * other.group0()[0]),
                    (self.group1()[3] * other.group0()[1]),
                    (self.group1()[3] * other.group0()[2]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for VersorOddAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                - (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[2] * other.group1()[2]) + (self.group2()[1] * other.group1()[1]) + (self.group2()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group0()[2])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for VersorOddAligningOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group2()[0] * other.group0()[0] * -1.0),
            (self.group2()[1] * other.group0()[0] * -1.0),
            (self.group2()[2] * other.group0()[0] * -1.0),
            ((self.group2()[2] * other.group0()[3]) + (self.group2()[1] * other.group0()[2]) - (self.group0()[3] * other.group0()[0]) + (self.group2()[0] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorOddAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group2(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group1()[0]) + (self.group2()[1] * other.group0()[2])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1])),
                    ((self.group1()[3] * other.group1()[2]) + (self.group2()[0] * other.group0()[1])),
                    (-(self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for VersorOddAligningOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group2()[1], self.group2()[2], self.group2()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group2()[2], self.group2()[0], self.group2()[1]]))),
        );
    }
}
impl AntiWedge<AntiMotor> for VersorOddAligningOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]])),
            // e1, e2, e3, e5
            (-(swizzle!(self.group2(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + (self.group0() * Simd32x4::from(other.group1()[3]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[2]) + (self.group1()[3] * other.group1()[0])),
                    ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1])),
                    ((self.group2()[0] * other.group0()[1]) + (self.group1()[3] * other.group1()[2])),
                    (-(self.group2()[1] * other.group1()[1]) - (self.group2()[0] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for VersorOddAligningOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
            (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
            ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for VersorOddAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for VersorOddAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for VersorOddAligningOrigin {
    type Output = VersorOddAligningOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorOddAligningOrigin::from_groups(
            // e41, e42, e43, e45
            (self.group0() * Simd32x4::from(other[e12345])),
            // e15, e25, e35, e1234
            (self.group1() * Simd32x4::from(other[e12345])),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for VersorOddAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group2()[3] * other.group0()[3]) + (self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorOddAligningOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        6       12        0
    //  no simd       12       24        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group2() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            ((swizzle!(self.group2(), 2, 1, 2, 0) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                - (swizzle!(other.group1(), 3, 1, 3, 0) * Simd32x4::from([self.group0()[3], self.group2()[2], self.group1()[1], self.group2()[1]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                    (-(self.group2()[3] * other.group0()[0]) - (self.group1()[0] * other.group1()[3])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group1()[2] * other.group1()[3])),
                ])),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorOddAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[2] * other.group1()[3]) + (self.group2()[1] * other.group1()[2]) + (self.group2()[0] * other.group1()[1])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group0()[3] * other.group0()[3])
                        - (self.group1()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (-(self.group2()[0] * other.group0()[3]) + (self.group2()[3] * other.group0()[0])),
                (-(self.group2()[1] * other.group0()[3]) + (self.group2()[3] * other.group0()[1])),
                (-(self.group2()[2] * other.group0()[3]) + (self.group2()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Circle> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       22       33        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       23       34        0
    //  no simd       26       37        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[3] * other.group1()[0]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[3] * other.group1()[2]) + (self.group2()[0] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group2(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    ((self.group2()[3] * other.group0()[0]) + (self.group1()[3] * other.group2()[0])),
                    ((self.group2()[3] * other.group0()[1]) + (self.group1()[3] * other.group2()[1])),
                    ((self.group2()[3] * other.group0()[2]) + (self.group1()[3] * other.group2()[2])),
                    (-(self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) - (self.group2()[1] * other.group2()[2]) + (self.group2()[2] * other.group2()[1])),
                ((self.group2()[3] * other.group1()[1]) + (self.group2()[0] * other.group2()[2]) - (self.group2()[2] * other.group2()[0])),
                ((self.group2()[3] * other.group1()[2]) - (self.group2()[0] * other.group2()[1]) + (self.group2()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       22       33        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[3] * other.group1()[0]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[3] * other.group1()[2]) + (self.group2()[0] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                ((self.group1()[3] * other.group2()[0]) + (self.group2()[3] * other.group0()[0])),
                ((self.group1()[3] * other.group2()[1]) + (self.group2()[3] * other.group0()[1])),
                ((self.group1()[3] * other.group2()[2]) + (self.group2()[3] * other.group0()[2])),
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) - (self.group2()[1] * other.group2()[2]) + (self.group2()[2] * other.group2()[1])),
                ((self.group2()[3] * other.group1()[1]) + (self.group2()[0] * other.group2()[2]) - (self.group2()[2] * other.group2()[0])),
                ((self.group2()[3] * other.group1()[2]) - (self.group2()[0] * other.group2()[1]) + (self.group2()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       21        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       11       22        0
    //  no simd       14       25        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group0()[3] * other.group0()[3])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group2(), 0, 1, 2, 2) * swizzle!(other.group0(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group1()[3] * other.group1()[0]),
                    (self.group1()[3] * other.group1()[1]),
                    (self.group1()[3] * other.group1()[2]),
                    (-(self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group0()[0]) - (self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[3] * other.group0()[1]) + (self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                ((self.group2()[3] * other.group0()[2]) - (self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorOddAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       14       24        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) + (self.group2()[3] * other.group0()[0])),
                ((self.group1()[3] * other.group1()[1]) + (self.group2()[3] * other.group0()[1])),
                ((self.group1()[3] * other.group1()[2]) + (self.group2()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                (-(self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       21        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[3] * other.group1()[0]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[3] * other.group1()[2]) + (self.group2()[0] * other.group0()[1])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[3] * other.group1()[0]),
                (self.group2()[3] * other.group1()[1]),
                (self.group2()[3] * other.group1()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorOddAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       24        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       15       25        0
    //  no simd       18       28        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[3] * other.group0()[3])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group2()[3] * other.group0()[0]) + (self.group1()[3] * other.group1()[0]) - (self.group2()[0] * other.group0()[3])),
                ((self.group2()[3] * other.group0()[1]) + (self.group1()[3] * other.group1()[1]) - (self.group2()[1] * other.group0()[3])),
                ((self.group2()[3] * other.group0()[2]) + (self.group1()[3] * other.group1()[2]) - (self.group2()[2] * other.group0()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                (-(self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Dipole> for VersorOddAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        9       14        0
    //  no simd       15       20        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + (swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group2()[2] * other.group1()[1])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group2()[1] * other.group1()[0])),
                    ((self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group2()[3] * other.group1()[3]) - (self.group2()[2] * other.group2()[2]) - (self.group2()[0] * other.group2()[0]) - (self.group2()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorOddAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       14        0
    //  no simd        9       17        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    ((self.group2()[3] * other.group0()[0]) * -1.0),
                    ((self.group2()[3] * other.group0()[1]) * -1.0),
                    ((self.group2()[3] * other.group0()[2]) * -1.0),
                    ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                ])),
            // e5
            (-(self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorOddAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       14        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group2()[2] * other.group0()[1]) + (self.group1()[3] * other.group1()[0]) + (self.group2()[1] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[0]) + (self.group1()[3] * other.group1()[1]) - (self.group2()[0] * other.group0()[2])),
                (-(self.group2()[1] * other.group0()[0]) + (self.group1()[3] * other.group1()[2]) + (self.group2()[0] * other.group0()[1])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e5
            (-(self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorOddAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       12        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) - (self.group2()[3] * other.group0()[0])),
                ((self.group1()[3] * other.group1()[1]) - (self.group2()[3] * other.group0()[1])),
                ((self.group1()[3] * other.group1()[2]) - (self.group2()[3] * other.group0()[2])),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorOddAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       12        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0] * -1.0),
                (self.group2()[3] * other.group0()[1] * -1.0),
                (self.group2()[3] * other.group0()[2] * -1.0),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group0()[0])),
            ]),
            // e5
            (self.group2()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorOddAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       14        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       10       15        0
    //  no simd       13       18        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group2()[2] * other.group1()[1]) + (self.group1()[3] * other.group2()[0])),
                    (-(self.group2()[3] * other.group0()[1]) + (self.group1()[3] * other.group2()[1]) - (self.group2()[0] * other.group1()[2])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group2()[1] * other.group1()[0]) + (self.group1()[3] * other.group2()[2])),
                    ((self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group2()[2] * other.group2()[2]) - (self.group2()[0] * other.group2()[0]) - (self.group2()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        5        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        7        0
    //  no simd        0       13        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                (self.group1()[3] * other.group0()[0]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e15, e25, e35, e1234
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorOddAligningOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (Simd32x2::from(other[e45]) * Simd32x2::from([self.group1()[3], self.group2()[3]]) * Simd32x2::from([1.0, -1.0])),
        );
    }
}
impl AntiWedge<FlatPoint> for VersorOddAligningOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        4        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        3        5        0
    //  no simd        3        8        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group1()[3]) * other.group0()),
            // e5
            (-(self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorOddAligningOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Flector> for VersorOddAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       22        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       13       25        0
    //  no simd       22       34        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                (-(self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                (self.group1()[3] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            ((swizzle!(other.group1(), 3, 3, 3, 2) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[2]]))
                - (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) - (self.group2()[0] * other.group0()[0])
                        + (self.group1()[1] * other.group1()[1])
                        + (self.group0()[3] * other.group1()[3])
                        + (self.group1()[0] * other.group1()[0])),
                ])),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[3]) * other.group0())
                + Simd32x4::from([
                    (self.group0()[0] * other.group1()[3]),
                    (self.group0()[1] * other.group1()[3]),
                    (self.group0()[2] * other.group1()[3]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorOddAligningOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]])),
            // e1, e2, e3, e5
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group1()[3] * other.group0()[0]),
                    (self.group1()[3] * other.group0()[1]),
                    (self.group1()[3] * other.group0()[2]),
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) - (self.group2()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorOddAligningOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       15        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       17        0
    //  no simd       12       23        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (self.group1()[3] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e4
            (-(swizzle!(other.group0(), 3, 1, 2, 3) * Simd32x4::from([self.group2()[1], self.group2()[2], self.group2()[0], self.group0()[2]]))
                + (swizzle!(other.group0(), 2, 3, 1, 0) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2]))])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[3] * other.group0()[1] * -1.0),
                (self.group2()[3] * other.group0()[2] * -1.0),
                (self.group2()[3] * other.group0()[3] * -1.0),
                (-(self.group2()[3] * other.group0()[0])
                    + (self.group1()[2] * other.group0()[3])
                    + (self.group1()[0] * other.group0()[1])
                    + (self.group1()[1] * other.group0()[2])),
            ]),
        );
    }
}
impl AntiWedge<Horizon> for VersorOddAligningOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]])),
            // e1, e2, e3, e5
            (self.group0() * Simd32x4::from(other[e3215])),
        );
    }
}
impl AntiWedge<Infinity> for VersorOddAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       21        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group0()[0]) - (self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[3] * other.group0()[1]) + (self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                ((self.group2()[3] * other.group0()[2]) - (self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorOddAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<LineOnOrigin> for VersorOddAligningOrigin {
    type Output = DipoleAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd3        0        1        0
    // Totals...
    // yes simd        2        7        0
    //  no simd        2        9        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return DipoleAligningOrigin::from_groups(
            // e41, e42, e43, e45
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35
            (Simd32x3::from(self.group2()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<Motor> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       26        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       15       28        0
    //  no simd       18       34        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[3]),
                    (self.group0()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group2()[0] * other.group0()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group0()[0]) + (self.group2()[2] * other.group1()[1]) + (self.group1()[0] * other.group0()[3]) - (self.group2()[1] * other.group1()[2])),
                ((self.group2()[3] * other.group0()[1]) - (self.group2()[2] * other.group1()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group2()[0] * other.group1()[2])),
                ((self.group2()[3] * other.group0()[2]) + (self.group2()[1] * other.group1()[0]) + (self.group1()[2] * other.group0()[3]) - (self.group2()[0] * other.group1()[1])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorOddAligningOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       13        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                ((self.group1()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for VersorOddAligningOrigin {
    type Output = VersorOddAligningOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOddAligningOrigin::from_groups(
            // e41, e42, e43, e45
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group1()[3] * other.group0()[0]),
                    (self.group1()[3] * other.group0()[1]),
                    (self.group1()[3] * other.group0()[2]),
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) - (self.group2()[0] * other.group0()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group2()[3] * other.group0()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group2()[3] * other.group0()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group2()[3] * other.group0()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for VersorOddAligningOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       32       47        0
    //    simd3        7       11        0
    //    simd4        6        7        0
    // Totals...
    // yes simd       45       65        0
    //  no simd       77      108        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group2()[3] * other.group1()[3])
                    + (self.group2()[2] * other.group1()[2])
                    + (self.group2()[1] * other.group1()[1])
                    + (self.group2()[0] * other.group1()[0])
                    + (self.group1()[3] * other[e1])
                    - (self.group1()[2] * other.group7()[2])
                    - (self.group1()[1] * other.group7()[1])
                    - (self.group1()[0] * other.group7()[0])
                    - (self.group0()[3] * other.group6()[3])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group5()[2], other.group5()[0], other.group5()[1], other.group3()[2]]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group4()[0], other.group4()[1], other.group4()[2], other.group3()[3]]))
                - (Simd32x4::from(other.group9()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group3()[0]) - (self.group2()[2] * other.group5()[1]) + (self.group0()[0] * other[e45])),
                    (-(self.group2()[3] * other.group3()[1]) - (self.group2()[0] * other.group5()[2]) + (self.group0()[1] * other[e45])),
                    (-(self.group2()[3] * other.group3()[2]) - (self.group2()[1] * other.group5()[0]) + (self.group0()[2] * other[e45])),
                    ((self.group2()[1] * other.group3()[1]) + (self.group2()[0] * other.group3()[0])
                        - (self.group0()[2] * other.group9()[3])
                        - (self.group0()[0] * other.group9()[1])
                        - (self.group0()[1] * other.group9()[2])),
                ])),
            // e5
            (-(self.group2()[3] * other.group3()[3]) - (self.group2()[2] * other.group4()[2]) - (self.group2()[1] * other.group4()[1]) - (self.group2()[0] * other.group4()[0])
                + (self.group1()[2] * other.group9()[3])
                + (self.group1()[1] * other.group9()[2])
                + (self.group0()[3] * other[e45])
                + (self.group1()[0] * other.group9()[1])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group2(), 2, 0, 1, 2) * Simd32x4::from([other.group7()[1], other.group7()[2], other.group7()[0], other.group6()[2]]))
                + (self.group0() * Simd32x4::from(other.group0()[1]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group7()[2]) + (self.group1()[3] * other.group6()[0])),
                    ((self.group2()[2] * other.group7()[0]) + (self.group1()[3] * other.group6()[1])),
                    ((self.group2()[0] * other.group7()[1]) + (self.group1()[3] * other.group6()[2])),
                    (-(self.group2()[1] * other.group6()[1]) - (self.group2()[0] * other.group6()[0])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group2()[3]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))
                + (swizzle!(other.group8(), 1, 2, 0) * Simd32x3::from([self.group2()[2], self.group2()[0], self.group2()[1]]))
                + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                - (swizzle!(other.group8(), 2, 0, 1) * Simd32x3::from([self.group2()[1], self.group2()[2], self.group2()[0]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group2()[3]) * other.group7()) + (Simd32x3::from(self.group1()[3]) * other.group8())
                - (Simd32x3::from(other.group6()[3]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))),
            // e415, e425, e435, e321
            (-(swizzle!(self.group2(), 1, 2, 0, 3) * swizzle!(other.group9(), 3, 1, 2, 0))
                + Simd32x4::from([
                    (self.group2()[2] * other.group9()[2]),
                    (self.group2()[0] * other.group9()[3]),
                    (self.group2()[1] * other.group9()[1]),
                    (self.group1()[3] * other[e45]),
                ])),
            // e423, e431, e412
            ((Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))
                - (Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))),
            // e235, e315, e125
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))
                - (Simd32x3::from(self.group2()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e1234, e4235, e4315, e4125
            (Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group1()[3], self.group2()[0], self.group2()[1], self.group2()[2]])),
            // e3215
            (self.group2()[3] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorOddAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorOddAligningOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group2()[3] * other.group0()[0] * -1.0),
            (self.group2()[3] * other.group0()[1] * -1.0),
            (self.group2()[3] * other.group0()[2] * -1.0),
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorOddAligningOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group2() * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group0()[3], self.group1()[0], self.group1()[1], self.group1()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorOddAligningOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorOddAligningOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        3       12        0
    //  no simd        6       21        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group2() * Simd32x4::from(other.group0()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            (-(Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group0()[3], self.group1()[0], self.group1()[1], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                    ((self.group2()[3] * other.group0()[0]) * -1.0),
                    ((self.group2()[3] * other.group0()[1]) * -1.0),
                    ((self.group2()[3] * other.group0()[2]) * -1.0),
                ])),
        );
    }
}
impl AntiWedge<Origin> for VersorOddAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group2()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for VersorOddAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       25        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       26        0
    //  no simd       11       29        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            ((swizzle!(other.group0(), 3, 3, 3, 2) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[3] * other.group0()[0]) * -1.0),
                    ((self.group2()[3] * other.group0()[1]) * -1.0),
                    ((self.group2()[3] * other.group0()[2]) * -1.0),
                    ((self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group0()[0])),
                ])),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorOddAligningOrigin {
    type Output = VersorEvenAligningOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       21        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorEvenAligningOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0] * -1.0),
                (self.group2()[3] * other.group0()[1] * -1.0),
                (self.group2()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for VersorOddAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        5        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self.group2()[3] * other.group0()[3])
                + (self.group2()[2] * other.group0()[2])
                + (self.group2()[1] * other.group0()[1])
                + (self.group1()[3] * other[e2])
                + (self.group2()[0] * other.group0()[0])),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for VersorOddAligningOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ ((self.group1()[3] * other.group0()[1]) + (self.group2()[3] * other.group0()[0])));
    }
}
impl AntiWedge<Sphere> for VersorOddAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       21        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       10       25        0
    //  no simd       19       37        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group1()[3] * other.group0()[0]) - (self.group2()[0] * other[e4315])),
                ((self.group1()[3] * other.group0()[1]) - (self.group2()[1] * other[e4315])),
                ((self.group1()[3] * other.group0()[2]) - (self.group2()[2] * other[e4315])),
                0.0,
            ]),
            // e415, e425, e435, e321
            (-(swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other[e4315]]))
                + (swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[3]]))),
            // e235, e315, e125, e5
            ((swizzle!(other.group0(), 3, 3, 3, 2) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[2]]))
                + Simd32x4::from([
                    ((self.group2()[3] * other.group0()[0]) * -1.0),
                    ((self.group2()[3] * other.group0()[1]) * -1.0),
                    ((self.group2()[3] * other.group0()[2]) * -1.0),
                    ((self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group0()[0])),
                ])),
            // e1, e2, e3, e4
            (-(Simd32x4::from(other[e4315]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[3]),
                    (self.group0()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorOddAligningOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        4       17        0
    //  no simd        4       20        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group2()[0] * other.group0()[1] * -1.0),
                (self.group2()[1] * other.group0()[1] * -1.0),
                (self.group2()[2] * other.group0()[1] * -1.0),
                ((self.group1()[3] * other.group0()[0]) - (self.group2()[3] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[3]])),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[0]) - (self.group1()[0] * other.group0()[1])),
                ((self.group0()[1] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[0]) - (self.group1()[2] * other.group0()[1])),
                (self.group0()[3] * other.group0()[1] * -1.0),
            ]),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorOddAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       33        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group1()[3] * other.group0()[0]) - (self.group2()[0] * other.group0()[3])),
                ((self.group1()[3] * other.group0()[1]) - (self.group2()[1] * other.group0()[3])),
                ((self.group1()[3] * other.group0()[2]) - (self.group2()[2] * other.group0()[3])),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                (self.group2()[3] * other.group0()[3] * -1.0),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0] * -1.0),
                (self.group2()[3] * other.group0()[1] * -1.0),
                (self.group2()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3] * -1.0),
                (self.group1()[1] * other.group0()[3] * -1.0),
                (self.group1()[2] * other.group0()[3] * -1.0),
                (-(self.group0()[3] * other.group0()[3])
                    - (self.group0()[2] * other.group0()[2])
                    - (self.group0()[0] * other.group0()[0])
                    - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       30        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       23       36        0
    //  no simd       38       54        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group3()[3]]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[3]),
                    (self.group0()[2] * other.group0()[3]),
                    ((self.group2()[2] * other.group3()[2]) + (self.group2()[1] * other.group3()[1]) + (self.group2()[0] * other.group3()[0])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[3] * other.group1()[3])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            ((other.group0() * Simd32x4::from([self.group2()[3], self.group2()[3], self.group2()[3], self.group0()[3]]))
                - (swizzle!(self.group2(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group1()[3] * other.group2()[0]),
                    (self.group1()[3] * other.group2()[1]),
                    (self.group1()[3] * other.group2()[2]),
                    (-(self.group2()[1] * other.group1()[1]) - (self.group2()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) + (self.group2()[2] * other.group2()[1]) + (self.group1()[0] * other.group0()[3]) - (self.group2()[1] * other.group2()[2])),
                ((self.group2()[3] * other.group1()[1]) - (self.group2()[2] * other.group2()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group2()[0] * other.group2()[2])),
                ((self.group2()[3] * other.group1()[2]) + (self.group2()[1] * other.group2()[0]) + (self.group1()[2] * other.group0()[3]) - (self.group2()[0] * other.group2()[1])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       27        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       19       32        0
    //  no simd       31       47        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[3]),
                    (self.group0()[2] * other.group0()[3]),
                    (-(self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            ((other.group0() * Simd32x4::from([self.group2()[3], self.group2()[3], self.group2()[3], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group1()[3] * other.group2()[0]),
                    (self.group1()[3] * other.group2()[1]),
                    (self.group1()[3] * other.group2()[2]),
                    (-(self.group2()[2] * other.group1()[2]) - (self.group2()[1] * other.group1()[1]) - (self.group2()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) + (self.group2()[2] * other.group2()[1]) + (self.group1()[0] * other.group0()[3]) - (self.group2()[1] * other.group2()[2])),
                ((self.group2()[3] * other.group1()[1]) - (self.group2()[2] * other.group2()[0]) + (self.group1()[1] * other.group0()[3]) + (self.group2()[0] * other.group2()[2])),
                ((self.group2()[3] * other.group1()[2]) + (self.group2()[1] * other.group2()[0]) + (self.group1()[2] * other.group0()[3]) - (self.group2()[0] * other.group2()[1])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       25        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       19       29        0
    //  no simd       28       41        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 0, 0, 0, 3) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group2()[2]]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[1] * other.group0()[2]) + (self.group2()[0] * other.group0()[1])
                        - (self.group0()[3] * other.group1()[3])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group2(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group1()[3] * other.group2()[0]),
                    (self.group1()[3] * other.group2()[1]),
                    (self.group1()[3] * other.group2()[2]),
                    (-(self.group2()[1] * other.group1()[1]) + (self.group0()[3] * other.group0()[0]) - (self.group2()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) + (self.group2()[2] * other.group2()[1]) + (self.group1()[0] * other.group0()[0]) - (self.group2()[1] * other.group2()[2])),
                ((self.group2()[3] * other.group1()[1]) - (self.group2()[2] * other.group2()[0]) + (self.group1()[1] * other.group0()[0]) + (self.group2()[0] * other.group2()[2])),
                ((self.group2()[3] * other.group1()[2]) + (self.group2()[1] * other.group2()[0]) + (self.group1()[2] * other.group0()[0]) - (self.group2()[0] * other.group2()[1])),
                (self.group1()[3] * other.group0()[0]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorOddAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[3] * other.group1()[3])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) + (self.group2()[3] * other.group0()[0])),
                ((self.group1()[3] * other.group1()[1]) + (self.group2()[3] * other.group0()[1])),
                ((self.group1()[3] * other.group1()[2]) + (self.group2()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                (-(self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorOddAligningOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       22        0
    //    simd4        2        3        0
    // Totals...
    // yes simd       12       25        0
    //  no simd       18       34        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + (swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[3] * other.group1()[0])),
                    ((self.group0()[1] * other.group0()[3]) + (self.group1()[3] * other.group1()[1])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[3] * other.group1()[2])),
                    (-(self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[1] * other.group1()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group2()[0] * other.group1()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group2()[3] * other.group1()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group2()[3] * other.group1()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group2()[3] * other.group1()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorOddAligningOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       25        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       20       27        0
    //  no simd       26       33        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group2()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[2] * other.group2()[2])
                        + (self.group2()[1] * other.group2()[1])
                        + (self.group2()[0] * other.group2()[0])
                        + (self.group1()[3] * other.group1()[3])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[3] * other.group0()[3])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group2()[3] * other.group0()[0]) + (self.group1()[3] * other.group1()[0]) - (self.group2()[0] * other.group0()[3])),
                ((self.group2()[3] * other.group0()[1]) + (self.group1()[3] * other.group1()[1]) - (self.group2()[1] * other.group0()[3])),
                ((self.group2()[3] * other.group0()[2]) + (self.group1()[3] * other.group1()[2]) - (self.group2()[2] * other.group0()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                (-(self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOdd> for VersorOddAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       26        0
    //    simd4        6        7        0
    // Totals...
    // yes simd       24       33        0
    //  no simd       42       54        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group1()[3] * other.group3()[0]) - (self.group2()[0] * other.group2()[3])),
                ((self.group1()[3] * other.group3()[1]) - (self.group2()[1] * other.group2()[3])),
                ((self.group1()[3] * other.group3()[2]) - (self.group2()[2] * other.group2()[3])),
                0.0,
            ]),
            // e415, e425, e435, e321
            (-(swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group3()[2], other.group3()[0], other.group3()[1], other.group2()[3]]))
                + (swizzle!(other.group3(), 1, 2, 0, 3) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[3]]))),
            // e235, e315, e125, e5
            ((swizzle!(other.group3(), 3, 3, 3, 2) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[2]]))
                - (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group2()[2]) - (self.group2()[1] * other.group2()[1]) - (self.group2()[0] * other.group2()[0])
                        + (self.group1()[1] * other.group3()[1])
                        + (self.group0()[3] * other.group3()[3])
                        + (self.group1()[0] * other.group3()[0])),
                ])),
            // e1, e2, e3, e4
            ((swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                - (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group2()[2] * other.group1()[1]) + (self.group0()[0] * other.group3()[3])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2]) + (self.group0()[1] * other.group3()[3])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group2()[1] * other.group1()[0]) + (self.group0()[2] * other.group3()[3])),
                    ((self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group3()[2])
                        - (self.group0()[0] * other.group3()[0])
                        - (self.group0()[1] * other.group3()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorOddAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       24        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       21       30        0
    //  no simd       36       48        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group1()[3] * other.group2()[0]) - (self.group2()[0] * other.group1()[3])),
                ((self.group1()[3] * other.group2()[1]) - (self.group2()[1] * other.group1()[3])),
                ((self.group1()[3] * other.group2()[2]) - (self.group2()[2] * other.group1()[3])),
                0.0,
            ]),
            // e415, e425, e435, e321
            (-(swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group2()[2], other.group2()[0], other.group2()[1], other.group1()[3]]))
                + (swizzle!(other.group2(), 1, 2, 0, 3) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[3]]))),
            // e235, e315, e125, e5
            ((swizzle!(other.group2(), 3, 3, 3, 2) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[2]]))
                - (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2]) - (self.group2()[1] * other.group1()[1]) - (self.group2()[0] * other.group1()[0])
                        + (self.group1()[1] * other.group2()[1])
                        + (self.group0()[3] * other.group2()[3])
                        + (self.group1()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                - (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) + (self.group0()[0] * other.group2()[3])),
                    (-(self.group2()[3] * other.group0()[1]) + (self.group0()[1] * other.group2()[3])),
                    (-(self.group2()[3] * other.group0()[2]) + (self.group0()[2] * other.group2()[3])),
                    ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorOddAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       28        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       19       31        0
    //  no simd       28       40        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[3] * other.group2()[0]),
                (self.group1()[3] * other.group2()[1]),
                (self.group1()[3] * other.group2()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group2()[2]) + (self.group2()[2] * other.group2()[1])),
                ((self.group2()[0] * other.group2()[2]) - (self.group2()[2] * other.group2()[0])),
                (-(self.group2()[0] * other.group2()[1]) + (self.group2()[1] * other.group2()[0])),
                (self.group1()[3] * other.group2()[3]),
            ]),
            // e235, e315, e125, e5
            ((swizzle!(other.group2(), 3, 3, 3, 2) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[2]]))
                - (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[3]) - (self.group2()[1] * other.group0()[2]) - (self.group2()[0] * other.group0()[1])
                        + (self.group1()[1] * other.group2()[1])
                        + (self.group0()[3] * other.group2()[3])
                        + (self.group1()[0] * other.group2()[0])),
                ])),
            // e1, e2, e3, e4
            ((swizzle!(other.group1(), 2, 0, 1, 3) * Simd32x4::from([self.group2()[1], self.group2()[2], self.group2()[0], self.group1()[3]]))
                + Simd32x4::from([
                    (-(self.group2()[2] * other.group1()[1]) + (self.group0()[0] * other.group2()[3]) + (self.group1()[3] * other.group0()[1])),
                    (-(self.group2()[0] * other.group1()[2]) + (self.group0()[1] * other.group2()[3]) + (self.group1()[3] * other.group0()[2])),
                    (-(self.group2()[1] * other.group1()[0]) + (self.group0()[2] * other.group2()[3]) + (self.group1()[3] * other.group0()[3])),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorOddAligningOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       23        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       25        0
    //  no simd       16       31        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group2()[0] * other.group1()[3] * -1.0),
                (self.group2()[1] * other.group1()[3] * -1.0),
                (self.group2()[2] * other.group1()[3] * -1.0),
                ((self.group1()[3] * other.group0()[3]) - (self.group2()[3] * other.group1()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[1] * other.group1()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group2()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 3, 3, 3, 2) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group2()[2]]))
                - (Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) + (self.group1()[3] * other.group1()[0])),
                    (-(self.group2()[3] * other.group0()[1]) + (self.group1()[3] * other.group1()[1])),
                    (-(self.group2()[3] * other.group0()[2]) + (self.group1()[3] * other.group1()[2])),
                    ((self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorOddAligningOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       37        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       16       38        0
    //  no simd       19       41        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[1]) - (self.group2()[0] * other.group1()[0])),
                ((self.group1()[3] * other.group1()[2]) - (self.group2()[1] * other.group1()[0])),
                ((self.group1()[3] * other.group1()[3]) - (self.group2()[2] * other.group1()[0])),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[3]) + (self.group2()[2] * other.group1()[2])),
                ((self.group2()[0] * other.group1()[3]) - (self.group2()[2] * other.group1()[1])),
                (-(self.group2()[0] * other.group1()[2]) + (self.group2()[1] * other.group1()[1])),
                (self.group2()[3] * other.group1()[0] * -1.0),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[3] * other.group1()[1] * -1.0),
                (self.group2()[3] * other.group1()[2] * -1.0),
                (self.group2()[3] * other.group1()[3] * -1.0),
                (-(self.group2()[3] * other.group0()[3])
                    + (self.group1()[2] * other.group1()[3])
                    + (self.group1()[0] * other.group1()[1])
                    + (self.group1()[1] * other.group1()[2])),
            ]),
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group1()[0]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    ((self.group2()[3] * other.group0()[0]) * -1.0),
                    ((self.group2()[3] * other.group0()[1]) * -1.0),
                    ((self.group2()[3] * other.group0()[2]) * -1.0),
                    ((self.group2()[2] * other.group0()[2])
                        + (self.group2()[1] * other.group0()[1])
                        + (self.group2()[0] * other.group0()[0])
                        + (self.group1()[3] * other.group0()[3])
                        - (self.group0()[2] * other.group1()[3])
                        - (self.group0()[0] * other.group1()[1])
                        - (self.group0()[1] * other.group1()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorOddAligningOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       29        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       16       31        0
    //  no simd       22       37        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group2()[0] * other.group2()[3] * -1.0),
                (self.group2()[1] * other.group2()[3] * -1.0),
                (self.group2()[2] * other.group2()[3] * -1.0),
                ((self.group1()[3] * other.group1()[3]) - (self.group2()[3] * other.group2()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group1()[3]),
                (self.group2()[1] * other.group1()[3]),
                (self.group2()[2] * other.group1()[3]),
                (-(self.group2()[2] * other.group2()[2]) - (self.group2()[1] * other.group2()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group2()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                - (Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group2()[2] * other.group1()[1])
                        + (self.group1()[3] * other.group2()[0])
                        + (self.group0()[0] * other.group1()[3])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2])
                        + (self.group1()[3] * other.group2()[1])
                        + (self.group0()[1] * other.group1()[3])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group2()[1] * other.group1()[0])
                        + (self.group1()[3] * other.group2()[2])
                        + (self.group0()[2] * other.group1()[3])),
                    ((self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl InfixAntiWedge for VersorOddAtInfinity {}
impl AntiWedge<AntiCircleOnOrigin> for VersorOddAtInfinity {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group2()[2] * other.group1()[1])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group2()[1] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorOddAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (-(self.group2()[0] * other.group0()[3]) + (self.group2()[3] * other.group0()[0])),
                (-(self.group2()[1] * other.group0()[3]) + (self.group2()[3] * other.group0()[1])),
                (-(self.group2()[2] * other.group0()[3]) + (self.group2()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDualNum> for VersorOddAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return MotorAtInfinity::from_groups(
            // e235, e315, e125, e5
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for VersorOddAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e321]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorOddAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        3       14        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]) * Simd32x4::from(-1.0)),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for VersorOddAtInfinity {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       16        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3] * -1.0),
                (self.group2()[1] * other.group0()[3] * -1.0),
                (self.group2()[2] * other.group0()[3] * -1.0),
                ((self.group2()[2] * other.group1()[2]) + (self.group2()[1] * other.group1()[1]) - (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group1()[0])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for VersorOddAtInfinity {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group2()[0] * other.group0()[0] * -1.0),
            (self.group2()[1] * other.group0()[0] * -1.0),
            (self.group2()[2] * other.group0()[0] * -1.0),
            ((self.group2()[2] * other.group0()[3]) + (self.group2()[1] * other.group0()[2]) - (self.group1()[3] * other.group0()[0]) + (self.group2()[0] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorOddAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        6        0
    //  no simd        5        9        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group2(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for VersorOddAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group2()[1], self.group2()[2], self.group2()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group2()[2], self.group2()[0], self.group2()[1]]))),
        );
    }
}
impl AntiWedge<AntiMotor> for VersorOddAtInfinity {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        6       13        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            Simd32x4::from([
                (self.group2()[0] * other.group1()[3]),
                (self.group2()[1] * other.group1()[3]),
                (self.group2()[2] * other.group1()[3]),
                0.0,
            ]),
            // e1, e2, e3, e5
            (-(swizzle!(self.group2(), 2, 0, 1, 2) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group2()[1] * other.group1()[1]) + (self.group1()[3] * other.group1()[3]) - (self.group2()[0] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for VersorOddAtInfinity {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
            (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
            ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for VersorOddAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for VersorOddAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for VersorOddAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            (self.group0() * Simd32x4::from(other[e12345])),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other[e12345])),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for VersorOddAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group2()[3] * other.group0()[3]) + (self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorOddAtInfinity {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        6       12        0
    //  no simd       12       24        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group2() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            ((swizzle!(self.group2(), 2, 1, 2, 0) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                - (swizzle!(other.group1(), 3, 1, 3, 0) * Simd32x4::from([self.group1()[3], self.group2()[2], self.group0()[2], self.group2()[1]]))
                + Simd32x4::from([
                    ((self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                    (-(self.group2()[3] * other.group0()[0]) - (self.group0()[1] * other.group1()[3])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group0()[3] * other.group1()[3])),
                ])),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorOddAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[0]]))
                - (swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[2] * other.group1()[3]) + (self.group2()[1] * other.group1()[2]) + (self.group2()[0] * other.group1()[1])
                        - (self.group0()[3] * other.group0()[2])
                        - (self.group0()[1] * other.group0()[0])
                        - (self.group0()[2] * other.group0()[1])),
                ])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (-(self.group2()[0] * other.group0()[3]) + (self.group2()[3] * other.group0()[0])),
                (-(self.group2()[1] * other.group0()[3]) + (self.group2()[3] * other.group0()[1])),
                (-(self.group2()[2] * other.group0()[3]) + (self.group2()[3] * other.group0()[2])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Circle> for VersorOddAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       27        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       17       28        0
    //  no simd       20       31        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (-(self.group1()[3] * other.group1()[3])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[3] * other.group0()[2])
                    - (self.group0()[1] * other.group0()[0])
                    - (self.group0()[2] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group2(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group2()[3] * other.group0()[0]),
                    (self.group2()[3] * other.group0()[1]),
                    (self.group2()[3] * other.group0()[2]),
                    (-(self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) - (self.group2()[1] * other.group2()[2]) + (self.group2()[2] * other.group2()[1])),
                ((self.group2()[3] * other.group1()[1]) + (self.group2()[0] * other.group2()[2]) - (self.group2()[2] * other.group2()[0])),
                ((self.group2()[3] * other.group1()[2]) - (self.group2()[0] * other.group2()[1]) + (self.group2()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorOddAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       16       27        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[3] * other.group0()[2])
                    - (self.group0()[1] * other.group0()[0])
                    - (self.group0()[2] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) - (self.group2()[1] * other.group2()[2]) + (self.group2()[2] * other.group2()[1])),
                ((self.group2()[3] * other.group1()[1]) + (self.group2()[0] * other.group2()[2]) - (self.group2()[2] * other.group2()[0])),
                ((self.group2()[3] * other.group1()[2]) - (self.group2()[0] * other.group2()[1]) + (self.group2()[1] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorOddAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       22        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[3] * other.group0()[3])
                    - (self.group1()[2] * other.group0()[2])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group1()[1] * other.group0()[1])),
                ((self.group2()[3] * other.group0()[0]) - (self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[3] * other.group0()[1]) + (self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                ((self.group2()[3] * other.group0()[2]) - (self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3] * -1.0),
                (self.group2()[1] * other.group0()[3] * -1.0),
                (self.group2()[2] * other.group0()[3] * -1.0),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorOddAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                (-(self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for VersorOddAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       21        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (-(self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[3] * other.group0()[2])
                    - (self.group0()[1] * other.group0()[0])
                    - (self.group0()[2] * other.group0()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group2()[3] * other.group1()[0]),
                (self.group2()[3] * other.group1()[1]),
                (self.group2()[3] * other.group1()[2]),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorOddAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       12       22        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[3]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (-(self.group2()[0] * other.group0()[3]) + (self.group2()[3] * other.group0()[0])),
                (-(self.group2()[1] * other.group0()[3]) + (self.group2()[3] * other.group0()[1])),
                (-(self.group2()[2] * other.group0()[3]) + (self.group2()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                (-(self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Dipole> for VersorOddAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       13        0
    //  no simd       11       16        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group2()[2] * other.group1()[1])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group2()[1] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group2()[3] * other.group1()[3]) - (self.group2()[2] * other.group2()[2]) - (self.group2()[0] * other.group2()[0]) - (self.group2()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorOddAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       13        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0] * -1.0),
                (self.group2()[3] * other.group0()[1] * -1.0),
                (self.group2()[3] * other.group0()[2] * -1.0),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorOddAtInfinity {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        6       10        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group2(), 2, 0, 1, 3) * swizzle!(other.group0(), 1, 2, 0, 3))
                + Simd32x4::from([
                    (self.group2()[1] * other.group0()[2]),
                    (self.group2()[2] * other.group0()[0]),
                    (self.group2()[0] * other.group0()[1]),
                    (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorOddAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       12        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0] * -1.0),
                (self.group2()[3] * other.group0()[1] * -1.0),
                (self.group2()[3] * other.group0()[2] * -1.0),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e5
            (-(self.group2()[2] * other.group1()[2]) - (self.group2()[0] * other.group1()[0]) - (self.group2()[1] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorOddAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       11        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0] * -1.0),
                (self.group2()[3] * other.group0()[1] * -1.0),
                (self.group2()[3] * other.group0()[2] * -1.0),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e5
            (self.group2()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorOddAtInfinity {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       11        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        7       12        0
    //  no simd       10       15        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group2()[2] * other.group1()[1])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group2()[1] * other.group1()[0])),
                    ((self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
                ])),
            // e5
            (-(self.group2()[2] * other.group2()[2]) - (self.group2()[0] * other.group2()[0]) - (self.group2()[1] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for VersorOddAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            (self.group0() * Simd32x4::from(other.group0()[1])),
            // e23, e31, e12, e45
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorOddAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        2        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Infinity::from_groups(/* e5 */ (self.group2()[3] * other[e45] * -1.0));
    }
}
impl AntiWedge<FlatPoint> for VersorOddAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group2()[3] * other.group0()[3]) - (self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorOddAtInfinity {
    type Output = Infinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return Infinity::from_groups(
            // e5
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<Flector> for VersorOddAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[2] * other.group1()[1])),
                (-(self.group1()[0] * other.group1()[2]) + (self.group1()[2] * other.group1()[0])),
                ((self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group1()[0])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                (-(self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) - (self.group2()[0] * other.group0()[0])
                        + (self.group0()[3] * other.group1()[2])
                        + (self.group0()[1] * other.group1()[0])
                        + (self.group0()[2] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorOddAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return MotorAtInfinity::from_groups(/* e235, e315, e125, e5 */ Simd32x4::from([
            (self.group2()[0] * other.group0()[3]),
            (self.group2()[1] * other.group0()[3]),
            (self.group2()[2] * other.group0()[3]),
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3]) - (self.group2()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorOddAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        9       22        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                ((self.group1()[1] * other.group0()[3]) - (self.group1()[2] * other.group0()[2])),
                (-(self.group1()[0] * other.group0()[3]) + (self.group1()[2] * other.group0()[1])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[3]) + (self.group2()[2] * other.group0()[2])),
                ((self.group2()[0] * other.group0()[3]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[1] * other.group0()[1])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[3] * other.group0()[1] * -1.0),
                (self.group2()[3] * other.group0()[2] * -1.0),
                (self.group2()[3] * other.group0()[3] * -1.0),
                (-(self.group2()[3] * other.group0()[0])
                    + (self.group0()[3] * other.group0()[3])
                    + (self.group0()[1] * other.group0()[1])
                    + (self.group0()[2] * other.group0()[2])),
            ]),
        );
    }
}
impl AntiWedge<Horizon> for VersorOddAtInfinity {
    type Output = MotorAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return MotorAtInfinity::from_groups(
            // e235, e315, e125, e5
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]])),
        );
    }
}
impl AntiWedge<Line> for VersorOddAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       10       15        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                ((self.group2()[3] * other.group0()[0]) - (self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[3] * other.group0()[1]) + (self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                ((self.group2()[3] * other.group0()[2]) - (self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorOddAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(
            // e15, e25, e35
            (-(swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group2()[1], self.group2()[2], self.group2()[0]]))
                + (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group2()[2], self.group2()[0], self.group2()[1]]))),
        );
    }
}
impl AntiWedge<LineOnOrigin> for VersorOddAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                0.0,
                0.0,
                0.0,
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<Motor> for VersorOddAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       19        0
    //    simd4        1        2        0
    // Totals...
    // yes simd       12       21        0
    //  no simd       15       27        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((swizzle!(other.group0(), 3, 0, 1, 2) * Simd32x4::from([self.group0()[0], self.group2()[3], self.group2()[3], self.group2()[3]]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                    ((self.group2()[2] * other.group1()[1]) + (self.group0()[1] * other.group0()[3]) - (self.group2()[1] * other.group1()[2])),
                    (-(self.group2()[2] * other.group1()[0]) + (self.group0()[2] * other.group0()[3]) + (self.group2()[0] * other.group1()[2])),
                    ((self.group2()[1] * other.group1()[0]) + (self.group0()[3] * other.group0()[3]) - (self.group2()[0] * other.group1()[1])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3])
                    - (self.group2()[0] * other.group0()[0])),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorOddAtInfinity {
    type Output = FlatPointAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return FlatPointAtInfinity::from_groups(/* e15, e25, e35 */ Simd32x3::from([
            (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
            ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
            (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for VersorOddAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                    (self.group2()[3] * other.group0()[0]),
                    (self.group2()[3] * other.group0()[1]),
                    (self.group2()[3] * other.group0()[2]),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3]),
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3])
                    - (self.group2()[0] * other.group0()[0])),
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for VersorOddAtInfinity {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       33       52        0
    //    simd3        6       11        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       42       66        0
    //  no simd       63       97        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group2()[3] * other.group1()[3]) + (self.group2()[2] * other.group1()[2]) + (self.group2()[1] * other.group1()[1]) + (self.group2()[0] * other.group1()[0])
                    - (self.group1()[3] * other.group6()[3])
                    - (self.group1()[2] * other.group6()[2])
                    - (self.group1()[1] * other.group6()[1])
                    - (self.group1()[0] * other.group6()[0])
                    - (self.group0()[3] * other.group7()[2])
                    - (self.group0()[2] * other.group7()[1])
                    + (self.group0()[0] * other.group0()[1])
                    - (self.group0()[1] * other.group7()[0])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group5()[2], other.group5()[0], other.group5()[1], other.group3()[2]]))
                - (swizzle!(other.group9(), 2, 0, 1, 0) * Simd32x4::from([self.group1()[2], self.group0()[2], self.group1()[1], self.group1()[3]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group3()[0]) - (self.group2()[2] * other.group5()[1]) - (self.group0()[1] * other.group9()[0])
                        + (self.group1()[1] * other.group9()[3])),
                    (-(self.group2()[3] * other.group3()[1]) - (self.group2()[0] * other.group5()[2]) + (self.group1()[2] * other.group9()[1])
                        - (self.group1()[0] * other.group9()[3])),
                    (-(self.group2()[3] * other.group3()[2]) - (self.group2()[1] * other.group5()[0]) - (self.group0()[3] * other.group9()[0])
                        + (self.group1()[0] * other.group9()[2])),
                    ((self.group2()[1] * other.group3()[1]) + (self.group2()[0] * other.group3()[0])),
                ])),
            // e5
            (-(self.group2()[3] * other.group3()[3]) - (self.group2()[2] * other.group4()[2]) - (self.group2()[1] * other.group4()[1]) - (self.group2()[0] * other.group4()[0])
                + (self.group1()[3] * other[e45])
                + (self.group0()[3] * other.group9()[3])
                + (self.group0()[1] * other.group9()[1])
                + (self.group0()[2] * other.group9()[2])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group2(), 2, 0, 1, 2) * Simd32x4::from([other.group7()[1], other.group7()[2], other.group7()[0], other.group6()[2]]))
                + Simd32x4::from([
                    (self.group2()[1] * other.group7()[2]),
                    (self.group2()[2] * other.group7()[0]),
                    (self.group2()[0] * other.group7()[1]),
                    (-(self.group2()[1] * other.group6()[1]) + (self.group1()[3] * other.group0()[1]) - (self.group2()[0] * other.group6()[0])),
                ])),
            // e15, e25, e35
            ((Simd32x3::from(self.group2()[3]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))
                + (swizzle!(other.group8(), 1, 2, 0) * Simd32x3::from([self.group2()[2], self.group2()[0], self.group2()[1]]))
                + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group0()[1], self.group0()[2], self.group0()[3]]))
                - (swizzle!(other.group8(), 2, 0, 1) * Simd32x3::from([self.group2()[1], self.group2()[2], self.group2()[0]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group2()[3]) * other.group7()) + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                - (Simd32x3::from(other.group6()[3]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group9()[3]) + (self.group2()[2] * other.group9()[2])),
                ((self.group2()[0] * other.group9()[3]) - (self.group2()[2] * other.group9()[1])),
                (-(self.group2()[0] * other.group9()[2]) + (self.group2()[1] * other.group9()[1])),
                (self.group2()[3] * other.group9()[0] * -1.0),
            ]),
            // e423, e431, e412
            (Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]) * Simd32x3::from(-1.0)),
            // e235, e315, e125
            ((Simd32x3::from(other[e45]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))
                - (Simd32x3::from(self.group2()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([
                0.0,
                (self.group2()[0] * other.group0()[1]),
                (self.group2()[1] * other.group0()[1]),
                (self.group2()[2] * other.group0()[1]),
            ]),
            // e3215
            (self.group2()[3] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorOddAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group2()[1] * other.group0()[2]) - (self.group2()[2] * other.group0()[1])),
                (-(self.group2()[0] * other.group0()[2]) + (self.group2()[2] * other.group0()[0])),
                ((self.group2()[0] * other.group0()[1]) - (self.group2()[1] * other.group0()[0])),
                (-(self.group0()[3] * other.group0()[2]) - (self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1])),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorOddAtInfinity {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(/* e1, e2, e3, e4 */ Simd32x4::from([
            (self.group2()[3] * other.group0()[0] * -1.0),
            (self.group2()[3] * other.group0()[1] * -1.0),
            (self.group2()[3] * other.group0()[2] * -1.0),
            ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorOddAtInfinity {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        4        0
    // no simd        0       16        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group2() * Simd32x4::from(other[e1234]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[3], self.group0()[1], self.group0()[2], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorOddAtInfinity {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group0()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorOddAtInfinity {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        3       12        0
    //  no simd        6       21        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group2() * Simd32x4::from(other.group0()[3]) * Simd32x4::from(-1.0)),
            // e4, e1, e2, e3
            (-(Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[3], self.group0()[1], self.group0()[2], self.group0()[3]]))
                + Simd32x4::from([
                    ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                    ((self.group2()[3] * other.group0()[0]) * -1.0),
                    ((self.group2()[3] * other.group0()[1]) * -1.0),
                    ((self.group2()[3] * other.group0()[2]) * -1.0),
                ])),
        );
    }
}
impl AntiWedge<Origin> for VersorOddAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group2()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for VersorOddAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       21        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       12       25        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group2()[3] * other.group0()[0]) * -1.0),
                    ((self.group2()[3] * other.group0()[1]) * -1.0),
                    ((self.group2()[3] * other.group0()[2]) * -1.0),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorOddAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       21        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0] * -1.0),
                (self.group2()[3] * other.group0()[1] * -1.0),
                (self.group2()[3] * other.group0()[2] * -1.0),
                ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for VersorOddAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group2()[3] * other.group0()[3]) + (self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for VersorOddAtInfinity {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group2()[3] * other.group0()[0]));
    }
}
impl AntiWedge<Sphere> for VersorOddAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       34        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       12       35        0
    //  no simd       15       38        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group2()[0] * other[e4315] * -1.0),
                (self.group2()[1] * other[e4315] * -1.0),
                (self.group2()[2] * other[e4315] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                (self.group2()[3] * other[e4315] * -1.0),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group2()[3] * other.group0()[0]) * -1.0),
                    ((self.group2()[3] * other.group0()[1]) * -1.0),
                    ((self.group2()[3] * other.group0()[2]) * -1.0),
                    ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
                ])),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) - (self.group0()[1] * other[e4315]) + (self.group1()[1] * other.group0()[2])),
                ((self.group1()[2] * other.group0()[0]) - (self.group0()[2] * other[e4315]) - (self.group1()[0] * other.group0()[2])),
                (-(self.group1()[1] * other.group0()[0]) - (self.group0()[3] * other[e4315]) + (self.group1()[0] * other.group0()[1])),
                (self.group1()[3] * other[e4315] * -1.0),
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorOddAtInfinity {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        5        0
    // no simd        0       20        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group2() * Simd32x4::from(other.group0()[1]) * Simd32x4::from(-1.0)),
            // e235, e315, e125, e5
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]])),
            // e1, e2, e3, e4
            (Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[3], self.group1()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorOddAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       34        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3] * -1.0),
                (self.group2()[1] * other.group0()[3] * -1.0),
                (self.group2()[2] * other.group0()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group0()[2]) + (self.group2()[2] * other.group0()[1])),
                ((self.group2()[0] * other.group0()[2]) - (self.group2()[2] * other.group0()[0])),
                (-(self.group2()[0] * other.group0()[1]) + (self.group2()[1] * other.group0()[0])),
                (self.group2()[3] * other.group0()[3] * -1.0),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0] * -1.0),
                (self.group2()[3] * other.group0()[1] * -1.0),
                (self.group2()[3] * other.group0()[2] * -1.0),
                ((self.group0()[3] * other.group0()[2]) + (self.group0()[1] * other.group0()[0]) + (self.group0()[2] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[1]) - (self.group0()[1] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                ((self.group1()[2] * other.group0()[0]) - (self.group0()[2] * other.group0()[3]) - (self.group1()[0] * other.group0()[2])),
                (-(self.group1()[1] * other.group0()[0]) - (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                (self.group1()[3] * other.group0()[3] * -1.0),
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for VersorOddAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       19       27        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       23       32        0
    //  no simd       35       47        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group3()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[2] * other.group3()[2]) + (self.group2()[1] * other.group3()[1]) + (self.group2()[0] * other.group3()[0])
                        - (self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group0()[1])
                        + (self.group0()[0] * other.group0()[3])
                        - (self.group0()[1] * other.group0()[0])),
                ])),
            // e23, e31, e12, e45
            ((other.group0() * Simd32x4::from([self.group2()[3], self.group2()[3], self.group2()[3], self.group1()[3]]))
                - (swizzle!(self.group2(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[3]),
                    (self.group1()[1] * other.group0()[3]),
                    (self.group1()[2] * other.group0()[3]),
                    (-(self.group2()[1] * other.group1()[1]) - (self.group2()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) + (self.group2()[2] * other.group2()[1]) + (self.group0()[1] * other.group0()[3]) - (self.group2()[1] * other.group2()[2])),
                ((self.group2()[3] * other.group1()[1]) - (self.group2()[2] * other.group2()[0]) + (self.group0()[2] * other.group0()[3]) + (self.group2()[0] * other.group2()[2])),
                ((self.group2()[3] * other.group1()[2]) + (self.group2()[1] * other.group2()[0]) + (self.group0()[3] * other.group0()[3]) - (self.group2()[0] * other.group2()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorOddAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       24        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       19       28        0
    //  no simd       28       40        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group0()[1])
                        + (self.group0()[0] * other.group0()[3])
                        - (self.group0()[1] * other.group0()[0])),
                ])),
            // e23, e31, e12, e45
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group2()[3] * other.group0()[0]),
                    (self.group2()[3] * other.group0()[1]),
                    (self.group2()[3] * other.group0()[2]),
                    (-(self.group2()[2] * other.group1()[2]) - (self.group2()[1] * other.group1()[1]) - (self.group2()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group2()[3] * other.group1()[0]) + (self.group2()[2] * other.group2()[1]) + (self.group0()[1] * other.group0()[3]) - (self.group2()[1] * other.group2()[2])),
                ((self.group2()[3] * other.group1()[1]) - (self.group2()[2] * other.group2()[0]) + (self.group0()[2] * other.group0()[3]) + (self.group2()[0] * other.group2()[2])),
                ((self.group2()[3] * other.group1()[2]) + (self.group2()[1] * other.group2()[0]) + (self.group0()[3] * other.group0()[3]) - (self.group2()[0] * other.group2()[1])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorOddAtInfinity {
    type Output = VersorOddAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        5        6        0
    // Totals...
    // yes simd       10       19        0
    //  no simd       25       37        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddAtInfinity::from_groups(
            // scalar, e15, e25, e35
            ((swizzle!(self.group2(), 2, 3, 3, 3) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]]))
                + (swizzle!(self.group2(), 1, 2, 0, 1) * Simd32x4::from([other.group0()[2], other.group2()[1], other.group2()[2], other.group2()[0]]))
                + (swizzle!(other.group0(), 1, 0, 0, 0) * Simd32x4::from([self.group2()[0], self.group0()[1], self.group0()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group1()[3]) - (self.group1()[2] * other.group1()[2]) - (self.group1()[1] * other.group1()[1])
                        + (self.group0()[0] * other.group0()[0])
                        - (self.group1()[0] * other.group1()[0])),
                    ((self.group2()[1] * other.group2()[2]) * -1.0),
                    ((self.group2()[2] * other.group2()[0]) * -1.0),
                    ((self.group2()[0] * other.group2()[1]) * -1.0),
                ])),
            // e23, e31, e12, e45
            ((self.group1() * Simd32x4::from(other.group0()[0])) - (swizzle!(self.group2(), 0, 1, 2, 2) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group2()[1] * other.group1()[1]) - (self.group2()[0] * other.group1()[0]))])),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorOddAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       11        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       13        0
    //  no simd       12       19        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * swizzle!(other.group0(), 2, 0, 1, 3))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group0()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[1] * other.group0()[0]) - (self.group0()[2] * other.group0()[1]))])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                (-(self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorOddAtInfinity {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       18        0
    //    simd4        3        4        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       22       34        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group0()[1])
                        + (self.group0()[0] * other.group0()[3])
                        - (self.group0()[1] * other.group0()[0])),
                ])),
            // e23, e31, e12, e45
            ((self.group1() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group2()[3] * other.group0()[0]),
                    (self.group2()[3] * other.group0()[1]),
                    (self.group2()[3] * other.group0()[2]),
                    (-(self.group2()[2] * other.group1()[2]) - (self.group2()[1] * other.group1()[1]) - (self.group2()[0] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[1] * other.group0()[3]) + (self.group2()[3] * other.group1()[0])),
                ((self.group0()[2] * other.group0()[3]) + (self.group2()[3] * other.group1()[1])),
                ((self.group0()[3] * other.group0()[3]) + (self.group2()[3] * other.group1()[2])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            (self.group2() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorOddAtInfinity {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group2(), 1, 2, 0, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group2()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group2()[2], self.group2()[0], self.group2()[1], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group2()[2] * other.group2()[2]) + (self.group2()[1] * other.group2()[1]) + (self.group2()[0] * other.group2()[0])
                        - (self.group0()[3] * other.group0()[2])
                        - (self.group0()[1] * other.group0()[0])
                        - (self.group0()[2] * other.group0()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (-(self.group2()[0] * other.group0()[3]) + (self.group2()[3] * other.group0()[0])),
                (-(self.group2()[1] * other.group0()[3]) + (self.group2()[3] * other.group0()[1])),
                (-(self.group2()[2] * other.group0()[3]) + (self.group2()[3] * other.group0()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[2]) + (self.group2()[2] * other.group1()[1])),
                ((self.group2()[0] * other.group1()[2]) - (self.group2()[2] * other.group1()[0])),
                (-(self.group2()[0] * other.group1()[1]) + (self.group2()[1] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOdd> for VersorOddAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       34        0
    //    simd4        4        4        0
    // Totals...
    // yes simd       22       38        0
    //  no simd       34       50        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group2()[0] * other.group2()[3] * -1.0),
                (self.group2()[1] * other.group2()[3] * -1.0),
                (self.group2()[2] * other.group2()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group3()[2]) + (self.group2()[2] * other.group3()[1])),
                ((self.group2()[0] * other.group3()[2]) - (self.group2()[2] * other.group3()[0])),
                (-(self.group2()[0] * other.group3()[1]) + (self.group2()[1] * other.group3()[0])),
                (self.group2()[3] * other.group2()[3] * -1.0),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group3()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group2()[2]) - (self.group2()[1] * other.group2()[1]) - (self.group2()[0] * other.group2()[0])
                        + (self.group0()[3] * other.group3()[2])
                        + (self.group0()[1] * other.group3()[0])
                        + (self.group0()[2] * other.group3()[1])),
                ])),
            // e1, e2, e3, e4
            ((swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                - (swizzle!(self.group1(), 2, 0, 1, 3) * Simd32x4::from([other.group3()[1], other.group3()[2], other.group3()[0], other.group2()[3]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group2()[2] * other.group1()[1]) - (self.group0()[1] * other.group2()[3])
                        + (self.group1()[1] * other.group3()[2])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2]) + (self.group1()[2] * other.group3()[0])
                        - (self.group0()[2] * other.group2()[3])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group2()[1] * other.group1()[0]) - (self.group0()[3] * other.group2()[3])
                        + (self.group1()[0] * other.group3()[1])),
                    ((self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorOddAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       16       32        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       19       35        0
    //  no simd       28       44        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group2()[0] * other.group1()[3] * -1.0),
                (self.group2()[1] * other.group1()[3] * -1.0),
                (self.group2()[2] * other.group1()[3] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group2()[2]) + (self.group2()[2] * other.group2()[1])),
                ((self.group2()[0] * other.group2()[2]) - (self.group2()[2] * other.group2()[0])),
                (-(self.group2()[0] * other.group2()[1]) + (self.group2()[1] * other.group2()[0])),
                (self.group2()[3] * other.group1()[3] * -1.0),
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group1()[2]) - (self.group2()[1] * other.group1()[1]) - (self.group2()[0] * other.group1()[0])
                        + (self.group0()[3] * other.group2()[2])
                        + (self.group0()[1] * other.group2()[0])
                        + (self.group0()[2] * other.group2()[1])),
                ])),
            // e1, e2, e3, e4
            (-(swizzle!(self.group1(), 2, 0, 1, 3) * Simd32x4::from([other.group2()[1], other.group2()[2], other.group2()[0], other.group1()[3]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group0()[1] * other.group1()[3]) + (self.group1()[1] * other.group2()[2])),
                    (-(self.group2()[3] * other.group0()[1]) + (self.group1()[2] * other.group2()[0]) - (self.group0()[2] * other.group1()[3])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group0()[3] * other.group1()[3]) + (self.group1()[0] * other.group2()[1])),
                    ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorOddAtInfinity {
    type Output = VersorEvenAtInfinity;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       17       24        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       19       26        0
    //  no simd       25       32        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEvenAtInfinity::from_groups(
            // e12345, e1, e2, e3
            Simd32x4::from([
                0.0,
                (-(self.group2()[2] * other.group1()[1]) + (self.group2()[1] * other.group1()[2]) + (self.group1()[1] * other.group2()[2])
                    - (self.group1()[2] * other.group2()[1])),
                ((self.group2()[2] * other.group1()[0]) - (self.group2()[0] * other.group1()[2]) - (self.group1()[0] * other.group2()[2]) + (self.group1()[2] * other.group2()[0])),
                (-(self.group2()[1] * other.group1()[0]) + (self.group2()[0] * other.group1()[1]) + (self.group1()[0] * other.group2()[1])
                    - (self.group1()[1] * other.group2()[0])),
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group2()[2]) + (self.group2()[2] * other.group2()[1])),
                ((self.group2()[0] * other.group2()[2]) - (self.group2()[2] * other.group2()[0])),
                (-(self.group2()[0] * other.group2()[1]) + (self.group2()[1] * other.group2()[0])),
                0.0,
            ]),
            // e235, e315, e125, e5
            ((Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group1()[3]]))
                - (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[3]) - (self.group2()[1] * other.group0()[2]) - (self.group2()[0] * other.group0()[1])
                        + (self.group0()[3] * other.group2()[2])
                        + (self.group0()[1] * other.group2()[0])
                        + (self.group0()[2] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorOddAtInfinity {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        1        3        0
    // Totals...
    // yes simd        6       19        0
    //  no simd        9       28        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group2() * Simd32x4::from(other.group1()[3]) * Simd32x4::from(-1.0)),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group0()[3]),
                (self.group2()[1] * other.group0()[3]),
                (self.group2()[2] * other.group0()[3]),
                (-(self.group2()[2] * other.group1()[2]) - (self.group2()[1] * other.group1()[1]) + (self.group1()[3] * other.group0()[3])
                    - (self.group2()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[3], self.group1()[3]]))
                + Simd32x4::from([
                    ((self.group2()[3] * other.group0()[0]) * -1.0),
                    ((self.group2()[3] * other.group0()[1]) * -1.0),
                    ((self.group2()[3] * other.group0()[2]) * -1.0),
                    ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorOddAtInfinity {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       36        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       15       37        0
    //  no simd       18       40        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group2()[0] * other.group1()[0] * -1.0),
                (self.group2()[1] * other.group1()[0] * -1.0),
                (self.group2()[2] * other.group1()[0] * -1.0),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group2()[1] * other.group1()[3]) + (self.group2()[2] * other.group1()[2])),
                ((self.group2()[0] * other.group1()[3]) - (self.group2()[2] * other.group1()[1])),
                (-(self.group2()[0] * other.group1()[2]) + (self.group2()[1] * other.group1()[1])),
                (self.group2()[3] * other.group1()[0] * -1.0),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[3] * other.group1()[1] * -1.0),
                (self.group2()[3] * other.group1()[2] * -1.0),
                (self.group2()[3] * other.group1()[3] * -1.0),
                (-(self.group2()[3] * other.group0()[3])
                    + (self.group0()[3] * other.group1()[3])
                    + (self.group0()[1] * other.group1()[1])
                    + (self.group0()[2] * other.group1()[2])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group1(), 2, 0, 1, 0) * Simd32x4::from([self.group1()[2], self.group0()[2], self.group1()[1], self.group1()[3]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group0()[1] * other.group1()[0]) + (self.group1()[1] * other.group1()[3])),
                    (-(self.group2()[3] * other.group0()[1]) + (self.group1()[2] * other.group1()[1]) - (self.group1()[0] * other.group1()[3])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group1()[2])),
                    ((self.group2()[2] * other.group0()[2]) + (self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorOddAtInfinity {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       15        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       15       31        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (self.group2() * Simd32x4::from(other.group2()[3]) * Simd32x4::from(-1.0)),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group2()[0] * other.group1()[3]),
                (self.group2()[1] * other.group1()[3]),
                (self.group2()[2] * other.group1()[3]),
                (-(self.group2()[2] * other.group2()[2]) - (self.group2()[1] * other.group2()[1]) + (self.group1()[3] * other.group1()[3])
                    - (self.group2()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            (-(Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group0()[1], self.group0()[2], self.group0()[3], self.group1()[3]]))
                + (swizzle!(self.group2(), 1, 2, 0, 2) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[3] * other.group0()[0]) - (self.group2()[2] * other.group1()[1])),
                    (-(self.group2()[3] * other.group0()[1]) - (self.group2()[0] * other.group1()[2])),
                    (-(self.group2()[3] * other.group0()[2]) - (self.group2()[1] * other.group1()[0])),
                    ((self.group2()[1] * other.group0()[1]) + (self.group2()[0] * other.group0()[0])),
                ])),
        );
    }
}
impl InfixAntiWedge for VersorOddAtOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for VersorOddAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiDualNum> for VersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[3], self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            ((self.group1()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorOddAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group1()));
    }
}
impl AntiWedge<AntiMotor> for VersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group1()[3]),
            ((self.group0()[0] * other.group1()[3]) + (self.group1()[3] * other.group1()[0])),
            ((self.group0()[1] * other.group1()[3]) + (self.group1()[3] * other.group1()[1])),
            ((self.group0()[2] * other.group1()[3]) + (self.group1()[3] * other.group1()[2])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for VersorOddAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other.group0()[3]));
    }
}
impl AntiWedge<AntiScalar> for VersorOddAtOrigin {
    type Output = VersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorOddAtOrigin::from_groups(
            // e41, e42, e43, e3215
            (self.group0() * Simd32x4::from(other[e12345])),
            // e15, e25, e35, e1234
            (self.group1() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for VersorOddAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other.group0()[3]));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group1()[3] * -1.0),
            (-(self.group0()[3] * other.group0()[0]) - (self.group1()[0] * other.group1()[3])),
            (-(self.group0()[3] * other.group0()[1]) - (self.group1()[1] * other.group1()[3])),
            (-(self.group0()[3] * other.group0()[2]) - (self.group1()[2] * other.group1()[3])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group1()[0]) - (self.group1()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Circle> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[3] * other.group2()[0])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group2()[1])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[3] * other.group2()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[3] * other.group2()[0])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group2()[1])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[3] * other.group2()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       12        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            ((self.group0()[3] * other.group0()[0]) + (self.group1()[3] * other.group1()[0])),
            ((self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group1()[1])),
            ((self.group0()[3] * other.group0()[2]) + (self.group1()[3] * other.group1()[2])),
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleOnOrigin> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       12        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0]),
                (self.group0()[3] * other.group1()[1]),
                (self.group0()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       12        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            ((self.group0()[3] * other.group0()[0]) + (self.group1()[3] * other.group1()[0])),
            ((self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group1()[1])),
            ((self.group0()[3] * other.group0()[2]) + (self.group1()[3] * other.group1()[2])),
            (-(self.group1()[2] * other.group0()[2])
                - (self.group1()[1] * other.group0()[1])
                - (self.group1()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<Dipole> for VersorOddAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[3] * other.group0()[0]) + (self.group1()[3] * other.group2()[0])),
                (-(self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group2()[1])),
                (-(self.group0()[3] * other.group0()[2]) + (self.group1()[3] * other.group2()[2])),
                (self.group1()[3] * other.group1()[3]),
            ]),
            // e5
            (self.group0()[3] * other.group1()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorOddAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group0()[3] * other.group0()[0]) + (self.group1()[3] * other.group1()[0])),
                (-(self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group1()[1])),
                (-(self.group0()[3] * other.group0()[2]) + (self.group1()[3] * other.group1()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e5
            (self.group0()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorOddAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        6        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
            // e5
            (self.group0()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorOddAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(Simd32x3::from(self.group0()[3]) * other.group0()) + (Simd32x3::from(self.group1()[3]) * other.group1())),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorOddAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        4        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (other.group0() * Simd32x4::from([self.group0()[3], self.group0()[3], self.group0()[3], self.group1()[3]]) * Simd32x4::from([-1.0, -1.0, -1.0, 1.0])),
            // e5
            (self.group0()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorOddAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(Simd32x3::from(self.group0()[3]) * other.group0()) + (Simd32x3::from(self.group1()[3]) * other.group2())),
        );
    }
}
impl AntiWedge<DualNum> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        5        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        6        0
    //  no simd        0        9        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                (self.group1()[3] * other.group0()[0]),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e15, e25, e35, e1234
            (self.group1() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorOddAtOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (Simd32x2::from(other[e45]) * Simd32x2::from([self.group1()[3], self.group0()[3]]) * Simd32x2::from([1.0, -1.0])),
        );
    }
}
impl AntiWedge<FlatPoint> for VersorOddAtOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        6        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group1()[3]) * other.group0()),
            // e5
            (self.group0()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorOddAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group0()));
    }
}
impl AntiWedge<Flector> for VersorOddAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       18        0
    //  no simd        9       24        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group1()[3]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[0] * -1.0),
                (self.group0()[3] * other.group1()[1] * -1.0),
                (self.group0()[3] * other.group1()[2] * -1.0),
                ((self.group1()[2] * other.group1()[2]) + (self.group1()[1] * other.group1()[1]) - (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[3]) * other.group0())
                + Simd32x4::from([
                    (self.group0()[0] * other.group1()[3]),
                    (self.group0()[1] * other.group1()[3]),
                    (self.group0()[2] * other.group1()[3]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group0()[3]),
            ((self.group0()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorOddAtOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       17        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            Simd32x4::from([
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (self.group1()[3] * other.group0()[3]),
                ((self.group1()[3] * other.group0()[0]) - (self.group0()[2] * other.group0()[3]) - (self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                (self.group0()[3] * other.group0()[3] * -1.0),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[1] * other.group0()[2]) - (self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<Horizon> for VersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group1()[3], self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<Infinity> for VersorOddAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2       12        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0]),
                (self.group0()[3] * other.group0()[1]),
                (self.group0()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for VersorOddAtOrigin {
    type Output = DipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return DipoleAtOrigin::from_groups(
            // e41, e42, e43
            (Simd32x3::from(self.group1()[3]) * other.group0()),
            // e15, e25, e35
            (Simd32x3::from(self.group0()[3]) * other.group0()),
        );
    }
}
impl AntiWedge<Motor> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       18        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[3]),
                    (self.group0()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            ((self.group1()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for VersorOddAtOrigin {
    type Output = VersorOddAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOddAtOrigin::from_groups(
            // e41, e42, e43, e3215
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                ((self.group0()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                ((self.group0()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for VersorOddAtOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       22        0
    //    simd3        2        7        0
    //    simd4        3        5        0
    // Totals...
    // yes simd       19       34        0
    //  no simd       32       63        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other[e1]) - (self.group1()[2] * other.group7()[2]) - (self.group1()[1] * other.group7()[1]) - (self.group1()[0] * other.group7()[0])
                    + (self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group4()[0], other.group4()[1], other.group4()[2], other.group3()[3]]))
                - (swizzle!(other.group9(), 0, 0, 0, 3) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[2]]))
                - (swizzle!(self.group0(), 3, 3, 3, 0) * Simd32x4::from([other.group3()[0], other.group3()[1], other.group3()[2], other.group9()[1]]))
                + (Simd32x4::from([1.0, 1.0, 1.0, -1.0]) * swizzle!(self.group0(), 0, 1, 2, 1) * Simd32x4::from([other[e45], other[e45], other[e45], other.group9()[2]]))),
            // e5
            ((self.group1()[2] * other.group9()[3]) + (self.group1()[1] * other.group9()[2]) - (self.group0()[3] * other.group3()[3]) + (self.group1()[0] * other.group9()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[1]) + (self.group1()[3] * other.group6()[0])),
                ((self.group0()[1] * other.group0()[1]) + (self.group1()[3] * other.group6()[1])),
                ((self.group0()[2] * other.group0()[1]) + (self.group1()[3] * other.group6()[2])),
                0.0,
            ]),
            // e15, e25, e35
            ((Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))
                + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group0()[3]) * other.group7()) + (Simd32x3::from(self.group1()[3]) * other.group8())),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[3] * other.group9()[0]) + (self.group1()[3] * other[e45]))]),
            // e423, e431, e412
            (Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]])),
            // e235, e315, e125
            (Simd32x3::from(self.group0()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]) * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group1()[3] * other.group0()[1]), 0.0, 0.0, 0.0]),
            // e3215
            (self.group0()[3] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorOddAtOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group0()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group0()[3], self.group1()[0], self.group1()[1], self.group1()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group0()[3] * other.group0()[0]),
            (self.group0()[3] * other.group0()[1]),
            (self.group0()[3] * other.group0()[2]),
            (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) + (self.group0()[3] * other.group0()[3]) - (self.group1()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group0()[3] * other.group0()[3] * -1.0),
            (-(self.group0()[3] * other.group0()[0]) - (self.group1()[0] * other.group0()[3])),
            (-(self.group0()[3] * other.group0()[1]) - (self.group1()[1] * other.group0()[3])),
            (-(self.group0()[3] * other.group0()[2]) - (self.group1()[2] * other.group0()[3])),
        ]));
    }
}
impl AntiWedge<Origin> for VersorOddAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group0()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for VersorOddAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       15        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        4       16        0
    //  no simd        4       19        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group1()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorOddAtOrigin {
    type Output = VersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4       15        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for VersorOddAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ ((self.group0()[3] * other.group0()[3]) + (self.group1()[3] * other[e2])));
    }
}
impl AntiWedge<RoundPointAtOrigin> for VersorOddAtOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ ((self.group0()[3] * other.group0()[0]) + (self.group1()[3] * other.group0()[1])));
    }
}
impl AntiWedge<Sphere> for VersorOddAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       23        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                (-(self.group0()[3] * other[e4315]) + (self.group1()[3] * other.group0()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[3]) - (self.group1()[0] * other[e4315])),
                ((self.group0()[1] * other.group0()[3]) - (self.group1()[1] * other[e4315])),
                ((self.group0()[2] * other.group0()[3]) - (self.group1()[2] * other[e4315])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        1        2        0
    // no simd        4        8        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (-(Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group0()[3], self.group1()[0], self.group1()[1], self.group1()[2]]))
                + (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[3], self.group0()[0], self.group0()[1], self.group0()[2]]))),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorOddAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       18        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        4       20        0
    //  no simd        4       26        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (other.group0() * Simd32x4::from([self.group1()[3], self.group1()[3], self.group1()[3], self.group0()[3]]) * Simd32x4::from([1.0, 1.0, 1.0, -1.0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group0()[0] * -1.0),
                (self.group0()[3] * other.group0()[1] * -1.0),
                (self.group0()[3] * other.group0()[2] * -1.0),
                ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[0] * other.group0()[3] * -1.0),
                (self.group1()[1] * other.group0()[3] * -1.0),
                (self.group1()[2] * other.group0()[3] * -1.0),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<VersorEven> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group3()[3]]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[3] * other.group2()[0])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group2()[1])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[3] * other.group2()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group1()[3]]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group0()[3] * other.group0()[0]) + (self.group1()[3] * other.group2()[0])),
                ((self.group0()[3] * other.group0()[1]) + (self.group1()[3] * other.group2()[1])),
                ((self.group0()[3] * other.group0()[2]) + (self.group1()[3] * other.group2()[2])),
                (self.group0()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       17        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       18        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[0]),
                    (self.group0()[1] * other.group0()[0]),
                    (self.group0()[2] * other.group0()[0]),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group2()[0]),
                (self.group1()[3] * other.group2()[1]),
                (self.group1()[3] * other.group2()[2]),
                (self.group0()[3] * other.group0()[0]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[0])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[0])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[0])),
                (self.group1()[3] * other.group0()[0]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        7        8        0
    //  no simd       13       14        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(self.group0()[3]) * other.group0())
                + (Simd32x4::from(self.group1()[3]) * other.group1())
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorOddAtOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from([other.group0()[3], other.group0()[3], other.group0()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group1()[3] * other.group1()[0]),
                    (self.group1()[3] * other.group1()[1]),
                    (self.group1()[3] * other.group1()[2]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e3215
            (Simd32x4::from(self.group0()[3]) * other.group0()),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group0()[3] * other.group1()[0]) + (self.group1()[0] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[1]) + (self.group1()[1] * other.group0()[3])),
                ((self.group0()[3] * other.group1()[2]) + (self.group1()[2] * other.group0()[3])),
                (self.group1()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorOddAtOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        7        8        0
    //  no simd       13       14        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(self.group0()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group2()[3]]))
                + (Simd32x4::from(self.group1()[3]) * other.group1())
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOdd> for VersorOddAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       23        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       25        0
    //  no simd       16       31        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group1()[3] * other.group3()[0]),
                (self.group1()[3] * other.group3()[1]),
                (self.group1()[3] * other.group3()[2]),
                (-(self.group0()[3] * other.group2()[3]) + (self.group1()[3] * other.group3()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group3()[0] * -1.0),
                (self.group0()[3] * other.group3()[1] * -1.0),
                (self.group0()[3] * other.group3()[2] * -1.0),
                ((self.group1()[2] * other.group3()[2]) + (self.group1()[1] * other.group3()[1]) - (self.group0()[3] * other.group1()[3]) + (self.group1()[0] * other.group3()[0])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                - (swizzle!(self.group0(), 3, 3, 3, 2) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group3()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group2()[3]) + (self.group0()[0] * other.group3()[3])),
                    (-(self.group1()[1] * other.group2()[3]) + (self.group0()[1] * other.group3()[3])),
                    (-(self.group1()[2] * other.group2()[3]) + (self.group0()[2] * other.group3()[3])),
                    (-(self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorOddAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       23        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       25        0
    //  no simd       16       31        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group1()[3] * other.group2()[0]),
                (self.group1()[3] * other.group2()[1]),
                (self.group1()[3] * other.group2()[2]),
                (-(self.group0()[3] * other.group1()[3]) + (self.group1()[3] * other.group2()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0] * -1.0),
                (self.group0()[3] * other.group2()[1] * -1.0),
                (self.group0()[3] * other.group2()[2] * -1.0),
                ((self.group1()[2] * other.group2()[2]) + (self.group1()[1] * other.group2()[1]) - (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                - (swizzle!(self.group0(), 3, 3, 3, 2) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group2()[2]]))
                + Simd32x4::from([
                    (-(self.group1()[0] * other.group1()[3]) + (self.group0()[0] * other.group2()[3])),
                    (-(self.group1()[1] * other.group1()[3]) + (self.group0()[1] * other.group2()[3])),
                    (-(self.group1()[2] * other.group1()[3]) + (self.group0()[2] * other.group2()[3])),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorOddAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       18        0
    //  no simd        9       24        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group1()[3]) * other.group2()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group2()[0] * -1.0),
                (self.group0()[3] * other.group2()[1] * -1.0),
                (self.group0()[3] * other.group2()[2] * -1.0),
                ((self.group1()[2] * other.group2()[2]) + (self.group1()[1] * other.group2()[1]) - (self.group0()[3] * other.group1()[3]) + (self.group1()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[3], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group2()[3]),
                    (self.group0()[1] * other.group2()[3]),
                    (self.group0()[2] * other.group2()[3]),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        5        8        0
    //  no simd       11       14        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (-(Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group0()[3], self.group1()[0], self.group1()[1], self.group1()[2]]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]]))
                + Simd32x4::from([
                    0.0,
                    ((self.group0()[0] * other.group0()[3]) - (self.group0()[3] * other.group0()[0])),
                    ((self.group0()[1] * other.group0()[3]) - (self.group0()[3] * other.group0()[1])),
                    ((self.group0()[2] * other.group0()[3]) - (self.group0()[3] * other.group0()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorOddAtOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       12        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        6       16        0
    //  no simd       12       28        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (swizzle!(other.group1(), 1, 2, 3, 0)
                * Simd32x4::from([self.group1()[3], self.group1()[3], self.group1()[3], self.group0()[3]])
                * Simd32x4::from([1.0, 1.0, 1.0, -1.0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group0()[3] * other.group1()[1] * -1.0),
                (self.group0()[3] * other.group1()[2] * -1.0),
                (self.group0()[3] * other.group1()[3] * -1.0),
                ((self.group1()[2] * other.group1()[3]) + (self.group1()[1] * other.group1()[2]) - (self.group0()[3] * other.group0()[3]) + (self.group1()[0] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(self.group0(), 3, 3, 3, 2) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                - (swizzle!(other.group1(), 0, 0, 0, 1) * Simd32x4::from([self.group1()[0], self.group1()[1], self.group1()[2], self.group0()[0]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group1()[3] * other.group0()[3]) - (self.group0()[1] * other.group1()[2]))])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorOddAtOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        5        8        0
    //  no simd       11       14        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (-(Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group0()[3], self.group1()[0], self.group1()[1], self.group1()[2]]))
                + (Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group1()[3], other.group2()[0], other.group2()[1], other.group2()[2]]))
                + Simd32x4::from([
                    0.0,
                    ((self.group0()[0] * other.group1()[3]) - (self.group0()[3] * other.group0()[0])),
                    ((self.group0()[1] * other.group1()[3]) - (self.group0()[3] * other.group0()[1])),
                    ((self.group0()[2] * other.group1()[3]) - (self.group0()[3] * other.group0()[2])),
                ])),
        );
    }
}
impl InfixAntiWedge for VersorOddOnOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for VersorOddOnOrigin {
    type Output = AntiSphereOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        2        9        0
    //  no simd        5       12        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiSphereOnOrigin::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group1()[1]) * -1.0),
                    ((self.group1()[1] * other.group1()[2]) * -1.0),
                    ((self.group1()[2] * other.group1()[0]) * -1.0),
                    ((self.group1()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       14        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[2] * other.group0()[2]) - (self.group1()[3] * other.group0()[1])),
                (-(self.group1()[1] * other.group0()[2]) + (self.group1()[3] * other.group0()[0])),
                ((self.group1()[1] * other.group0()[1]) - (self.group1()[2] * other.group0()[0])),
                (self.group0()[3] * other.group0()[3] * -1.0),
            ]),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[1] * other.group0()[3] * -1.0),
                (self.group1()[2] * other.group0()[3] * -1.0),
                (self.group1()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiDualNum> for VersorOddOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (swizzle!(self.group1(), 1, 2, 3, 0) * Simd32x4::from(other.group0()[0])),
            // e1, e2, e3, e5
            (self.group0() * Simd32x4::from(other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiFlatOrigin> for VersorOddOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiFlatOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            (Simd32x4::from(other[e321]) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[3], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorOddOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            (-(Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[3], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (self.group1()[0] * other.group0()[2]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[1]) + (self.group1()[2] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlector> for VersorOddOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            ((swizzle!(self.group1(), 0, 0, 0, 3) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[2]]))
                - (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[3], self.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group1()[1]) + (self.group1()[1] * other.group1()[0]) + (self.group1()[0] * other.group1()[3])
                        - (self.group0()[2] * other.group0()[2])
                        - (self.group0()[0] * other.group0()[0])
                        - (self.group0()[1] * other.group0()[1])),
                ])),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[1]) + (self.group1()[2] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<AntiFlectorOnOrigin> for VersorOddOnOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: AntiFlectorOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[1] * other.group0()[0] * -1.0),
            (self.group1()[2] * other.group0()[0] * -1.0),
            (self.group1()[3] * other.group0()[0] * -1.0),
            ((self.group1()[3] * other.group0()[3]) + (self.group1()[2] * other.group0()[2]) - (self.group0()[3] * other.group0()[0]) + (self.group1()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorOddOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlane::from_groups(
            // e1, e2, e3, e5
            (-(swizzle!(self.group1(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group1()[0]) + (self.group1()[2] * other.group0()[2])),
                    ((self.group1()[3] * other.group0()[0]) + (self.group1()[0] * other.group1()[1])),
                    ((self.group1()[0] * other.group1()[2]) + (self.group1()[1] * other.group0()[1])),
                    (-(self.group1()[1] * other.group1()[0]) - (self.group1()[2] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<AntiLineOnOrigin> for VersorOddOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: AntiLineOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group1()[2], self.group1()[3], self.group1()[1]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group1()[3], self.group1()[1], self.group1()[2]]))),
        );
    }
}
impl AntiWedge<AntiMotor> for VersorOddOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (swizzle!(self.group1(), 1, 2, 3, 0) * Simd32x4::from(other.group1()[3])),
            // e1, e2, e3, e5
            (-(swizzle!(self.group1(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + (self.group0() * Simd32x4::from(other.group1()[3]))
                + Simd32x4::from([
                    ((self.group1()[2] * other.group0()[2]) + (self.group1()[0] * other.group1()[0])),
                    ((self.group1()[3] * other.group0()[0]) + (self.group1()[0] * other.group1()[1])),
                    ((self.group1()[1] * other.group0()[1]) + (self.group1()[0] * other.group1()[2])),
                    (-(self.group1()[2] * other.group1()[1]) - (self.group1()[1] * other.group1()[0])),
                ])),
        );
    }
}
impl AntiWedge<AntiMotorOnOrigin> for VersorOddOnOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: AntiMotorOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ Simd32x3::from([
            ((self.group1()[2] * other.group0()[2]) - (self.group1()[3] * other.group0()[1])),
            (-(self.group1()[1] * other.group0()[2]) + (self.group1()[3] * other.group0()[0])),
            ((self.group1()[1] * other.group0()[1]) - (self.group1()[2] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for VersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group1()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[1]) + (self.group1()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[0])),
        );
    }
}
impl AntiWedge<AntiPlaneOnOrigin> for VersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiPlaneOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group1()[3] * other.group0()[2]) + (self.group1()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiScalar> for VersorOddOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            (self.group0() * Simd32x4::from(other[e12345])),
            // e1234, e4235, e4315, e4125
            (self.group1() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for VersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(
            // scalar
            ((self.group1()[3] * other.group0()[2]) + (self.group1()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorOddOnOrigin {
    type Output = AntiVersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiVersorOddOnOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group1()[1] * other.group1()[3] * -1.0),
                (self.group1()[2] * other.group1()[3] * -1.0),
                (self.group1()[3] * other.group1()[3] * -1.0),
                0.0,
            ]),
            // e4, e1, e2, e3
            ((swizzle!(self.group1(), 3, 2, 3, 1) * Simd32x4::from([other.group0()[2], other.group1()[2], other.group1()[0], other.group1()[1]]))
                - (swizzle!(other.group1(), 3, 1, 2, 0) * Simd32x4::from([self.group0()[3], self.group1()[3], self.group1()[1], self.group1()[2]]))
                + Simd32x4::from([((self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[0])), 0.0, 0.0, 0.0])),
        );
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorOddOnOrigin {
    type Output = AntiVersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        8        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3       10        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiVersorEvenOnOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 2, 3, 1, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                - (swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group1()[3], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([0.0, 0.0, 0.0, ((self.group1()[2] * other.group1()[2]) + (self.group1()[1] * other.group1()[1]))])),
            // e23, e31, e12, e1234
            Simd32x4::from([
                (self.group1()[1] * other.group0()[3] * -1.0),
                (self.group1()[2] * other.group0()[3] * -1.0),
                (self.group1()[3] * other.group0()[3] * -1.0),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Circle> for VersorOddOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       24        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       14       25        0
    //  no simd       17       28        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group1()[3] * other.group0()[1]) + (self.group1()[0] * other.group1()[0]) + (self.group1()[2] * other.group0()[2])),
                ((self.group1()[3] * other.group0()[0]) + (self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group0()[2])),
                (-(self.group1()[2] * other.group0()[0]) + (self.group1()[0] * other.group1()[2]) + (self.group1()[1] * other.group0()[1])),
                (-(self.group0()[3] * other.group1()[3])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 1, 2, 3, 3) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group1()[0] * other.group2()[0]),
                    (self.group1()[0] * other.group2()[1]),
                    (self.group1()[0] * other.group2()[2]),
                    (-(self.group1()[1] * other.group1()[0]) - (self.group1()[2] * other.group1()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group2()[2]) + (self.group1()[3] * other.group2()[1])),
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[3] * other.group2()[0])),
                (-(self.group1()[1] * other.group2()[1]) + (self.group1()[2] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorOddOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       13       24        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (-(self.group1()[3] * other.group0()[1]) + (self.group1()[0] * other.group1()[0]) + (self.group1()[2] * other.group0()[2])),
                ((self.group1()[3] * other.group0()[0]) + (self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group0()[2])),
                (-(self.group1()[2] * other.group0()[0]) + (self.group1()[0] * other.group1()[2]) + (self.group1()[1] * other.group0()[1])),
                (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group2()[0]),
                (self.group1()[0] * other.group2()[1]),
                (self.group1()[0] * other.group2()[2]),
                (-(self.group1()[3] * other.group1()[2]) - (self.group1()[1] * other.group1()[0]) - (self.group1()[2] * other.group1()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group2()[2]) + (self.group1()[3] * other.group2()[1])),
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[3] * other.group2()[0])),
                (-(self.group1()[1] * other.group2()[1]) + (self.group1()[2] * other.group2()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorOddOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        8       19        0
    //  no simd       11       22        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[0] * other.group0()[2]),
                (-(self.group0()[3] * other.group0()[3])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 1, 2, 3, 3) * swizzle!(other.group0(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group1()[0] * other.group1()[0]),
                    (self.group1()[0] * other.group1()[1]),
                    (self.group1()[0] * other.group1()[2]),
                    (-(self.group1()[1] * other.group0()[0]) - (self.group1()[2] * other.group0()[1])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group1()[2]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[3] * other.group1()[0])),
                (-(self.group1()[1] * other.group1()[1]) + (self.group1()[2] * other.group1()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorOddOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       18        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                ((self.group1()[2] * other.group0()[2]) - (self.group1()[3] * other.group0()[1])),
                (-(self.group1()[1] * other.group0()[2]) + (self.group1()[3] * other.group0()[0])),
                ((self.group1()[1] * other.group0()[1]) - (self.group1()[2] * other.group0()[0])),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[0] * other.group1()[1]),
                (self.group1()[0] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group1()[2]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[3] * other.group1()[0])),
                (-(self.group1()[1] * other.group1()[1]) + (self.group1()[2] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleOnOrigin> for VersorOddOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        5        9        0
    //  no simd        8       12        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(self.group1(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + Simd32x4::from([
                    ((self.group1()[0] * other.group1()[0]) + (self.group1()[2] * other.group0()[2])),
                    ((self.group1()[3] * other.group0()[0]) + (self.group1()[0] * other.group1()[1])),
                    ((self.group1()[0] * other.group1()[2]) + (self.group1()[1] * other.group0()[1])),
                    (-(self.group1()[1] * other.group1()[0]) - (self.group1()[2] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorOddOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       12       22        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (-(swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group1()[3], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group1()[2] * other.group0()[2]),
                    (self.group1()[3] * other.group0()[0]),
                    (self.group1()[1] * other.group0()[1]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group0()[3])),
                ((self.group1()[0] * other.group1()[1]) - (self.group1()[2] * other.group0()[3])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[3] * other.group0()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group1()[2]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[3] * other.group1()[0])),
                (-(self.group1()[1] * other.group1()[1]) + (self.group1()[2] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Dipole> for VersorOddOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3       11        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        5       13        0
    //  no simd       11       19        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 0, 3, 0, 3) * Simd32x4::from([other.group2()[0], other.group1()[0], other.group2()[2], other.group0()[2]]))
                + (swizzle!(self.group1(), 2, 0, 1, 2) * Simd32x4::from([other.group1()[2], other.group2()[1], other.group1()[1], other.group0()[1]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group1()[1]) * -1.0),
                    ((self.group1()[1] * other.group1()[2]) * -1.0),
                    ((self.group1()[2] * other.group1()[0]) * -1.0),
                    ((self.group1()[0] * other.group1()[3]) + (self.group1()[1] * other.group0()[0])),
                ])),
            // e5
            (-(self.group1()[3] * other.group2()[2]) - (self.group1()[1] * other.group2()[0]) - (self.group1()[2] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorOddOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       10        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[0] * other.group1()[1]),
                (self.group1()[0] * other.group1()[2]),
                ((self.group1()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[1]) + (self.group1()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[0])),
            ]),
            // e5
            (-(self.group1()[3] * other.group1()[2]) - (self.group1()[1] * other.group1()[0]) - (self.group1()[2] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorOddOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       13        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[3] * other.group0()[1]) + (self.group1()[0] * other.group1()[0]) + (self.group1()[2] * other.group0()[2])),
                ((self.group1()[3] * other.group0()[0]) + (self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group0()[2])),
                (-(self.group1()[2] * other.group0()[0]) + (self.group1()[0] * other.group1()[2]) + (self.group1()[1] * other.group0()[1])),
                (self.group1()[0] * other.group0()[3]),
            ]),
            // e5
            (-(self.group1()[3] * other.group1()[2]) - (self.group1()[1] * other.group1()[0]) - (self.group1()[2] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorOddOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        4        9        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[0] * other.group1()[1]),
                (self.group1()[0] * other.group1()[2]),
                ((self.group1()[3] * other.group0()[2]) + (self.group1()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[1])),
            ]),
            // e5
            (-(self.group1()[3] * other.group1()[2]) - (self.group1()[1] * other.group1()[0]) - (self.group1()[2] * other.group1()[1])),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorOddOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            ((self.group1()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[1]) + (self.group1()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[0])),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorOddOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        3        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        4        7        0
    //  no simd       10       19        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 0, 3, 0, 3) * Simd32x4::from([other.group2()[0], other.group1()[0], other.group2()[2], other.group0()[2]]))
                + (swizzle!(self.group1(), 2, 0, 1, 1) * Simd32x4::from([other.group1()[2], other.group2()[1], other.group1()[1], other.group0()[0]]))
                + (Simd32x4::from([-1.0, -1.0, -1.0, 1.0])
                    * swizzle!(self.group1(), 3, 1, 2, 2)
                    * Simd32x4::from([other.group1()[1], other.group1()[2], other.group1()[0], other.group0()[1]]))),
            // e5
            (-(self.group1()[3] * other.group2()[2]) - (self.group1()[1] * other.group2()[0]) - (self.group1()[2] * other.group2()[1])),
        );
    }
}
impl AntiWedge<DualNum> for VersorOddOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        9        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                (self.group1()[0] * other.group0()[0]),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([0.0, 0.0, 0.0, (self.group0()[3] * other.group0()[1])]),
            // e15, e25, e35, e1234
            Simd32x4::from([0.0, 0.0, 0.0, (self.group1()[0] * other.group0()[1])]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group1()[1] * other.group0()[1]),
                (self.group1()[2] * other.group0()[1]),
                (self.group1()[3] * other.group0()[1]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorOddOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return Origin::from_groups(/* e4 */ (self.group1()[0] * other[e45]));
    }
}
impl AntiWedge<FlatPoint> for VersorOddOnOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        3        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        2        4        0
    //  no simd        2        7        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group1()[0]) * other.group0()),
            // e5
            (-(self.group1()[3] * other.group0()[2]) - (self.group1()[1] * other.group0()[0]) - (self.group1()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorOddOnOrigin {
    type Output = AntiPlane;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiPlane::from_groups(/* e1, e2, e3, e5 */ Simd32x4::from([
            (self.group1()[0] * other.group0()[0]),
            (self.group1()[0] * other.group0()[1]),
            (self.group1()[0] * other.group0()[2]),
            (-(self.group1()[3] * other.group0()[2]) - (self.group1()[1] * other.group0()[0]) - (self.group1()[2] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Flector> for VersorOddOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       23        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       24        0
    //  no simd       12       27        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[0] * other.group1()[1]),
                (self.group1()[0] * other.group1()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[2] * other.group1()[2]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[3] * other.group1()[0])),
                (-(self.group1()[1] * other.group1()[1]) + (self.group1()[2] * other.group1()[0])),
                (self.group1()[0] * other.group1()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (self.group1()[3] * other.group1()[3]),
                (-(self.group1()[3] * other.group0()[2]) - (self.group1()[2] * other.group0()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group1()[1] * other.group0()[0])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group1()[0]) * other.group0())
                + Simd32x4::from([
                    (self.group0()[0] * other.group1()[3]),
                    (self.group0()[1] * other.group1()[3]),
                    (self.group0()[2] * other.group1()[3]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorOddOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (swizzle!(self.group1(), 1, 2, 3, 0) * Simd32x4::from(other.group0()[3])),
            // e1, e2, e3, e5
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (self.group1()[0] * other.group0()[2]),
                    (-(self.group1()[3] * other.group0()[2]) - (self.group1()[2] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                ])),
        );
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorOddOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        3        7        0
    //  no simd        9       13        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[0] * other.group0()[2]),
                (self.group1()[0] * other.group0()[3]),
                0.0,
            ]),
            // e415, e425, e435, e4
            (-(swizzle!(other.group0(), 3, 1, 2, 3) * Simd32x4::from([self.group1()[2], self.group1()[3], self.group1()[1], self.group0()[2]]))
                + (swizzle!(self.group1(), 3, 1, 2, 0) * swizzle!(other.group0(), 2, 3, 1, 0))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2]))])),
        );
    }
}
impl AntiWedge<Horizon> for VersorOddOnOrigin {
    type Output = AntiFlector;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlector::from_groups(
            // e235, e315, e125, e321
            (swizzle!(self.group1(), 1, 2, 3, 0) * Simd32x4::from(other[e3215])),
            // e1, e2, e3, e5
            (self.group0() * Simd32x4::from(other[e3215])),
        );
    }
}
impl AntiWedge<Infinity> for VersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group1()[0] * other[e5]));
    }
}
impl AntiWedge<Line> for VersorOddOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       18        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[0] * other.group0()[2]),
                (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[0] * other.group1()[1]),
                (self.group1()[0] * other.group1()[2]),
                (-(self.group1()[3] * other.group0()[2]) - (self.group1()[1] * other.group0()[0]) - (self.group1()[2] * other.group0()[1])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group1()[2]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[3] * other.group1()[0])),
                (-(self.group1()[1] * other.group1()[1]) + (self.group1()[2] * other.group1()[0])),
                0.0,
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from(0.0),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorOddOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[0] * other.group0()[2]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[1]) + (self.group1()[2] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<LineOnOrigin> for VersorOddOnOrigin {
    type Output = DipoleOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return DipoleOnOrigin::from_groups(/* e41, e42, e43, e45 */ Simd32x4::from([
            (self.group1()[0] * other.group0()[0]),
            (self.group1()[0] * other.group0()[1]),
            (self.group1()[0] * other.group0()[2]),
            (-(self.group1()[3] * other.group0()[2]) - (self.group1()[1] * other.group0()[0]) - (self.group1()[2] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<Motor> for VersorOddOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       23        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        9       24        0
    //  no simd       12       27        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((Simd32x4::from(self.group1()[0]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group0()[3]),
                    (self.group0()[1] * other.group0()[3]),
                    (self.group0()[2] * other.group0()[3]),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[0] * other.group1()[1]),
                (self.group1()[0] * other.group1()[2]),
                (-(self.group1()[3] * other.group0()[2]) - (self.group1()[2] * other.group0()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group1()[1] * other.group0()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group1()[2]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[3] * other.group1()[0])),
                (-(self.group1()[1] * other.group1()[1]) + (self.group1()[2] * other.group1()[0])),
                (self.group1()[0] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (self.group1()[3] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorOddOnOrigin {
    type Output = AntiMotor;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        6       13        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiMotor::from_groups(
            // e23, e31, e12, scalar
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[0] * other.group0()[2]),
                ((self.group1()[0] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
            // e15, e25, e35, e3215
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[1]) + (self.group1()[2] * other.group0()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<MotorOnOrigin> for VersorOddOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        6        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        3        8        0
    //  no simd        6       14        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group1()[0] * other.group0()[0]),
                    (self.group1()[0] * other.group0()[1]),
                    (self.group1()[0] * other.group0()[2]),
                    (-(self.group1()[3] * other.group0()[2]) - (self.group1()[2] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                ])),
            // e1234, e4235, e4315, e4125
            (self.group1() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<MultiVector> for VersorOddOnOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       25       39        0
    //    simd3        3        7        0
    //    simd4        4        5        0
    // Totals...
    // yes simd       32       51        0
    //  no simd       50       80        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group1()[3] * other.group1()[2]) + (self.group1()[2] * other.group1()[1]) + (self.group1()[1] * other.group1()[0]) + (self.group1()[0] * other[e1])
                    - (self.group0()[3] * other.group6()[3])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 2, 3, 1, 3) * Simd32x4::from([other.group5()[2], other.group5()[0], other.group5()[1], other.group3()[2]]))
                + (swizzle!(self.group1(), 0, 0, 0, 2) * Simd32x4::from([other.group4()[0], other.group4()[1], other.group4()[2], other.group3()[1]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group5()[1]) + (self.group0()[0] * other[e45])),
                    (-(self.group1()[1] * other.group5()[2]) + (self.group0()[1] * other[e45])),
                    (-(self.group1()[2] * other.group5()[0]) + (self.group0()[2] * other[e45])),
                    ((self.group1()[1] * other.group3()[0]) + (self.group1()[0] * other.group3()[3])
                        - (self.group0()[3] * other.group9()[0])
                        - (self.group0()[2] * other.group9()[3])
                        - (self.group0()[0] * other.group9()[1])
                        - (self.group0()[1] * other.group9()[2])),
                ])),
            // e5
            (-(self.group1()[3] * other.group4()[2]) - (self.group1()[2] * other.group4()[1]) + (self.group0()[3] * other[e45]) - (self.group1()[1] * other.group4()[0])),
            // e41, e42, e43, e45
            (-(swizzle!(self.group1(), 3, 1, 2, 3) * Simd32x4::from([other.group7()[1], other.group7()[2], other.group7()[0], other.group6()[2]]))
                + (self.group0() * Simd32x4::from(other.group0()[1]))
                + Simd32x4::from([
                    ((self.group1()[2] * other.group7()[2]) + (self.group1()[0] * other.group6()[0])),
                    ((self.group1()[3] * other.group7()[0]) + (self.group1()[0] * other.group6()[1])),
                    ((self.group1()[1] * other.group7()[1]) + (self.group1()[0] * other.group6()[2])),
                    (-(self.group1()[2] * other.group6()[1]) - (self.group1()[1] * other.group6()[0])),
                ])),
            // e15, e25, e35
            (-(swizzle!(other.group8(), 2, 0, 1) * Simd32x3::from([self.group1()[2], self.group1()[3], self.group1()[1]]))
                + (swizzle!(other.group8(), 1, 2, 0) * Simd32x3::from([self.group1()[3], self.group1()[1], self.group1()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group1()[0]) * other.group8()) - (Simd32x3::from(other.group6()[3]) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[3]]))),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[2] * other.group9()[3]) + (self.group1()[3] * other.group9()[2])),
                ((self.group1()[1] * other.group9()[3]) - (self.group1()[3] * other.group9()[1])),
                (-(self.group1()[1] * other.group9()[2]) + (self.group1()[2] * other.group9()[1])),
                (self.group1()[0] * other[e45]),
            ]),
            // e423, e431, e412
            ((Simd32x3::from(self.group1()[0]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]))
                - (Simd32x3::from(other.group9()[0]) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[3]]))),
            // e235, e315, e125
            (Simd32x3::from(other[e45]) * Simd32x3::from([self.group1()[1], self.group1()[2], self.group1()[3]])),
            // e1234, e4235, e4315, e4125
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e3215
            0.0,
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorOddOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(
            // e41, e42, e43
            ((swizzle!(other.group0(), 2, 0, 1) * Simd32x3::from([self.group1()[2], self.group1()[3], self.group1()[1]]))
                - (swizzle!(other.group0(), 1, 2, 0) * Simd32x3::from([self.group1()[3], self.group1()[1], self.group1()[2]]))),
        );
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorOddOnOrigin {
    type Output = Origin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        3        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return Origin::from_groups(
            // e4
            ((self.group1()[3] * other.group0()[2]) + (self.group1()[1] * other.group0()[0]) + (self.group1()[2] * other.group0()[1])),
        );
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorOddOnOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return NullVersorEvenAtOrigin::from_groups(
            // e423, e431, e412, e4
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[3], self.group0()[3]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorOddOnOrigin {
    type Output = NullDipoleAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        6        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return NullDipoleAtOrigin::from_groups(/* e41, e42, e43 */ Simd32x3::from([
            ((self.group1()[2] * other.group0()[2]) - (self.group1()[3] * other.group0()[1])),
            (-(self.group1()[1] * other.group0()[2]) + (self.group1()[3] * other.group0()[0])),
            ((self.group1()[1] * other.group0()[1]) - (self.group1()[2] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorOddOnOrigin {
    type Output = NullVersorEvenAtOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3       10        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return NullVersorEvenAtOrigin::from_groups(/* e423, e431, e412, e4 */ Simd32x4::from([
            (self.group1()[1] * other.group0()[3] * -1.0),
            (self.group1()[2] * other.group0()[3] * -1.0),
            (self.group1()[3] * other.group0()[3] * -1.0),
            ((self.group1()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[1]) - (self.group0()[3] * other.group0()[3]) + (self.group1()[1] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Plane> for VersorOddOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       16        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        5       17        0
    //  no simd        5       20        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[0] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[1]) + (self.group1()[2] * other.group0()[0])),
                (self.group1()[0] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[3], self.group0()[3]])),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorOddOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       12        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[0] * other.group0()[0]),
                (self.group1()[0] * other.group0()[1]),
                (self.group1()[0] * other.group0()[2]),
                0.0,
            ]),
            // e415, e425, e435, e4
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[1]) + (self.group1()[2] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for VersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        4        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(
            // scalar
            ((self.group1()[3] * other.group0()[2]) + (self.group1()[2] * other.group0()[1]) + (self.group1()[0] * other[e2]) + (self.group1()[1] * other.group0()[0])),
        );
    }
}
impl AntiWedge<RoundPointAtOrigin> for VersorOddOnOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group1()[0] * other.group0()[1]));
    }
}
impl AntiWedge<Sphere> for VersorOddOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        9       20        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        9       21        0
    //  no simd        9       24        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other[e4315])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[2] * other[e4315])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[3] * other[e4315])),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[2] * other.group0()[2]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[3] * other.group0()[0])),
                (-(self.group1()[1] * other.group0()[1]) + (self.group1()[2] * other.group0()[0])),
                (self.group1()[0] * other.group0()[3]),
            ]),
            // e235, e315, e125, e5
            (Simd32x4::from(other.group0()[3]) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[3], self.group0()[3]])),
            // e1, e2, e3, e4
            Simd32x4::from([
                (self.group0()[0] * other.group0()[3]),
                (self.group0()[1] * other.group0()[3]),
                (self.group0()[2] * other.group0()[3]),
                (-(self.group0()[3] * other[e4315]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorOddOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        5        0
    // no simd        0       20        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (swizzle!(self.group1(), 1, 2, 3, 0)
                * Simd32x4::from([other.group0()[1], other.group0()[1], other.group0()[1], other.group0()[0]])
                * Simd32x4::from([-1.0, -1.0, -1.0, 1.0])),
            // e235, e315, e125, e5
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[3], self.group0()[3]])),
            // e1, e2, e3, e4
            (self.group0() * Simd32x4::from([other.group0()[0], other.group0()[0], other.group0()[0], other.group0()[1]]) * Simd32x4::from([1.0, 1.0, 1.0, -1.0])),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorOddOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       12        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       13        0
    //  no simd        9       16        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[3])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[2] * other.group0()[3])),
                ((self.group1()[0] * other.group0()[2]) - (self.group1()[3] * other.group0()[3])),
                0.0,
            ]),
            // e415, e425, e435, e4
            (-(swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[3], self.group1()[1], self.group0()[3]]))
                + Simd32x4::from([
                    (self.group1()[3] * other.group0()[1]),
                    (self.group1()[1] * other.group0()[2]),
                    (self.group1()[2] * other.group0()[0]),
                    (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEven> for VersorOddOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       13       28        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       16       31        0
    //  no simd       25       40        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 2, 3, 1, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group3()[2]]))
                + (swizzle!(self.group1(), 0, 0, 0, 2) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group3()[1]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group0()[1]) + (self.group0()[0] * other.group0()[3])),
                    (-(self.group1()[1] * other.group0()[2]) + (self.group0()[1] * other.group0()[3])),
                    (-(self.group1()[2] * other.group0()[0]) + (self.group0()[2] * other.group0()[3])),
                    ((self.group1()[1] * other.group3()[0]) + (self.group1()[0] * other.group2()[3])
                        - (self.group0()[3] * other.group1()[3])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 1, 2, 3, 3) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group1()[0] * other.group2()[0]),
                    (self.group1()[0] * other.group2()[1]),
                    (self.group1()[0] * other.group2()[2]),
                    (-(self.group1()[2] * other.group1()[1]) + (self.group0()[3] * other.group0()[3]) - (self.group1()[1] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group2()[2]) + (self.group1()[3] * other.group2()[1])),
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[3] * other.group2()[0])),
                (-(self.group1()[1] * other.group2()[1]) + (self.group1()[2] * other.group2()[0])),
                (self.group1()[0] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (self.group1()[3] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorOddOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       29        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       15       30        0
    //  no simd       18       33        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 2, 3, 1, 0) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group2()[3]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group0()[1]) + (self.group0()[0] * other.group0()[3]) + (self.group1()[0] * other.group1()[0])),
                    (-(self.group1()[1] * other.group0()[2]) + (self.group0()[1] * other.group0()[3]) + (self.group1()[0] * other.group1()[1])),
                    (-(self.group1()[2] * other.group0()[0]) + (self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group1()[2])),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            Simd32x4::from([
                (self.group1()[0] * other.group2()[0]),
                (self.group1()[0] * other.group2()[1]),
                (self.group1()[0] * other.group2()[2]),
                (-(self.group1()[3] * other.group1()[2]) - (self.group1()[2] * other.group1()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group1()[1] * other.group1()[0])),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group2()[2]) + (self.group1()[3] * other.group2()[1])),
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[3] * other.group2()[0])),
                (-(self.group1()[1] * other.group2()[1]) + (self.group1()[2] * other.group2()[0])),
                (self.group1()[0] * other.group0()[3]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (self.group1()[3] * other.group0()[3]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorOddOnOrigin {
    type Output = VersorOdd;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       22        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       13       25        0
    //  no simd       22       34        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOdd::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(other.group0(), 0, 0, 0, 3) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + (swizzle!(self.group1(), 0, 0, 0, 2) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[1] * other.group0()[1]) + (self.group1()[0] * other.group2()[3])
                        - (self.group0()[3] * other.group1()[3])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e45
            (-(swizzle!(self.group1(), 1, 2, 3, 3) * swizzle!(other.group1(), 3, 3, 3, 2))
                + Simd32x4::from([
                    (self.group1()[0] * other.group2()[0]),
                    (self.group1()[0] * other.group2()[1]),
                    (self.group1()[0] * other.group2()[2]),
                    (-(self.group1()[2] * other.group1()[1]) + (self.group0()[3] * other.group0()[0]) - (self.group1()[1] * other.group1()[0])),
                ])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group2()[2]) + (self.group1()[3] * other.group2()[1])),
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[3] * other.group2()[0])),
                (-(self.group1()[1] * other.group2()[1]) + (self.group1()[2] * other.group2()[0])),
                (self.group1()[0] * other.group0()[0]),
            ]),
            // e4235, e4315, e4125, e3215
            Simd32x4::from([
                (self.group1()[1] * other.group0()[0]),
                (self.group1()[2] * other.group0()[0]),
                (self.group1()[3] * other.group0()[0]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorOddOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       18        0
    //    simd4        1        1        0
    // Totals...
    // yes simd        6       19        0
    //  no simd        9       22        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 2, 3, 1, 0) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group1()[3]]))
                + Simd32x4::from([
                    ((self.group1()[3] * other.group0()[1]) * -1.0),
                    ((self.group1()[1] * other.group0()[2]) * -1.0),
                    ((self.group1()[2] * other.group0()[0]) * -1.0),
                    (-(self.group0()[2] * other.group1()[2]) - (self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[0] * other.group1()[0]),
                (self.group1()[0] * other.group1()[1]),
                (self.group1()[0] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group1()[2]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[3] * other.group1()[0])),
                (-(self.group1()[1] * other.group1()[1]) + (self.group1()[2] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorOddOnOrigin {
    type Output = VersorOddOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        8        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        6       11        0
    //  no simd       12       20        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddOnOrigin::from_groups(
            // e41, e42, e43, e45
            (-(swizzle!(self.group1(), 3, 1, 2, 3) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[0], other.group1()[2]]))
                + (swizzle!(other.group0(), 2, 0, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[3], self.group1()[1], self.group0()[3]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[0] * other.group1()[0])),
                    ((self.group0()[1] * other.group0()[3]) + (self.group1()[0] * other.group1()[1])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group1()[2])),
                    (-(self.group1()[2] * other.group1()[1]) - (self.group1()[1] * other.group1()[0])),
                ])),
            // e1234, e4235, e4315, e4125
            (self.group1() * Simd32x4::from(other.group0()[3])),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorOddOnOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       18        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       20        0
    //  no simd       19       26        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((swizzle!(self.group1(), 2, 3, 1, 3) * Simd32x4::from([other.group0()[2], other.group0()[0], other.group0()[1], other.group2()[2]]))
                - (swizzle!(other.group0(), 1, 2, 0, 3) * Simd32x4::from([self.group1()[3], self.group1()[1], self.group1()[2], self.group0()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group2()[1]) + (self.group1()[1] * other.group2()[0]) + (self.group1()[0] * other.group1()[3])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[0]) - (self.group1()[1] * other.group0()[3])),
                ((self.group1()[0] * other.group1()[1]) - (self.group1()[2] * other.group0()[3])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[3] * other.group0()[3])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (-(self.group1()[2] * other.group1()[2]) + (self.group1()[3] * other.group1()[1])),
                ((self.group1()[1] * other.group1()[2]) - (self.group1()[3] * other.group1()[0])),
                (-(self.group1()[1] * other.group1()[1]) + (self.group1()[2] * other.group1()[0])),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<VersorOdd> for VersorOddOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       17       32        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       19       34        0
    //  no simd       25       40        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group1()[0] * other.group3()[0]) - (self.group1()[1] * other.group2()[3])),
                ((self.group1()[0] * other.group3()[1]) - (self.group1()[2] * other.group2()[3])),
                ((self.group1()[0] * other.group3()[2]) - (self.group1()[3] * other.group2()[3])),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[2] * other.group3()[2]) + (self.group1()[3] * other.group3()[1])),
                ((self.group1()[1] * other.group3()[2]) - (self.group1()[3] * other.group3()[0])),
                (-(self.group1()[1] * other.group3()[1]) + (self.group1()[2] * other.group3()[0])),
                (self.group1()[0] * other.group3()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[1] * other.group3()[3]),
                (self.group1()[2] * other.group3()[3]),
                (self.group1()[3] * other.group3()[3]),
                (-(self.group1()[3] * other.group2()[2]) - (self.group1()[2] * other.group2()[1]) + (self.group0()[3] * other.group3()[3])
                    - (self.group1()[1] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(self.group1(), 0, 0, 0, 2) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[1]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group1()[1]) + (self.group0()[0] * other.group3()[3])),
                    (-(self.group1()[1] * other.group1()[2]) + (self.group0()[1] * other.group3()[3])),
                    (-(self.group1()[2] * other.group1()[0]) + (self.group0()[2] * other.group3()[3])),
                    ((self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group1()[3])
                        - (self.group0()[3] * other.group2()[3])
                        - (self.group0()[2] * other.group3()[2])
                        - (self.group0()[0] * other.group3()[0])
                        - (self.group0()[1] * other.group3()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorOddOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       15       30        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       16       31        0
    //  no simd       19       34        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group1()[0] * other.group2()[0]) - (self.group1()[1] * other.group1()[3])),
                ((self.group1()[0] * other.group2()[1]) - (self.group1()[2] * other.group1()[3])),
                ((self.group1()[0] * other.group2()[2]) - (self.group1()[3] * other.group1()[3])),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[2] * other.group2()[2]) + (self.group1()[3] * other.group2()[1])),
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[3] * other.group2()[0])),
                (-(self.group1()[1] * other.group2()[1]) + (self.group1()[2] * other.group2()[0])),
                (self.group1()[0] * other.group2()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[1] * other.group2()[3]),
                (self.group1()[2] * other.group2()[3]),
                (self.group1()[3] * other.group2()[3]),
                (-(self.group1()[3] * other.group1()[2]) - (self.group1()[2] * other.group1()[1]) + (self.group0()[3] * other.group2()[3])
                    - (self.group1()[1] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 0, 0, 0, 3) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[2]]))
                + Simd32x4::from([
                    (self.group0()[0] * other.group2()[3]),
                    (self.group0()[1] * other.group2()[3]),
                    (self.group0()[2] * other.group2()[3]),
                    ((self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])
                        - (self.group0()[3] * other.group1()[3])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorOddOnOrigin {
    type Output = VersorEven;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       29        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       15       30        0
    //  no simd       18       33        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEven::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                (self.group1()[0] * other.group2()[0]),
                (self.group1()[0] * other.group2()[1]),
                (self.group1()[0] * other.group2()[2]),
                0.0,
            ]),
            // e415, e425, e435, e321
            Simd32x4::from([
                (-(self.group1()[2] * other.group2()[2]) + (self.group1()[3] * other.group2()[1])),
                ((self.group1()[1] * other.group2()[2]) - (self.group1()[3] * other.group2()[0])),
                (-(self.group1()[1] * other.group2()[1]) + (self.group1()[2] * other.group2()[0])),
                (self.group1()[0] * other.group2()[3]),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[1] * other.group2()[3]),
                (self.group1()[2] * other.group2()[3]),
                (self.group1()[3] * other.group2()[3]),
                (-(self.group1()[3] * other.group0()[3]) - (self.group1()[2] * other.group0()[2]) + (self.group0()[3] * other.group2()[3])
                    - (self.group1()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 2, 3, 1, 0) * swizzle!(other.group1(), 2, 0, 1, 3))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group1()[1]) + (self.group0()[0] * other.group2()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group1()[1] * other.group1()[2]) + (self.group0()[1] * other.group2()[3]) + (self.group1()[0] * other.group0()[2])),
                    (-(self.group1()[2] * other.group1()[0]) + (self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group0()[3])),
                    (-(self.group0()[2] * other.group2()[2]) - (self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorOddOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4        9        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        6       13        0
    //  no simd       12       25        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (swizzle!(self.group1(), 1, 2, 3, 0)
                * Simd32x4::from([other.group1()[3], other.group1()[3], other.group1()[3], other.group0()[3]])
                * Simd32x4::from([-1.0, -1.0, -1.0, 1.0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[1] * other.group0()[3]),
                (self.group1()[2] * other.group0()[3]),
                (self.group1()[3] * other.group0()[3]),
                (-(self.group1()[3] * other.group1()[2]) - (self.group1()[2] * other.group1()[1]) + (self.group0()[3] * other.group0()[3])
                    - (self.group1()[1] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 3, 3, 3, 2) * Simd32x4::from([self.group0()[0], self.group0()[1], self.group0()[2], self.group1()[3]]))
                + (swizzle!(self.group1(), 0, 0, 0, 2) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[1]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[3] * other.group1()[3]) + (self.group1()[1] * other.group0()[0]))])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorOddOnOrigin {
    type Output = VersorEvenOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        8       12        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       10       14        0
    //  no simd       16       20        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEvenOnOrigin::from_groups(
            // e423, e431, e412, e12345
            Simd32x4::from([
                ((self.group1()[0] * other.group1()[1]) - (self.group1()[1] * other.group1()[0])),
                ((self.group1()[0] * other.group1()[2]) - (self.group1()[2] * other.group1()[0])),
                ((self.group1()[0] * other.group1()[3]) - (self.group1()[3] * other.group1()[0])),
                0.0,
            ]),
            // e415, e425, e435, e4
            (-(swizzle!(other.group1(), 3, 1, 2, 0) * Simd32x4::from([self.group1()[2], self.group1()[3], self.group1()[1], self.group0()[3]]))
                + (swizzle!(self.group1(), 3, 1, 2, 3) * Simd32x4::from([other.group1()[2], other.group1()[3], other.group1()[1], other.group0()[2]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    ((self.group1()[2] * other.group0()[1]) + (self.group1()[1] * other.group0()[0]) + (self.group1()[0] * other.group0()[3])
                        - (self.group0()[2] * other.group1()[3])
                        - (self.group0()[0] * other.group1()[1])
                        - (self.group0()[1] * other.group1()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorOddOnOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       15        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        9       19        0
    //  no simd       15       31        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (swizzle!(self.group1(), 1, 2, 3, 0)
                * Simd32x4::from([other.group2()[3], other.group2()[3], other.group2()[3], other.group1()[3]])
                * Simd32x4::from([-1.0, -1.0, -1.0, 1.0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[1] * other.group1()[3]),
                (self.group1()[2] * other.group1()[3]),
                (self.group1()[3] * other.group1()[3]),
                (-(self.group1()[3] * other.group2()[2]) - (self.group1()[2] * other.group2()[1]) + (self.group0()[3] * other.group1()[3])
                    - (self.group1()[1] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(self.group1(), 2, 3, 1, 3) * Simd32x4::from([other.group1()[2], other.group1()[0], other.group1()[1], other.group0()[2]]))
                + (swizzle!(self.group1(), 0, 0, 0, 2) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group0()[1]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group1()[1]) + (self.group0()[0] * other.group1()[3])),
                    (-(self.group1()[1] * other.group1()[2]) + (self.group0()[1] * other.group1()[3])),
                    (-(self.group1()[2] * other.group1()[0]) + (self.group0()[2] * other.group1()[3])),
                    (-(self.group0()[3] * other.group2()[3]) + (self.group1()[1] * other.group0()[0])),
                ])),
        );
    }
}
impl InfixAntiWedge for VersorOddOrthogonalOrigin {}
impl AntiWedge<AntiCircleOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: AntiCircleOnOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<AntiDipoleOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiDipoleOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiDualNum> for VersorOddOrthogonalOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: AntiDualNum) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group2()[3], self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<AntiFlatPoint> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: AntiFlatPoint) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group2()[3] * other.group0()[0]),
            (self.group2()[3] * other.group0()[1]),
            (self.group2()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiFlector> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiFlector) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group2()[3] * other.group0()[0]),
            (self.group2()[3] * other.group0()[1]),
            (self.group2()[3] * other.group0()[2]),
            ((self.group2()[3] * other.group1()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<AntiLine> for VersorOddOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: AntiLine) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group2()[3]) * other.group1()));
    }
}
impl AntiWedge<AntiMotor> for VersorOddOrthogonalOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiMotor) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group2()[3] * other.group1()[3]),
            ((self.group0()[0] * other.group1()[3]) + (self.group2()[3] * other.group1()[0])),
            ((self.group0()[1] * other.group1()[3]) + (self.group2()[3] * other.group1()[1])),
            ((self.group0()[2] * other.group1()[3]) + (self.group2()[3] * other.group1()[2])),
        ]));
    }
}
impl AntiWedge<AntiPlane> for VersorOddOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiPlane) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group2()[3] * other.group0()[3]));
    }
}
impl AntiWedge<AntiScalar> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        3        0
    // no simd        0       12        0
    fn anti_wedge(self, other: AntiScalar) -> Self::Output {
        use crate::elements::*;
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            (self.group0() * Simd32x4::from(other[e12345])),
            // e23, e31, e12, e3215
            (self.group1() * Simd32x4::from(other[e12345])),
            // e15, e25, e35, e1234
            (self.group2() * Simd32x4::from(other[e12345])),
        );
    }
}
impl AntiWedge<AntiSphereOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: AntiSphereOnOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other.group0()[3]));
    }
}
impl AntiWedge<AntiVersorEvenOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: AntiVersorEvenOnOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group1()[3] * -1.0),
            (-(self.group1()[3] * other.group0()[0]) - (self.group2()[0] * other.group1()[3])),
            (-(self.group1()[3] * other.group0()[1]) - (self.group2()[1] * other.group1()[3])),
            (-(self.group1()[3] * other.group0()[2]) - (self.group2()[2] * other.group1()[3])),
        ]));
    }
}
impl AntiWedge<AntiVersorOddOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: AntiVersorOddOnOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group1()[0]) - (self.group2()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<Circle> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       21        0
    fn anti_wedge(self, other: Circle) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group1()[0]),
                (self.group2()[3] * other.group1()[1]),
                (self.group2()[3] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[3] * other.group0()[0]) + (self.group2()[3] * other.group2()[0])),
                ((self.group1()[3] * other.group0()[1]) + (self.group2()[3] * other.group2()[1])),
                ((self.group1()[3] * other.group0()[2]) + (self.group2()[3] * other.group2()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAligningOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32       11       21        0
    fn anti_wedge(self, other: CircleAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group1()[0]),
                (self.group2()[3] * other.group1()[1]),
                (self.group2()[3] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[1] * other.group1()[1])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group0()[2] * other.group2()[2])
                    - (self.group0()[0] * other.group2()[0])
                    - (self.group0()[1] * other.group2()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[3] * other.group0()[0]) + (self.group2()[3] * other.group2()[0])),
                ((self.group1()[3] * other.group0()[1]) + (self.group2()[3] * other.group2()[1])),
                ((self.group1()[3] * other.group0()[2]) + (self.group2()[3] * other.group2()[2])),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtInfinity> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: CircleAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group2()[3] * other.group1()[0]),
                (self.group2()[3] * other.group1()[1]),
                (self.group2()[3] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       12        0
    fn anti_wedge(self, other: CircleAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            ((self.group1()[3] * other.group0()[0]) + (self.group2()[3] * other.group1()[0])),
            ((self.group1()[3] * other.group0()[1]) + (self.group2()[3] * other.group1()[1])),
            ((self.group1()[3] * other.group0()[2]) + (self.group2()[3] * other.group1()[2])),
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<CircleOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: CircleOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group1()[0]),
                (self.group2()[3] * other.group1()[1]),
                (self.group2()[3] * other.group1()[2]),
                (-(self.group2()[2] * other.group0()[2])
                    - (self.group2()[1] * other.group0()[1])
                    - (self.group2()[0] * other.group0()[0])
                    - (self.group1()[2] * other.group1()[2])
                    - (self.group1()[0] * other.group1()[0])
                    - (self.group1()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0]),
                (self.group1()[3] * other.group1()[1]),
                (self.group1()[3] * other.group1()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<CircleOrthogonalOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        8       12        0
    fn anti_wedge(self, other: CircleOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            ((self.group1()[3] * other.group0()[0]) + (self.group2()[3] * other.group1()[0])),
            ((self.group1()[3] * other.group0()[1]) + (self.group2()[3] * other.group1()[1])),
            ((self.group1()[3] * other.group0()[2]) + (self.group2()[3] * other.group1()[2])),
            (-(self.group2()[2] * other.group0()[2])
                - (self.group2()[1] * other.group0()[1])
                - (self.group2()[0] * other.group0()[0])
                - (self.group0()[2] * other.group1()[2])
                - (self.group0()[0] * other.group1()[0])
                - (self.group0()[1] * other.group1()[1])),
        ]));
    }
}
impl AntiWedge<Dipole> for VersorOddOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: Dipole) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[3] * other.group0()[0]) + (self.group2()[3] * other.group2()[0])),
                (-(self.group1()[3] * other.group0()[1]) + (self.group2()[3] * other.group2()[1])),
                (-(self.group1()[3] * other.group0()[2]) + (self.group2()[3] * other.group2()[2])),
                (self.group2()[3] * other.group1()[3]),
            ]),
            // e5
            (self.group1()[3] * other.group1()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleAligningOrigin> for VersorOddOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        9        0
    fn anti_wedge(self, other: DipoleAligningOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            Simd32x4::from([
                (-(self.group1()[3] * other.group0()[0]) + (self.group2()[3] * other.group1()[0])),
                (-(self.group1()[3] * other.group0()[1]) + (self.group2()[3] * other.group1()[1])),
                (-(self.group1()[3] * other.group0()[2]) + (self.group2()[3] * other.group1()[2])),
                (self.group2()[3] * other.group0()[3]),
            ]),
            // e5
            (self.group1()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleAtInfinity> for VersorOddOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        6        0
    fn anti_wedge(self, other: DipoleAtInfinity) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]])),
            // e5
            (self.group1()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: DipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(Simd32x3::from(self.group1()[3]) * other.group0()) + (Simd32x3::from(self.group2()[3]) * other.group1())),
        );
    }
}
impl AntiWedge<DipoleOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        0        4        0
    //  no simd        0       10        0
    fn anti_wedge(self, other: DipoleOnOrigin) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (other.group0() * Simd32x4::from([self.group1()[3], self.group1()[3], self.group1()[3], self.group2()[3]]) * Simd32x4::from([-1.0, -1.0, -1.0, 1.0])),
            // e5
            (self.group1()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<DipoleOrthogonalOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        1        2        0
    // no simd        3        6        0
    fn anti_wedge(self, other: DipoleOrthogonalOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(
            // e1, e2, e3
            (-(Simd32x3::from(self.group1()[3]) * other.group0()) + (Simd32x3::from(self.group2()[3]) * other.group2())),
        );
    }
}
impl AntiWedge<DualNum> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        1        5        0
    //    simd4        0        2        0
    // Totals...
    // yes simd        1        7        0
    //  no simd        1       13        0
    fn anti_wedge(self, other: DualNum) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group0()[0] * other.group0()[1]),
                (self.group0()[1] * other.group0()[1]),
                (self.group0()[2] * other.group0()[1]),
                ((self.group0()[3] * other.group0()[1]) + (self.group2()[3] * other.group0()[0])),
            ]),
            // e23, e31, e12, e3215
            (self.group1() * Simd32x4::from(other.group0()[1])),
            // e15, e25, e35, e1234
            (self.group2() * Simd32x4::from(other.group0()[1])),
        );
    }
}
impl AntiWedge<FlatOrigin> for VersorOddOrthogonalOrigin {
    type Output = RoundPointAtOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd2        0        2        0
    // no simd        0        4        0
    fn anti_wedge(self, other: FlatOrigin) -> Self::Output {
        use crate::elements::*;
        return RoundPointAtOrigin::from_groups(
            // e4, e5
            (Simd32x2::from(other[e45]) * Simd32x2::from([self.group2()[3], self.group1()[3]]) * Simd32x2::from([1.0, -1.0])),
        );
    }
}
impl AntiWedge<FlatPoint> for VersorOddOrthogonalOrigin {
    type Output = RoundPoint;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        0        2        0
    //    simd4        0        1        0
    // Totals...
    // yes simd        0        3        0
    //  no simd        0        6        0
    fn anti_wedge(self, other: FlatPoint) -> Self::Output {
        return RoundPoint::from_groups(
            // e1, e2, e3, e4
            (Simd32x4::from(self.group2()[3]) * other.group0()),
            // e5
            (self.group1()[3] * other.group0()[3] * -1.0),
        );
    }
}
impl AntiWedge<FlatPointAtInfinity> for VersorOddOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        1        0
    // no simd        0        3        0
    fn anti_wedge(self, other: FlatPointAtInfinity) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group2()[3]) * other.group0()));
    }
}
impl AntiWedge<Flector> for VersorOddOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       18        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        9       21        0
    //  no simd       15       30        0
    fn anti_wedge(self, other: Flector) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group2()[3]) * other.group1()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group1()[0] * -1.0),
                (self.group1()[3] * other.group1()[1] * -1.0),
                (self.group1()[3] * other.group1()[2] * -1.0),
                ((self.group2()[2] * other.group1()[2]) + (self.group2()[1] * other.group1()[1]) - (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group1()[0])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group2()[3]) * other.group0())
                - (swizzle!(other.group1(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group1()[3]) + (self.group1()[1] * other.group1()[2])),
                    ((self.group1()[2] * other.group1()[0]) + (self.group0()[1] * other.group1()[3])),
                    ((self.group0()[2] * other.group1()[3]) + (self.group1()[0] * other.group1()[1])),
                    (-(self.group0()[0] * other.group1()[0]) - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<FlectorAtInfinity> for VersorOddOrthogonalOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: FlectorAtInfinity) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group2()[3] * other.group0()[3]),
            ((self.group0()[0] * other.group0()[3]) + (self.group2()[3] * other.group0()[0])),
            ((self.group0()[1] * other.group0()[3]) + (self.group2()[3] * other.group0()[1])),
            ((self.group0()[2] * other.group0()[3]) + (self.group2()[3] * other.group0()[2])),
        ]));
    }
}
impl AntiWedge<FlectorOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        4       15        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        6       17        0
    //  no simd       12       23        0
    fn anti_wedge(self, other: FlectorOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (self.group2()[3] * other.group0()[3]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                (self.group1()[3] * other.group0()[3] * -1.0),
                ((self.group2()[2] * other.group0()[3]) + (self.group2()[1] * other.group0()[2]) - (self.group1()[3] * other.group0()[0]) + (self.group2()[0] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            ((swizzle!(other.group0(), 3, 1, 2, 0) * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[0], self.group2()[3]]))
                - (swizzle!(other.group0(), 2, 3, 1, 3) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([0.0, 0.0, 0.0, (-(self.group0()[0] * other.group0()[1]) - (self.group0()[1] * other.group0()[2]))])),
        );
    }
}
impl AntiWedge<Horizon> for VersorOddOrthogonalOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        1        0
    // no simd        0        4        0
    fn anti_wedge(self, other: Horizon) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other[e3215]) * Simd32x4::from([self.group2()[3], self.group0()[0], self.group0()[1], self.group0()[2]])),
        );
    }
}
impl AntiWedge<Infinity> for VersorOddOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Infinity) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group2()[3] * other[e5]));
    }
}
impl AntiWedge<Line> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        5       15        0
    fn anti_wedge(self, other: Line) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2])
                    - (self.group1()[1] * other.group0()[1])
                    - (self.group1()[0] * other.group0()[0])
                    - (self.group0()[2] * other.group1()[2])
                    - (self.group0()[0] * other.group1()[0])
                    - (self.group0()[1] * other.group1()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from([
                (self.group2()[3] * other.group1()[0]),
                (self.group2()[3] * other.group1()[1]),
                (self.group2()[3] * other.group1()[2]),
                0.0,
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<LineAtInfinity> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: LineAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group2()[3] * other.group0()[0]),
            (self.group2()[3] * other.group0()[1]),
            (self.group2()[3] * other.group0()[2]),
            (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<LineOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        9        0
    fn anti_wedge(self, other: LineOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group1()[2] * other.group0()[2]) - (self.group1()[0] * other.group0()[0]) - (self.group1()[1] * other.group0()[1])),
            ]),
            // e23, e31, e12, e3215
            Simd32x4::from(0.0),
            // e15, e25, e35, e1234
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0]),
                (self.group1()[3] * other.group0()[1]),
                (self.group1()[3] * other.group0()[2]),
                0.0,
            ]),
        );
    }
}
impl AntiWedge<Motor> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: Motor) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group0()[2])
                        - (self.group1()[1] * other.group0()[1])
                        - (self.group1()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group2()[3] * other.group1()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group2()[3] * other.group1()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group2()[3] * other.group1()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[3] * other.group0()[0]) + (self.group2()[0] * other.group0()[3])),
                ((self.group1()[3] * other.group0()[1]) + (self.group2()[1] * other.group0()[3])),
                ((self.group1()[3] * other.group0()[2]) + (self.group2()[2] * other.group0()[3])),
                (self.group2()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<MotorAtInfinity> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: MotorAtInfinity) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group2()[3] * other.group0()[0]),
            (self.group2()[3] * other.group0()[1]),
            (self.group2()[3] * other.group0()[2]),
            ((self.group2()[3] * other.group0()[3]) - (self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<MotorOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5       13        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        6       15        0
    //  no simd        9       21        0
    fn anti_wedge(self, other: MotorOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + Simd32x4::from([
                    (self.group2()[3] * other.group0()[0]),
                    (self.group2()[3] * other.group0()[1]),
                    (self.group2()[3] * other.group0()[2]),
                    (-(self.group1()[2] * other.group0()[2]) - (self.group1()[1] * other.group0()[1]) - (self.group1()[0] * other.group0()[0])),
                ])),
            // e23, e31, e12, e3215
            (self.group1() * Simd32x4::from(other.group0()[3])),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[3] * other.group0()[0]) + (self.group2()[0] * other.group0()[3])),
                ((self.group1()[3] * other.group0()[1]) + (self.group2()[1] * other.group0()[3])),
                ((self.group1()[3] * other.group0()[2]) + (self.group2()[2] * other.group0()[3])),
                (self.group2()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<MultiVector> for VersorOddOrthogonalOrigin {
    type Output = MultiVector;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       24       37        0
    //    simd3        3        8        0
    //    simd4        3        3        0
    // Totals...
    // yes simd       30       48        0
    //  no simd       45       73        0
    fn anti_wedge(self, other: MultiVector) -> Self::Output {
        use crate::elements::*;
        return MultiVector::from_groups(
            // scalar, e12345
            Simd32x2::from([
                ((self.group2()[3] * other[e1]) - (self.group2()[2] * other.group7()[2]) - (self.group2()[1] * other.group7()[1]) - (self.group2()[0] * other.group7()[0])
                    + (self.group1()[3] * other.group1()[3])
                    - (self.group1()[2] * other.group6()[2])
                    - (self.group1()[1] * other.group6()[1])
                    - (self.group1()[0] * other.group6()[0])
                    + (self.group0()[3] * other.group0()[1])
                    - (self.group0()[2] * other.group8()[2])
                    - (self.group0()[0] * other.group8()[0])
                    - (self.group0()[1] * other.group8()[1])),
                0.0,
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group4()[0], other.group4()[1], other.group4()[2], other.group3()[3]]))
                - (swizzle!(other.group9(), 0, 0, 0, 3) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[2]]))
                - (swizzle!(other.group9(), 2, 3, 1, 1) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[0]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group3()[0]) + (self.group0()[0] * other[e45]) + (self.group1()[1] * other.group9()[3])),
                    (-(self.group1()[3] * other.group3()[1]) + (self.group1()[2] * other.group9()[1]) + (self.group0()[1] * other[e45])),
                    (-(self.group1()[3] * other.group3()[2]) + (self.group0()[2] * other[e45]) + (self.group1()[0] * other.group9()[2])),
                    ((self.group0()[1] * other.group9()[2]) * -1.0),
                ])),
            // e5
            ((self.group2()[2] * other.group9()[3]) + (self.group2()[1] * other.group9()[2]) - (self.group1()[3] * other.group3()[3]) + (self.group2()[0] * other.group9()[1])),
            // e41, e42, e43, e45
            Simd32x4::from([
                ((self.group0()[0] * other.group0()[1]) + (self.group2()[3] * other.group6()[0])),
                ((self.group0()[1] * other.group0()[1]) + (self.group2()[3] * other.group6()[1])),
                ((self.group0()[2] * other.group0()[1]) + (self.group2()[3] * other.group6()[2])),
                0.0,
            ]),
            // e15, e25, e35
            ((Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group6()[0], other.group6()[1], other.group6()[2]]))
                + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group2()[0], self.group2()[1], self.group2()[2]]))),
            // e23, e31, e12
            ((Simd32x3::from(self.group2()[3]) * other.group8())
                + (Simd32x3::from(other.group0()[1]) * Simd32x3::from([self.group1()[0], self.group1()[1], self.group1()[2]]))
                + (Simd32x3::from(self.group1()[3]) * other.group7())),
            // e415, e425, e435, e321
            Simd32x4::from([0.0, 0.0, 0.0, (-(self.group1()[3] * other.group9()[0]) + (self.group2()[3] * other[e45]))]),
            // e423, e431, e412
            (Simd32x3::from(self.group2()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]])),
            // e235, e315, e125
            (Simd32x3::from(self.group1()[3]) * Simd32x3::from([other.group9()[1], other.group9()[2], other.group9()[3]]) * Simd32x3::from(-1.0)),
            // e1234, e4235, e4315, e4125
            Simd32x4::from([(self.group2()[3] * other.group0()[1]), 0.0, 0.0, 0.0]),
            // e3215
            (self.group1()[3] * other.group0()[1]),
        );
    }
}
impl AntiWedge<NullCircleAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        2        6        0
    fn anti_wedge(self, other: NullCircleAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[0] * other.group0()[0]) - (self.group2()[1] * other.group0()[1])),
        ]));
    }
}
impl AntiWedge<NullDipoleAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiPlaneOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd3        0        2        0
    // no simd        0        6        0
    fn anti_wedge(self, other: NullDipoleAtOrigin) -> Self::Output {
        return AntiPlaneOnOrigin::from_groups(/* e1, e2, e3 */ (Simd32x3::from(self.group1()[3]) * other.group0() * Simd32x3::from(-1.0)));
    }
}
impl AntiWedge<NullSphereAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        0        2        0
    // no simd        0        8        0
    fn anti_wedge(self, other: NullSphereAtOrigin) -> Self::Output {
        use crate::elements::*;
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (Simd32x4::from(other[e1234]) * Simd32x4::from([self.group1()[3], self.group2()[0], self.group2()[1], self.group2()[2]]) * Simd32x4::from(-1.0)),
        );
    }
}
impl AntiWedge<NullVersorEvenAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        7        0
    fn anti_wedge(self, other: NullVersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(/* e23, e31, e12, scalar */ Simd32x4::from([
            (self.group1()[3] * other.group0()[0]),
            (self.group1()[3] * other.group0()[1]),
            (self.group1()[3] * other.group0()[2]),
            (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) + (self.group1()[3] * other.group0()[3]) - (self.group2()[0] * other.group0()[0])),
        ]));
    }
}
impl AntiWedge<NullVersorOddAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        3        8        0
    fn anti_wedge(self, other: NullVersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(/* e321, e1, e2, e3 */ Simd32x4::from([
            (self.group1()[3] * other.group0()[3] * -1.0),
            (-(self.group1()[3] * other.group0()[0]) - (self.group2()[0] * other.group0()[3])),
            (-(self.group1()[3] * other.group0()[1]) - (self.group2()[1] * other.group0()[3])),
            (-(self.group1()[3] * other.group0()[2]) - (self.group2()[2] * other.group0()[3])),
        ]));
    }
}
impl AntiWedge<Origin> for VersorOddOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        0        1        0
    fn anti_wedge(self, other: Origin) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ (self.group1()[3] * other[e4]));
    }
}
impl AntiWedge<Plane> for VersorOddOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        6       17        0
    //    simd4        1        2        0
    // Totals...
    // yes simd        7       19        0
    //  no simd       10       25        0
    fn anti_wedge(self, other: Plane) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group2()[3]) * other.group0()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    ((self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    ((self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<PlaneOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        7       21        0
    fn anti_wedge(self, other: PlaneOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                0.0,
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            Simd32x4::from([
                ((self.group1()[1] * other.group0()[2]) - (self.group1()[2] * other.group0()[1])),
                (-(self.group1()[0] * other.group0()[2]) + (self.group1()[2] * other.group0()[0])),
                ((self.group1()[0] * other.group0()[1]) - (self.group1()[1] * other.group0()[0])),
                (-(self.group0()[2] * other.group0()[2]) - (self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
            ]),
        );
    }
}
impl AntiWedge<RoundPoint> for VersorOddOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: RoundPoint) -> Self::Output {
        use crate::elements::*;
        return Scalar::from_groups(/* scalar */ ((self.group1()[3] * other.group0()[3]) + (self.group2()[3] * other[e2])));
    }
}
impl AntiWedge<RoundPointAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = Scalar;
    // Operative Statistics for this implementation:
    //      add/sub      mul      div
    // f32        1        2        0
    fn anti_wedge(self, other: RoundPointAtOrigin) -> Self::Output {
        return Scalar::from_groups(/* scalar */ ((self.group1()[3] * other.group0()[0]) + (self.group2()[3] * other.group0()[1])));
    }
}
impl AntiWedge<Sphere> for VersorOddOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       10       25        0
    //    simd4        1        1        0
    // Totals...
    // yes simd       11       26        0
    //  no simd       14       29        0
    fn anti_wedge(self, other: Sphere) -> Self::Output {
        use crate::elements::*;
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group2()[3] * other.group0()[0]),
                (self.group2()[3] * other.group0()[1]),
                (self.group2()[3] * other.group0()[2]),
                (-(self.group1()[3] * other[e4315]) + (self.group2()[3] * other.group0()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[0] * other[e4315]) + (self.group0()[0] * other.group0()[3]) + (self.group1()[1] * other.group0()[2])),
                    (-(self.group2()[1] * other[e4315]) + (self.group1()[2] * other.group0()[0]) + (self.group0()[1] * other.group0()[3])),
                    (-(self.group2()[2] * other[e4315]) + (self.group0()[2] * other.group0()[3]) + (self.group1()[0] * other.group0()[1])),
                    (-(self.group0()[0] * other.group0()[0]) - (self.group0()[1] * other.group0()[1])),
                ])),
        );
    }
}
impl AntiWedge<SphereAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //          add/sub      mul      div
    //   simd4        1        2        0
    // no simd        4        8        0
    fn anti_wedge(self, other: SphereAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (-(Simd32x4::from(other.group0()[1]) * Simd32x4::from([self.group1()[3], self.group2()[0], self.group2()[1], self.group2()[2]]))
                + (Simd32x4::from(other.group0()[0]) * Simd32x4::from([self.group2()[3], self.group0()[0], self.group0()[1], self.group0()[2]]))),
        );
    }
}
impl AntiWedge<SphereOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        2        9        0
    //    simd4        2        6        0
    // Totals...
    // yes simd        4       15        0
    //  no simd       10       33        0
    fn anti_wedge(self, other: SphereOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (other.group0() * Simd32x4::from([self.group2()[3], self.group2()[3], self.group2()[3], self.group1()[3]]) * Simd32x4::from([1.0, 1.0, 1.0, -1.0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group0()[0] * -1.0),
                (self.group1()[3] * other.group0()[1] * -1.0),
                (self.group1()[3] * other.group0()[2] * -1.0),
                ((self.group2()[2] * other.group0()[2]) + (self.group2()[0] * other.group0()[0]) + (self.group2()[1] * other.group0()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group0(), 3, 3, 3, 2) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[2]]))
                - (swizzle!(other.group0(), 1, 2, 0, 0) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[0]]))
                + (Simd32x4::from([1.0, 1.0, 1.0, -1.0])
                    * swizzle!(other.group0(), 2, 0, 1, 1)
                    * Simd32x4::from([self.group1()[1], self.group1()[2], self.group1()[0], self.group0()[1]]))),
        );
    }
}
impl AntiWedge<VersorEven> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       27        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       20       29        0
    //  no simd       26       35        0
    fn anti_wedge(self, other: VersorEven) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) - (self.group2()[0] * other.group0()[0])
                        + (self.group1()[3] * other.group3()[3])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group2()[3] * other.group2()[0]) + (self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                ((self.group2()[3] * other.group2()[1]) + (self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                ((self.group2()[3] * other.group2()[2]) + (self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) + (self.group2()[0] * other.group0()[3])),
                ((self.group1()[3] * other.group1()[1]) + (self.group2()[1] * other.group0()[3])),
                ((self.group1()[3] * other.group1()[2]) + (self.group2()[2] * other.group0()[3])),
                (self.group2()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAligningOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       18       27        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       20       29        0
    //  no simd       26       35        0
    fn anti_wedge(self, other: VersorEvenAligningOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2]) - (self.group2()[1] * other.group0()[1]) - (self.group2()[0] * other.group0()[0])
                        + (self.group1()[3] * other.group1()[3])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group2()[3] * other.group2()[0]) + (self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                ((self.group2()[3] * other.group2()[1]) + (self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                ((self.group2()[3] * other.group2()[2]) + (self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) + (self.group2()[0] * other.group0()[3])),
                ((self.group1()[3] * other.group1()[1]) + (self.group2()[1] * other.group0()[3])),
                ((self.group1()[3] * other.group1()[2]) + (self.group2()[2] * other.group0()[3])),
                (self.group2()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtInfinity> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: VersorEvenAtInfinity) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[0]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group2()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])
                        - (self.group0()[2] * other.group2()[2])
                        - (self.group0()[0] * other.group2()[0])
                        - (self.group0()[1] * other.group2()[1])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[0]) + (self.group2()[3] * other.group2()[0])),
                ((self.group1()[1] * other.group0()[0]) + (self.group2()[3] * other.group2()[1])),
                ((self.group1()[2] * other.group0()[0]) + (self.group2()[3] * other.group2()[2])),
                (self.group1()[3] * other.group0()[0]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) + (self.group2()[0] * other.group0()[0])),
                ((self.group1()[3] * other.group1()[1]) + (self.group2()[1] * other.group0()[0])),
                ((self.group1()[3] * other.group1()[2]) + (self.group2()[2] * other.group0()[0])),
                (self.group2()[3] * other.group0()[0]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        7        8        0
    //  no simd       13       14        0
    fn anti_wedge(self, other: VersorEvenAtOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(self.group1()[3]) * other.group0())
                + (Simd32x4::from(self.group2()[3]) * other.group1())
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorEvenOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorOddOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       11       20        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       13       22        0
    //  no simd       19       28        0
    fn anti_wedge(self, other: VersorEvenOnOrigin) -> Self::Output {
        return VersorOddOrthogonalOrigin::from_groups(
            // e41, e42, e43, scalar
            ((self.group0() * Simd32x4::from(other.group0()[3]))
                + (other.group1() * Simd32x4::from([self.group2()[3], self.group2()[3], self.group2()[3], self.group1()[3]]))
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group1()[2] * other.group1()[2])
                        - (self.group1()[1] * other.group1()[1])
                        - (self.group1()[0] * other.group1()[0])),
                ])),
            // e23, e31, e12, e3215
            Simd32x4::from([
                ((self.group1()[0] * other.group0()[3]) + (self.group1()[3] * other.group0()[0])),
                ((self.group1()[1] * other.group0()[3]) + (self.group1()[3] * other.group0()[1])),
                ((self.group1()[2] * other.group0()[3]) + (self.group1()[3] * other.group0()[2])),
                (self.group1()[3] * other.group0()[3]),
            ]),
            // e15, e25, e35, e1234
            Simd32x4::from([
                ((self.group1()[3] * other.group1()[0]) + (self.group2()[0] * other.group0()[3])),
                ((self.group1()[3] * other.group1()[1]) + (self.group2()[1] * other.group0()[3])),
                ((self.group1()[3] * other.group1()[2]) + (self.group2()[2] * other.group0()[3])),
                (self.group2()[3] * other.group0()[3]),
            ]),
        );
    }
}
impl AntiWedge<VersorEvenOrthogonalOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiMotorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        5        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        7        8        0
    //  no simd       13       14        0
    fn anti_wedge(self, other: VersorEvenOrthogonalOrigin) -> Self::Output {
        return AntiMotorOnOrigin::from_groups(
            // e23, e31, e12, scalar
            ((Simd32x4::from(self.group1()[3]) * Simd32x4::from([other.group0()[0], other.group0()[1], other.group0()[2], other.group2()[3]]))
                + (Simd32x4::from(self.group2()[3]) * other.group1())
                + Simd32x4::from([
                    0.0,
                    0.0,
                    0.0,
                    (-(self.group2()[2] * other.group0()[2])
                        - (self.group2()[1] * other.group0()[1])
                        - (self.group2()[0] * other.group0()[0])
                        - (self.group0()[2] * other.group1()[2])
                        - (self.group0()[0] * other.group1()[0])
                        - (self.group0()[1] * other.group1()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOdd> for VersorOddOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       29        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       16       31        0
    //  no simd       22       37        0
    fn anti_wedge(self, other: VersorOdd) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group2()[3] * other.group3()[0]),
                (self.group2()[3] * other.group3()[1]),
                (self.group2()[3] * other.group3()[2]),
                (-(self.group1()[3] * other.group2()[3]) + (self.group2()[3] * other.group3()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group3()[0] * -1.0),
                (self.group1()[3] * other.group3()[1] * -1.0),
                (self.group1()[3] * other.group3()[2] * -1.0),
                ((self.group2()[2] * other.group3()[2]) + (self.group2()[1] * other.group3()[1]) - (self.group1()[3] * other.group1()[3]) + (self.group2()[0] * other.group3()[0])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group2()[0], other.group2()[1], other.group2()[2], other.group1()[3]]))
                - (swizzle!(other.group3(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[0] * other.group2()[3]) - (self.group1()[3] * other.group0()[0])
                        + (self.group0()[0] * other.group3()[3])
                        + (self.group1()[1] * other.group3()[2])),
                    (-(self.group2()[1] * other.group2()[3]) - (self.group1()[3] * other.group0()[1])
                        + (self.group1()[2] * other.group3()[0])
                        + (self.group0()[1] * other.group3()[3])),
                    (-(self.group2()[2] * other.group2()[3]) - (self.group1()[3] * other.group0()[2])
                        + (self.group0()[2] * other.group3()[3])
                        + (self.group1()[0] * other.group3()[1])),
                    (-(self.group0()[0] * other.group3()[0]) - (self.group0()[1] * other.group3()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAligningOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32       14       29        0
    //    simd4        2        2        0
    // Totals...
    // yes simd       16       31        0
    //  no simd       22       37        0
    fn anti_wedge(self, other: VersorOddAligningOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            Simd32x4::from([
                (self.group2()[3] * other.group2()[0]),
                (self.group2()[3] * other.group2()[1]),
                (self.group2()[3] * other.group2()[2]),
                (-(self.group1()[3] * other.group1()[3]) + (self.group2()[3] * other.group2()[3])),
            ]),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group2()[0] * -1.0),
                (self.group1()[3] * other.group2()[1] * -1.0),
                (self.group1()[3] * other.group2()[2] * -1.0),
                ((self.group2()[2] * other.group2()[2]) + (self.group2()[1] * other.group2()[1]) - (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[0], other.group1()[1], other.group1()[2], other.group0()[3]]))
                - (swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    (-(self.group2()[0] * other.group1()[3]) - (self.group1()[3] * other.group0()[0])
                        + (self.group0()[0] * other.group2()[3])
                        + (self.group1()[1] * other.group2()[2])),
                    (-(self.group2()[1] * other.group1()[3]) - (self.group1()[3] * other.group0()[1])
                        + (self.group1()[2] * other.group2()[0])
                        + (self.group0()[1] * other.group2()[3])),
                    (-(self.group2()[2] * other.group1()[3]) - (self.group1()[3] * other.group0()[2])
                        + (self.group0()[2] * other.group2()[3])
                        + (self.group1()[0] * other.group2()[1])),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtInfinity> for VersorOddOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       18        0
    //    simd4        2        3        0
    // Totals...
    // yes simd        9       21        0
    //  no simd       15       30        0
    fn anti_wedge(self, other: VersorOddAtInfinity) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (Simd32x4::from(self.group2()[3]) * other.group2()),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group2()[0] * -1.0),
                (self.group1()[3] * other.group2()[1] * -1.0),
                (self.group1()[3] * other.group2()[2] * -1.0),
                ((self.group2()[2] * other.group2()[2]) + (self.group2()[1] * other.group2()[1]) - (self.group1()[3] * other.group1()[3]) + (self.group2()[0] * other.group2()[0])),
            ]),
            // e1, e2, e3, e4
            ((Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group0()[1], other.group0()[2], other.group0()[3], other.group1()[3]]))
                - (swizzle!(other.group2(), 1, 2, 0, 2) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[2]]))
                + Simd32x4::from([
                    ((self.group0()[0] * other.group2()[3]) + (self.group1()[1] * other.group2()[2])),
                    ((self.group1()[2] * other.group2()[0]) + (self.group0()[1] * other.group2()[3])),
                    ((self.group0()[2] * other.group2()[3]) + (self.group1()[0] * other.group2()[1])),
                    (-(self.group0()[0] * other.group2()[0]) - (self.group0()[1] * other.group2()[1])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddAtOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        5        8        0
    //  no simd       11       14        0
    fn anti_wedge(self, other: VersorOddAtOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (-(Simd32x4::from(other.group1()[3]) * Simd32x4::from([self.group1()[3], self.group2()[0], self.group2()[1], self.group2()[2]]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group0()[3], other.group1()[0], other.group1()[1], other.group1()[2]]))
                + Simd32x4::from([
                    0.0,
                    ((self.group0()[0] * other.group0()[3]) - (self.group1()[3] * other.group0()[0])),
                    ((self.group0()[1] * other.group0()[3]) - (self.group1()[3] * other.group0()[1])),
                    ((self.group0()[2] * other.group0()[3]) - (self.group1()[3] * other.group0()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOnOrigin> for VersorOddOrthogonalOrigin {
    type Output = VersorEvenOrthogonalOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        7       18        0
    //    simd4        2        4        0
    // Totals...
    // yes simd        9       22        0
    //  no simd       15       34        0
    fn anti_wedge(self, other: VersorOddOnOrigin) -> Self::Output {
        return VersorEvenOrthogonalOrigin::from_groups(
            // e423, e431, e412, e321
            (swizzle!(other.group1(), 1, 2, 3, 0)
                * Simd32x4::from([self.group2()[3], self.group2()[3], self.group2()[3], self.group1()[3]])
                * Simd32x4::from([1.0, 1.0, 1.0, -1.0])),
            // e235, e315, e125, e5
            Simd32x4::from([
                (self.group1()[3] * other.group1()[1] * -1.0),
                (self.group1()[3] * other.group1()[2] * -1.0),
                (self.group1()[3] * other.group1()[3] * -1.0),
                ((self.group2()[2] * other.group1()[3]) + (self.group2()[1] * other.group1()[2]) - (self.group1()[3] * other.group0()[3]) + (self.group2()[0] * other.group1()[1])),
            ]),
            // e1, e2, e3, e4
            (-(swizzle!(other.group1(), 0, 0, 0, 3) * Simd32x4::from([self.group2()[0], self.group2()[1], self.group2()[2], self.group0()[2]]))
                - (swizzle!(other.group1(), 2, 3, 1, 1) * Simd32x4::from([self.group1()[2], self.group1()[0], self.group1()[1], self.group0()[0]]))
                + Simd32x4::from([
                    (-(self.group1()[3] * other.group0()[0]) + (self.group1()[1] * other.group1()[3])),
                    (-(self.group1()[3] * other.group0()[1]) + (self.group1()[2] * other.group1()[1])),
                    (-(self.group1()[3] * other.group0()[2]) + (self.group1()[0] * other.group1()[2])),
                    ((self.group2()[3] * other.group0()[3]) - (self.group0()[1] * other.group1()[2])),
                ])),
        );
    }
}
impl AntiWedge<VersorOddOrthogonalOrigin> for VersorOddOrthogonalOrigin {
    type Output = AntiFlectorOnOrigin;
    // Operative Statistics for this implementation:
    //           add/sub      mul      div
    //      f32        3        6        0
    //    simd4        2        2        0
    // Totals...
    // yes simd        5        8        0
    //  no simd       11       14        0
    fn anti_wedge(self, other: VersorOddOrthogonalOrigin) -> Self::Output {
        return AntiFlectorOnOrigin::from_groups(
            // e321, e1, e2, e3
            (-(Simd32x4::from(other.group2()[3]) * Simd32x4::from([self.group1()[3], self.group2()[0], self.group2()[1], self.group2()[2]]))
                + (Simd32x4::from(self.group2()[3]) * Simd32x4::from([other.group1()[3], other.group2()[0], other.group2()[1], other.group2()[2]]))
                + Simd32x4::from([
                    0.0,
                    ((self.group0()[0] * other.group1()[3]) - (self.group1()[3] * other.group0()[0])),
                    ((self.group0()[1] * other.group1()[3]) - (self.group1()[3] * other.group0()[1])),
                    ((self.group0()[2] * other.group1()[3]) - (self.group1()[3] * other.group0()[2])),
                ])),
        );
    }
}
